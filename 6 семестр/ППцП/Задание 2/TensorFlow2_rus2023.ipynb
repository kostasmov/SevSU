{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Глубокое обучение на TensorFlow?\n",
    "**Мовенко Константин**\n",
    "\n",
    "Вы уже написали много кода во второй части заданий, чтобы реализовать весь набор функций необходимых для создания различных нейронных сетей. Dropout, блочная нормализация и двумерные свертки являются рабочими лошадками глубинного обучения в компьютерном зрении. Вы также приложили много усилий, чтобы сделать ваш код эффективным и векторизованным.\n",
    "\n",
    "Однако в данном блокноте мы не станем использовать ваш код, а перейдем к одной из  популярных платформ глубокого обучения - TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "## Что такое TensorFlow?\n",
    "TensorFlow - это система вычислений над тензорными объектами с использованием вычислительных графов и встроенной поддержкой выполнения обратного распространения. Тензоры представляют собой n-мерные массивы, аналогичные numpy ndarray.\n",
    "\n",
    "## Зачем изучать TensorFlow?\n",
    "Наш код теперь сможет исполняться на графических процессорах! В этом случае обучение будет проходить гораздо быстрее.\n",
    "Мы хотим, чтобы Вы были готовы использовать один из развитых фреймворков для своих проектов, чтобы Вы могли проводить эксперименты эффективнее, чем если бы Вы писали каждую функцию вручную.\n",
    "Мы хотим, чтобы Вы стояли на плечах гигантов! TensorFlow и PyTorch - отличные фреймворки, которые сделают вашу жизнь намного проще.\n",
    "Мы хотим, чтобы Вы ознакомились с подходом к кодированию глубинного обучения, который применяется в академических кругах или в промышленности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "## Как изучать TensorFlow?\n",
    "\n",
    "В интернете вы найдете много руководств по TensorFlow.\n",
    "\n",
    "Этот блокнот также послужит Вам руководством по разработке и обучению моделей в TensorFlow. Просмотрите имеющиеся в  этом блокноте некоторые ссылки на полезные руководства, если вы хотите узнать больше или если Вам требуются дополнительные разъяснения.\n",
    "\n",
    "**ПРИМЕЧАНИЕ. Этот блокнот предназначен для изучения последней версии Tensorflow 2.0. Большинство примеров в Интернете по-прежнему относятся к версии 1.x, поэтому будьте осторожны, чтобы не перепутать их при поиске документации**.\n",
    "\n",
    "## Установка Tensorflow 2.X\n",
    "\n",
    " Tensorflow 2.X  проще в использовании и более интуитивен, чем TF 1.x. Пожалуйста, убедитесь, что он установлен, прежде чем переходить к этому блокноту! Вот несколько шагов по установке (некоторые шаги могут быть пропущены, если вы из делали ранее):\n",
    "\n",
    "1. Установите последнюю версию Anaconda на вашем компьютере.\n",
    "2. Создайте новую среду conda с Python 3.6 или 3.7: `conda create --name tf2_env python=3.6`\n",
    "   Здесь `tf2_env` имя среды, которое мы выбрали сами.\n",
    "3. Активируйте созданную среду командой: `conda activate tf2_env`\n",
    "4. Установите  TF версии 2.0 или выше командой: `conda install tensorflow`\n",
    "5. Проверьте установку TF, выполнив код:\n",
    "\n",
    "   `import tensorflow as tf\n",
    "    print(tf.__version__)`\n",
    "    \n",
    " В результате будет напечатан номер установленной версии Tensorflow. Данные задания были проверены в версии 2.1.0 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "# Содержание\n",
    "\n",
    "Этот блокнот состоит из 5 частей. Мы будем рассматривать  TensorFlow на трех разных уровнях абстракции , что должно способствовать лучшему пониманию.\n",
    "\n",
    "1. Часть I, Подготовка: загрузка множества данных CIFAR-10.\n",
    "2. Часть II, Базовый TensorFlow (**уровень 1**): непосредственно используются низкоуровневые возможности TensorFlow.\n",
    "3. Часть III, API объекта tf.keras.Model (**уровень 2**): используется  `tf.keras.Model` для определения произвольной архитектуры нейронной сети.\n",
    "4.  Часть IV, Последовательный и функциональные API Keras (**уровень 3**): используется объект `tf.keras.Sequential` для определения простой линейной структуры сети прямого распространения и функциональные вызовы слоев сети при построении более сложных взаимосвязей. \n",
    "5. Часть V, Открытая задача классификации изображений  CIFAR-10: выполняется построение сети, которая способна обеспечить  точность классификации более 70% для базы изображений CIFAR-10. Вы можете экспериментировать с любым слоем, оптимизатором, гиперпараметрами или другими дополнительными свойствами.\n",
    "\n",
    "Сравнительная таблица свойств программных интерфейсов (API):\n",
    "\n",
    "| API           | Гибкость    | Простота    |\n",
    "|---------------|-------------|-------------|\n",
    "| Базовый       | Высокая     | Низкая      |\n",
    "| `tf.keras.Model`     | Высокая| Средняя   |\n",
    "| `tf.keras.Sequential`| Низкая | Высокая   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Часть I: Подготовительная\n",
    "\n",
    "Сначала загрузим набор данных CIFAR-10. Это может занять несколько минут для загрузки при первом запуске, но после этого файлы должны быть кэшированы на диске, а загрузка должна быть быстрее.\n",
    "\n",
    "В предыдущих частях задания мы использовали специфический код для загрузки и чтения набора данных CIFAR-10; однако пакет `tf.keras.datasets` в TensorFlow предоставляет предустановленные утилиты для загрузки многих распространенных наборов данных.\n",
    "\n",
    "Для целей задания мы по-прежнему будем писать собственный код для предварительной обработки данных и итерации на данных по мини-блокам. Модуль `tf.data` в TensorFlow предоставляет необходимые инструменты для автоматизации этого процесса, однако работа с этим модулем  выходит за рамки заданий, рассматриваемых в этом блокноте. Тем не менее, использование `tf.data` может быть намного эффективнее, чем простой подход, рассматриваемый ниже. Поэтому самостоятельно познакомьтесь с модулем  `tf.data` и используйте его в своих проектах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Форма обучающих данных:  (49000, 32, 32, 3)\n",
      "Форма меток обучающих данных:  (49000,) int32\n",
      "Форма валидационных данных:  (1000, 32, 32, 3)\n",
      "Форма меток валидационных данных:  (1000,)\n",
      "Форма тестовых данных:  (10000, 32, 32, 3)\n",
      "Форма меток тестовых данных:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "def load_cifar10(num_training=49000, num_validation=1000, num_test=10000):\n",
    "    \"\"\"\n",
    "    Функция извлекает набор данных CIFAR-10 из Интернета и выполняет предварительную\n",
    "    обработку данных. \n",
    "    Такие же шаги, что мы использовали ранее в блокноте SVM, но теперь они собраны в одной функции.\n",
    "      \n",
    "    \"\"\"\n",
    "    # Загрузка базы CIFAR-10 и приведение данных к необходимым типам и формам\n",
    "    cifar10 = tf.keras.datasets.cifar10.load_data()\n",
    "    (X_train, y_train), (X_test, y_test) = cifar10\n",
    "    X_train = np.asarray(X_train, dtype=np.float32)\n",
    "    y_train = np.asarray(y_train, dtype=np.int32).flatten()\n",
    "    X_test = np.asarray(X_test, dtype=np.float32)\n",
    "    y_test = np.asarray(y_test, dtype=np.int32).flatten()\n",
    "\n",
    "    # Создание подмножеств данных\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Нормализация данных: вычитание средних значений пикселей и деление на стандартное отклонение\n",
    "    mean_pixel = X_train.mean(axis=(0, 1, 2), keepdims=True)\n",
    "    std_pixel = X_train.std(axis=(0, 1, 2), keepdims=True)\n",
    "    X_train = (X_train - mean_pixel) / std_pixel\n",
    "    X_val = (X_val - mean_pixel) / std_pixel\n",
    "    X_test = (X_test - mean_pixel) / std_pixel\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "# Если при SSL загрузке  возникают ошибки, связанные с самозаверяющими сертификатами,\n",
    "# возможно, ваша версия Python была недавно установлена на данном компьютере.\n",
    "# См .: https://github.com/tensorflow/tensorflow/issues/10779\n",
    "# Чтобы исправить, запустите команду: /Applications/Python\\ 3.7 /Install\\ Certificates.command\n",
    "# ... заменив пути при необходимости.\n",
    "\n",
    "# Вызоваем вышеопределенную функцию, чтобы загрузить данные.\n",
    "NHW = (0, 1, 2)\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10()\n",
    "print('Форма обучающих данных: ', X_train.shape)\n",
    "print('Форма меток обучающих данных: ', y_train.shape, y_train.dtype)\n",
    "print('Форма валидационных данных: ', X_val.shape)\n",
    "print('Форма меток валидационных данных: ', y_val.shape)\n",
    "print('Форма тестовых данных: ', X_test.shape)\n",
    "print('Форма меток тестовых данных: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предварительная подготовка: объект Dataset\n",
    "\n",
    "Для удобства мы определим упрощенный класс `Dataset`, который позволит нам перебирать данные и метки. Это не самый гибкий или эффективный способ перебора данных, но он будет служить нашим целям."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, X, y, batch_size, shuffle=False):\n",
    "        \"\"\"\n",
    "        Создание объекта Dataset для итераций по набору данных X и  меткам y\n",
    "        \n",
    "         Входы:\n",
    "         - X: numpy массив данных любой формы\n",
    "         - y: numpy массив меток любой формы, но с y.shape [0] == X.shape [0]\n",
    "         - batch_size: целое число, указывающее количество элементов миниблока\n",
    "         - shuffle: (необязательный) Логическое значение, указывает следует ли \n",
    "           перемешивать данные на каждой эпохе\n",
    "        \"\"\"\n",
    "                \n",
    "        assert X.shape[0] == y.shape[0], 'Проверка совпадения количества данных и меток'\n",
    "        self.X, self.y = X, y\n",
    "        self.batch_size, self.shuffle = batch_size, shuffle\n",
    "\n",
    "    def __iter__(self):\n",
    "        N, B = self.X.shape[0], self.batch_size\n",
    "        idxs = np.arange(N)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(idxs)\n",
    "        return iter((self.X[i:i+B], self.y[i:i+B]) for i in range(0, N, B))\n",
    "\n",
    "# создаем экземпляры класса Dataset для работы с разными подмножествами данных\n",
    "train_dset = Dataset(X_train, y_train, batch_size=64, shuffle=True)\n",
    "val_dset = Dataset(X_val, y_val, batch_size=64, shuffle=False)\n",
    "test_dset = Dataset(X_test, y_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (64, 32, 32, 3) (64,)\n",
      "1 (64, 32, 32, 3) (64,)\n",
      "2 (64, 32, 32, 3) (64,)\n",
      "3 (64, 32, 32, 3) (64,)\n",
      "4 (64, 32, 32, 3) (64,)\n",
      "5 (64, 32, 32, 3) (64,)\n",
      "6 (64, 32, 32, 3) (64,)\n"
     ]
    }
   ],
   "source": [
    "# Мы можем выполнять итерации так:\n",
    "for t, (x, y) in enumerate(train_dset):\n",
    "    print(t, x.shape, y.shape)\n",
    "    if t > 5: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вы можете опционально использовать GPU, установив флаг **USE_GPU в True** ниже. Для этого задания не обязательно использовать GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  /cpu:0\n"
     ]
    }
   ],
   "source": [
    "# Задание некоторых глобальных переменных\n",
    "USE_GPU = False\n",
    "\n",
    "if USE_GPU:\n",
    "    device = '/device:GPU:0'\n",
    "else:\n",
    "    device = '/cpu:0'\n",
    "\n",
    "# Константа,определяющая через сколько итераций будет выполняться печать сообщений при обучении моделей\n",
    "print_every = 100\n",
    "\n",
    "print('Using device: ', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "# Часть II: Базовый интерфейс TensorFlow\n",
    "\n",
    "TensorFlow поставляется с различными высокоуровневыми API, что делает его очень удобным для определения и обучения нейронных сетей; мы рассмотрим некоторые из этих конструкций в части III и части IV этого блокнота. В этом разделе мы начнем с построения модели с базовыми конструкциями TensorFlow, чтобы далее помочь вам лучше разобраться с тем, что происходит под капотом высокоуровневых API.\n",
    "\n",
    "**«Базовый Tensorflow» важен для понимания строительных блоков TensorFlow. Значительная его часть включает в себя концепции из TensorFlow 1.x.** Поэтому мы будем использовать старые модули, например `tf.Variable`.\n",
    "\n",
    "Прочитайте ниже и поймите различия между устаревшей версией TF 1.x и новой TF 2.0.\n",
    "\n",
    "### Основная философия  TensorFlow 1.x\n",
    "TensorFlow 1.x, в первую очередь, - это фреймворк для  работы со **статическими вычислительными графами**. Ребрами  вычислительного графа являются тензоры, которые хранят n-мерные массивы; узлы  графа представляют собой функции, которые применяются к тензорам, когда выполняются вычисления в соответствии с вычислительным графом.\n",
    "\n",
    "Это означает, что типичная программа с использованием TensorFlow 1.x выполняется в два этапа:\n",
    "\n",
    "1. **Создание вычислительного графа, который описывает вычисления, подлежащие выполнению.** Этот этап фактически не выполняет никаких вычислений; он просто создает символическое представление ваших вычислений. Этот этап обычно определяет один или несколько объектов типа `placeholder`, которые представляют входные данные вычислительного графа.\n",
    "2. **Многократное исполнение вычислительного графа.** Каждый раз, когда граф исполняется, вы указываете, какие части графа вы хотите вычислить, и передаёте словарь `feed_dict`, который поставляет конкретные значения любому объекту ` placeholder `на вход графа.\n",
    "\n",
    "### Новая парадигма в Tensorflow 2.0\n",
    "В Tensorflow 2.0, мы можем  использовать Python-подобные функциональные формы, которые более похожи по духу на PyTorch и  операции  Numpy библиотеки. Вместо двухэтапной парадигмы работы со статическим графом вычислений, Tensorflow 2.0 использует динамический граф, что облегчает (помимо прочего) отладку кода TF 2.0.\n",
    "Более подробно об этом можно почитатать здесь https://www.tensorflow.org/guide/eager.\n",
    "\n",
    "В TF 2.0 не используются модули TF 1.x, такие как : `tf.Session`,` tf.run`, `placeholder`,` feed_dict`. Чтобы получить детальную информацию о различиях между двумя версиями и о том, как выполнять преобразование между ними, ознакомьтесь с официальным руководством по миграции: https://www.tensorflow.org/alpha/guide/migration_guide.\n",
    "\n",
    "Ниже, в соответствующих частях этого блокнота, мы будем рассмотривать этот новый, более простой подход."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "### Разминка с TensorFlow: функция flatten\n",
    "\n",
    "Определим простую функцию «уплощения данных» `flatten`, которая реформатирует данные изображений для использования в полносвязанной нейронной сети.\n",
    "\n",
    "В TensorFlow данные для сверточных карт признаков  обычно хранятся в тензоре формы N x H x W x C, где:\n",
    "\n",
    "- N - количество примеров данных (размер мини-блока);\n",
    "- H - высота карты; \n",
    "- W - ширина карты; \n",
    "- C - количество каналов карты.\n",
    "\n",
    "Это обычный способ представления данных для двумерной свертки, которая учитывает пространственные отношения между признаками изображений. Однако, когда мы используем полносвязанные слои нейронов для обработки изображений, то требуется, чтобы каждое  изображение представлялось вектором. Поэтому необходимо реформатировать изображение размером «H x W x C» в один \"длинный\" вектор. \n",
    "\n",
    "Обратите внимание,  что вызов метода `tf.reshape` в ячейке ниже использует аргумент `(N, -1)`, что означает, что он оставит первое измерение данных равным N, а затем  автоматически определит , каким должно быть  второе измерение выходных данных.  \n",
    "\n",
    "**ПРИМЕЧАНИЕ**: TensorFlow и PyTorch различаются своими представлениями тензоров по умолчанию; TensorFlow использует представление N x H x W x C, а PyTorch использует N x C x H x W.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "    \"\"\"    \n",
    "    Входные данные:\n",
    "     - Тензор формы (N, D1, ..., DM)\n",
    "    \n",
    "     Выход:\n",
    "     Тензор формы (N, D1 * ... * DM)\n",
    "    \"\"\"\n",
    "    \n",
    "    N = tf.shape(x)[0]\n",
    "    return tf.reshape(x, (N, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_np:\n",
      " [[[ 0  1  2  3]\n",
      "  [ 4  5  6  7]\n",
      "  [ 8  9 10 11]]\n",
      "\n",
      " [[12 13 14 15]\n",
      "  [16 17 18 19]\n",
      "  [20 21 22 23]]] \n",
      "\n",
      "x_flat_np:\n",
      " tf.Tensor(\n",
      "[[ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      " [12 13 14 15 16 17 18 19 20 21 22 23]], shape=(2, 12), dtype=int32) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_flatten():\n",
    "    # Создание массива х с использованием функций numpy\n",
    "    x_np = np.arange(24).reshape((2, 3, 4))\n",
    "    print('x_np:\\n', x_np, '\\n')\n",
    "    # вычисление выходного массива\n",
    "    x_flat_np = flatten(x_np)\n",
    "    print('x_flat_np:\\n', x_flat_np, '\\n')\n",
    "\n",
    "test_flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Базовый  TensorFlow: Двухслойная сеть\n",
    "Теперь мы реализуем нашу первую нейронную сеть на TensorFlow: двухслойную сеть с двумя полносвязными слоями без смещений и с ReLU нелинейностью. Пока будем использовать только низкоуровневые операторы TensorFlow для определения сети; позже мы увидим, как использовать абстракции более высокого уровня, предоставляемые `tf.keras`, чтобы упростить процесс.\n",
    "\n",
    "Определим функцию прямого распространения  `two_layer_fc`; она будет принимать тензоры входа и весов сети и возвращать тензор оценки предсказания класса. \n",
    "\n",
    "После определения  функции `two_layer_fc` мы проверим её реализацию, исполнив вычислительный граф, подав нули на входы сети и проверив формы выходных двнных.\n",
    "\n",
    "Важно, чтобы вы прочитали и поняли эту реализацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "def two_layer_fc(x, params):\n",
    "    \"\"\"\n",
    "    Полносвязная 2-х слойная нейронная сеть; \n",
    "    архитектура: полносвязный слой -> ReLU ->  полносвязный слой .\n",
    "    \n",
    "    Обратите внимание, что нам сейчас нужно только определить \n",
    "    прямое распространение;TensorFlow позаботится сам о\n",
    "    вычислении градиентов.\n",
    "    \n",
    "    Вход сети - мини-блок данных размерности (формы):\n",
    "    (N, d1, ..., dM), где d1 * ... * dM = D. \n",
    "    Скрытый слой содержит H нейронов.\n",
    "    Выходной слой вычисляет оценки рейтигов для C классов.\n",
    "\n",
    "    Входы:\n",
    "     - x: тензор формы (N, d1, ..., dM), представляющий мини-блок\n",
    "       входных данных.\n",
    "     - params: список [w1, w2] тензоров, представляющих веса сети,\n",
    "       где w1 имеет форму (D, H), а w2 имеет форму (H, C).\n",
    "    \n",
    "     Возвращает:\n",
    "     - scores: тензор формы (N, C), представляющий оценки рейтингов \n",
    "       принадлежности классам входных данных x.  \n",
    "    \"\"\"\n",
    "    \n",
    "    w1, w2 = params                   # распаковка параметров\n",
    "    x = flatten(x)                    # реформатируем x к форме (N, D)\n",
    "    h = tf.nn.relu(tf.matmul(x, w1))  # Скрытый слой : форма h - (N, H)\n",
    "    scores = tf.matmul(h, w2)         # Вычисление рейтингов, форма scores - (N, C)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Форма выходных данных 2х слойной сети: (64, 10)\n"
     ]
    }
   ],
   "source": [
    "def two_layer_fc_test():\n",
    "    \"\"\" Функция проверки двухслойной сети\"\"\"\n",
    "    hidden_layer_size = 42\n",
    "\n",
    "    # Разместим  код вычислительного графа в контексте менежджера устройств tf.device,\n",
    "    # что позволит указывать TensorFlow, где должны размещаться тензоры в CPU или GPU\n",
    "    with tf.device(device):        \n",
    "        x = tf.zeros((64, 32, 32, 3))\n",
    "        w1 = tf.zeros((32 * 32 * 3, hidden_layer_size))\n",
    "        w2 = tf.zeros((hidden_layer_size, 10))\n",
    "\n",
    "        # Вызов функции two_layer_fc для реализации прямого распространения \n",
    "        scores = two_layer_fc(x, [w1, w2])\n",
    "\n",
    "    print('Форма выходных данных 2х слойной сети:',scores.shape) #Вывод размерности score\n",
    "\n",
    "two_layer_fc_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Базовый TensorFlow: Трехслойная  ConvNet\n",
    "\n",
    "Реализуйте функцию `three_layer_convnet`, которая будет выполнять прямое распространение для трехслойной сверточной сети. Сеть  должна иметь следующую архитектуру:\n",
    "\n",
    "1. Сверточный слой (со смещением) с числом фильтров `channel_1`, каждый размером ` KW1 x KH1` и дополнением двумя нулями, P=2\n",
    "2. Нелинейность ReLU\n",
    "3. Сверточный слой (со смещением) с числом фильтров `channel_2`, каждый размером ` KW2 x KH2` и  дополнением одним нулем, P=1\n",
    "4. Нелинейность ReLU\n",
    "5. Полносвязанный слой со смещением, вычисляющий оценки рейтингов для `C` классов .\n",
    "\n",
    "\n",
    "**СОВЕТ**: Для реализации свертки используйте вызов: `tf.nn.conv2d(x,filter,strides,padding,use_cudnn_on_gpu = None,…)`, где параметр filter - список вида `[Н,W,Channel_in,Channel_out]`; strides - 4-х элементный список значений сдвига окна свертки в каждом направлении, часто `[1,1,1,1]`; `padding ='VALID'`.\n",
    "\n",
    "Будьте внимательны с добавлением нулей! Нули добавляем только по высоте и ширине изображения. Например, дополнение двумя нулями по высоте и ширине можно реализовать так:   `tf.pad(x, [[0, 0], [2, 2], [2, 2], [0, 0]], 'CONSTANT')`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def three_layer_convnet(x, params):\n",
    "   \n",
    "    \"\"\"\n",
    "    Трехслойная сверточная сеть с описанной выше архитектурой.\n",
    "    \n",
    "    Входы:\n",
    "    - x: тензор формы (N, H, W, 3), представляющий мини-блок изображений\n",
    "    - params: список тензоров, представляющих веса и смещения \n",
    "      сети; должен содержать следующее:\n",
    "      - conv_w1: тензор формы (KH1, KW1, 3, channel_1) -\n",
    "        веса первого сверточного слоя.\n",
    "      - conv_b1: тензор формы (channel_1,) - смещения\n",
    "        первого сверточного слоя.\n",
    "      - conv_w2: тензор формы (KH2, KW2, channel_1, channel_2) -\n",
    "        веса второго сверточного слоя\n",
    "      - conv_b2: тензор  формы (channel_2,) - смещения\n",
    "        второго сверточного слоя.\n",
    "      - fc_w: тензор представляющий весовые коэффициенты полносвязанного слоя.\n",
    "        Могли бы определить сами, какая должна быть форма?\n",
    "      - fc_b: тензор смещений полносвязанного слоя.\n",
    "        Могли бы определить сами, какая должна быть форма?    \n",
    "    \"\"\"\n",
    "\n",
    "    conv_w1, conv_b1, conv_w2, conv_b2, fc_w, fc_b = params\n",
    "    scores = None\n",
    "    \n",
    "    ############################################################################\n",
    "    # ЗАДАНИЕ: Реализуйте прямое распространение для 3-х слойной ConvNet.      #\n",
    "    ############################################################################\n",
    "    # *****НАЧАЛО ВАШЕГО КОДА (НЕ УДАЛЯЙТЕ/НЕ МОДИФИЦИРУЙТЕ ЭТУ СТРОКУ)*****\n",
    "   \n",
    "    # дополняем x двумя нулями по высоте и ширине\n",
    "    x_padded = tf.pad(x, [[0, 0], [2, 2], [2, 2], [0, 0]], 'CONSTANT')\n",
    "    \n",
    "    # реализуем первый свёрточный слой +relu\n",
    "    conv1 = tf.nn.conv2d(x_padded, conv_w1, [1, 1, 1, 1], padding='VALID') + conv_b1\n",
    "    relu1 = tf.nn.relu(conv1)\n",
    "    \n",
    "    # дополняем выход relu нулями по высоте и ширине\n",
    "    conv1_padded = tf.pad(relu1, [[0, 0], [1, 1], [1, 1], [0, 0]], 'CONSTANT')\n",
    "    \n",
    "    # реализуем второй свёрточный слой +relu\n",
    "    conv2 = tf.nn.conv2d(conv1_padded, conv_w2, [1, 1, 1, 1], padding='VALID') + conv_b2\n",
    "    relu2 = tf.nn.relu(conv2)\n",
    "    \n",
    "    # 'уплощаем' выход relu2 (преобразуем в вектор)\n",
    "    conv2_flattened = flatten(relu2)\n",
    "    \n",
    "    # реализуем полносвязанный слой\n",
    "    fc1 = tf.matmul(conv2_flattened, fc_w) + fc_b\n",
    "    scores = fc1\n",
    "\n",
    "    # *****КОНЕЦ ВАШЕГО КОДА (НЕ УДАЛЯЙТЕ/НЕ МОДИФИЦИРУЙТЕ ЭТУ СТРОКУ)*****\n",
    "    ############################################################################\n",
    "    #                              КОНЕЦ ВАШЕГО КОДА                           #\n",
    "    ############################################################################\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После определения  трехслойной ConvNet выше, запустите следующую ячейку, чтобы проверить вашу реализацию. Подобно двухслойной сети выполним проверку на миниблоке данных из нулей только для того, чтобы убедиться, что функция не дает сбоев, и создает выходы правильной формы (размерности).\n",
    "\n",
    "В результате выполнения этой функции `scores_np` должен будет иметь форму` (64, 10) `."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Форма выходных данных сверточной сети: (64, 10)\n"
     ]
    }
   ],
   "source": [
    "def three_layer_convnet_test():\n",
    "    \n",
    "    with tf.device(device):\n",
    "        x = tf.zeros((64, 32, 32, 3))      #N=64  HхW=32x32,  C=3\n",
    "        conv_w1 = tf.zeros((5, 5, 3, 6))   #KH1,KW1,3,ch1\n",
    "        conv_b1 = tf.zeros((6,))\n",
    "        conv_w2 = tf.zeros((3, 3, 6, 9))   #KH2,KW2,ch1,ch2\n",
    "        conv_b2 = tf.zeros((9,))\n",
    "        fc_w = tf.zeros((32 * 32 * 9, 10)) #32x32xch2\n",
    "        fc_b = tf.zeros((10,))\n",
    "        params = [conv_w1, conv_b1, conv_w2, conv_b2, fc_w, fc_b]\n",
    "        scores = three_layer_convnet(x, params)\n",
    "\n",
    "    # Входы  сверточных слоев представляют собой 4-мерные массивы с формой\n",
    "    # [размер_блока, высота, ширина, каналы]\n",
    "    print('Форма выходных данных сверточной сети:', scores.shape)\n",
    "\n",
    "three_layer_convnet_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Базовый TensorFlow: шаг обучения\n",
    "\n",
    "Теперь мы определим функцию `training_step`, которая выполняет один шаг обучения. Для этого необходимо выполнить три основных действия:\n",
    "\n",
    "1. Вычислить функцию потерь;\n",
    "2. Вычислить градиент функции потерь по отношению ко всем весам сети;\n",
    "3. Выполнить шаг обновления весов, используя стохастический градиентный спуск.\n",
    "\n",
    "Чтобы сделать все это, нам нужно использовать несколько новых функций и объектов TensorFlow:\n",
    "1. Для вычисления кросс-энтропийной функции потерь будем использовать метод `tf.nn.sparse_softmax_cross_entropy_with_logits(labels,logits)`, который  вычисляет softmax кросс-энтропию между вектором меток labels (размер (N, )) и  массивом logits (размер (N, C)) не масштабированных log вероятностей;\n",
    "2. Для усреднения потерь по мини-блоку данных  будем использовать метод `tf.math.reduce_mean(input_tensor, axis=None,  keepdims=False)`, который вычисляет среднее по измерениям входного тензора, axis – указывает по каким осям усреднять (None – по всем), keepdims=True – размеры сокращаются до 1);\n",
    "3.  Для вычисления градиентов функции потерь по отношению к весам будем использовать объект `tf.GradientTape()` , в области действия которого записывают те операции, которые образуют вычислительный граф нейросети;  по крайней мере, один из аргументов этих операций  должен быть наблюдаемым (\"watched\"); обучаемые переменные, созданные с помощью  `tf.Variable`  (где trainable=True по умолчанию) являются автоматически наблюдаемыми; в контекст `tf.GradientTape()` включают вызов метода `gradient` для вычисления производных.\n",
    "4. Будем изменять значения весов, хранящихся в виде тензоров TensorFlow, используя `tf.assign_sub(ref, value)` – обновляет тензор ref путем вычитания value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "def training_step(model_fn, x, y, params, learning_rate):\n",
    "    \"\"\"\n",
    "     Функция, реализующая шаг SGD обучения модели model_fn\n",
    "     Входы:\n",
    "     - model_fn: функция модели нейросети, которая реализует прямое распространение,\n",
    "       используя TensorFlow; она должна иметь следующую сигнатуру:\n",
    "       scores = model_fn (x, params), где x - тензор, представляющий\n",
    "       мини-блок данных с изображениями, params - список тензоров, хранящих\n",
    "       веса модели, scores - тензор  формы (N, C), содержащий\n",
    "       рейтинги  всех классов из x.\n",
    "     - x: входной тензор модели model_fn\n",
    "     - y: тензор, содержащий корректные метки классов\n",
    "     - params: cписок тензоров, представляющих  параметры модели\n",
    "     - learning_rate: скорость обучения SGD\n",
    "      Возвращает: total_loss – общие потери на блоке данных x\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        scores = model_fn(x, params) # Прямое распространение\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=scores)\n",
    "        total_loss = tf.reduce_mean(loss) # Вычисление средних потерь\n",
    "        grad_params = tape.gradient(total_loss, params) # Вычисление градиентов по параметрам\n",
    "\n",
    "        # Выполнение шага SGD по всем параметрам модели\n",
    "        # Ручное обновление весов с помощью assign_sub()\n",
    "        # zip формирует список кортежей из пар параметров\n",
    "        for w, grad_w in zip(params, grad_params):\n",
    "            w.assign_sub(learning_rate * grad_w)\n",
    "                        \n",
    "        return total_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "def train_part2(model_fn, init_fn, learning_rate):\n",
    "    \"\"\"\n",
    "    Обучение модели на множестве CIFAR-10.\n",
    "        \n",
    "    Входы:\n",
    "     - model_fn: функция модели нейросети, которая реализует прямое распространение,\n",
    "       используя TensorFlow; она должна иметь следующую сигнатуру:\n",
    "       scores = model_fn (x, params), где x - тензор, представляющий\n",
    "       мини-блок данных с изображениями, params - список тензоров, хранящих\n",
    "       веса модели, scores - тензор  формы (N, C), содержащий\n",
    "       рейтинги  всех классов из x.\n",
    "     - init_fn: функция, которая инициализирует параметры модели.\n",
    "       Она должна иметь сигнатуру params = init_fn(), где params - это список\n",
    "       тензоров, хранящих (случайно инициализированные) веса модели.\n",
    "     - learning_rate: скорость обучения.\n",
    "      \n",
    "    \"\"\"\n",
    "    \n",
    "    params = init_fn()  # Инициализация параметров модели            \n",
    "    # цикл по множеству обучающих данных    \n",
    "    for t, (x_np, y_np) in enumerate(train_dset):\n",
    "        # Выполнение шага обучения на блоке обучающих данных\n",
    "        loss = training_step(model_fn, x_np, y_np, params, learning_rate)\n",
    "        \n",
    "        # Периодически выводим потери и проверяем точность на валидационном множестве\n",
    "        if t % print_every == 0:\n",
    "            print('Итерации %d, потери = %.4f' % (t, loss))\n",
    "            #check_accuracy(val_dset, x_np, model_fn, params)\n",
    "            check_accuracy(val_dset, model_fn, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "def check_accuracy(dset, model_fn, params):\n",
    "    \"\"\"\n",
    "    Проверяет точность классификации.\n",
    "    \n",
    "     Входы:\n",
    "     - dset: данные типа Dataset, используемые для проверки точности\n",
    "     - model_fn: модель, которую мы будем вызывать для предказания по x\n",
    "     - params: параметры модели  model_fn\n",
    "      \n",
    "     Возвращает: ничего не возвращает, но выводит точность модели\n",
    "    \"\"\"\n",
    "    num_correct, num_samples = 0, 0\n",
    "    for x_batch, y_batch in dset:\n",
    "        scores_np = model_fn(x_batch, params).numpy()\n",
    "        y_pred = scores_np.argmax(axis=1)\n",
    "        num_samples += x_batch.shape[0]\n",
    "        num_correct += (y_pred == y_batch).sum()\n",
    "    acc = float(num_correct) / num_samples\n",
    "    print('Классифицировано %d / %d корректно (%.2f%%)' % (num_correct, num_samples, 100 * acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Базовый TensorFlow: инициализация весов нейросети\n",
    "Мы будем использовать функцию для инициализации матриц весов моделей, базирующуюся на методе\n",
    "нормировки, предложенном в\n",
    "\n",
    "[1] He et al, *Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification\n",
    "*, ICCV 2015, https://arxiv.org/abs/1502.01852"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaiming_normal(shape):   \n",
    "    if len(shape) == 2:                  # для полносвязной сети\n",
    "        fan_in, fan_out = shape[0], shape[1]\n",
    "    elif len(shape) == 4:                # для сверточной сети\n",
    "        fan_in, fan_out = np.prod(shape[:3]), shape[3]\n",
    "    # веса масштабируются обратно-пропорционально квадратному корню от \"число входов/2\"   \n",
    "    return tf.keras.backend.random_normal(shape) * np.sqrt(2.0 / fan_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Базовый  TensorFlow: обучение 2-х слойной нейросети\n",
    "Наконец, мы готовы использовать все части, определенные выше, для обучения двухслойной полносвязной сети на множестве данных CIFAR-10.\n",
    "\n",
    "Нам просто нужно определить функцию для инициализации весов модели и вызвать `train_part2`.\n",
    "\n",
    "Определение весов сети представляет собой еще один важный компонент TensorFlow API: `tf.Variable`. TensorFlow Variable - это тензор-переменная, значение которой хранится в графе и сохраняется на разных  циклах исполнения вычислительного графа; однако в отличие от констант, определенных с помощью `tf.zeros` или` tf.random_normal`, значения переменной могут быть изменены при выполнении графа; эти изменения будут сохраняться в графе. Обучаемые параметры сети обычно хранятся в переменных.\n",
    "\n",
    "Вам не нужно настраивать гиперпараметры, но вы должны достичь точности выше 40% после одной эпохи обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Итерации 0, потери = 3.2034\n",
      "Классифицировано 100 / 1000 корректно (10.00%)\n",
      "Итерации 100, потери = 2.0725\n",
      "Классифицировано 371 / 1000 корректно (37.10%)\n",
      "Итерации 200, потери = 1.5269\n",
      "Классифицировано 393 / 1000 корректно (39.30%)\n",
      "Итерации 300, потери = 1.8327\n",
      "Классифицировано 370 / 1000 корректно (37.00%)\n",
      "Итерации 400, потери = 1.7878\n",
      "Классифицировано 423 / 1000 корректно (42.30%)\n",
      "Итерации 500, потери = 1.7353\n",
      "Классифицировано 429 / 1000 корректно (42.90%)\n",
      "Итерации 600, потери = 1.8410\n",
      "Классифицировано 429 / 1000 корректно (42.90%)\n",
      "Итерации 700, потери = 1.9588\n",
      "Классифицировано 442 / 1000 корректно (44.20%)\n"
     ]
    }
   ],
   "source": [
    "def two_layer_fc_init():\n",
    "    \"\"\"\n",
    "    Инициализирует веса двухслойной сети для использования с\n",
    "    two_layer_network, определенной выше.\n",
    "    \n",
    "     Входы: отсутствуют\n",
    "    \n",
    "     Возвращает: список:\n",
    "     - w1: TensorFlow tf.Variable, представляющая веса первого слоя\n",
    "     - w2: TensorFlow tf.Variable, представляющая веса второго слоя\n",
    "    \"\"\"\n",
    "    hidden_layer_size = 4000\n",
    "    w1 = tf.Variable(kaiming_normal((3 * 32 * 32, 4000)))\n",
    "    w2 = tf.Variable(kaiming_normal((4000, 10)))\n",
    "    return [w1, w2]\n",
    "\n",
    "# Обучение двухслойной сети\n",
    "learning_rate = 1e-2\n",
    "train_part2(two_layer_fc, two_layer_fc_init, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Базовый  TensorFlow: Обучение 3-х слойной  ConvNet\n",
    "\n",
    "Теперь мы будем использовать TensorFlow для обучения трехслойной ConvNet на CIFAR-10.\n",
    "\n",
    "Вам нужно реализовать функцию `three_layer_convnet_init`. Напомним архитектуру сети:\n",
    "\n",
    "1. Сверточный слой (со смещением) с 32 фильтрами 5 × 5 с дополнением нулями Р=2\n",
    "2. ReLU\n",
    "3. Сверточный слой (со смещением) с 16 фильтрами 3x3 с дополнением нулями Р=1\n",
    "4. ReLU\n",
    "5. Полносвязный слой (со смещением) для вычисления оценок scores 10 классов\n",
    "\n",
    "Вам не нужно делать какие-либо настройки гиперпараметров, но вы должны получить точность выше 43% после одной эпохи обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Итерации 0, потери = 2.7652\n",
      "Классифицировано 101 / 1000 корректно (10.10%)\n",
      "Итерации 100, потери = 1.9594\n",
      "Классифицировано 354 / 1000 корректно (35.40%)\n",
      "Итерации 200, потери = 1.5008\n",
      "Классифицировано 381 / 1000 корректно (38.10%)\n",
      "Итерации 300, потери = 1.6558\n",
      "Классифицировано 395 / 1000 корректно (39.50%)\n",
      "Итерации 400, потери = 1.7575\n",
      "Классифицировано 431 / 1000 корректно (43.10%)\n",
      "Итерации 500, потери = 1.7014\n",
      "Классифицировано 434 / 1000 корректно (43.40%)\n",
      "Итерации 600, потери = 1.6804\n",
      "Классифицировано 453 / 1000 корректно (45.30%)\n",
      "Итерации 700, потери = 1.6238\n",
      "Классифицировано 448 / 1000 корректно (44.80%)\n"
     ]
    }
   ],
   "source": [
    "def three_layer_convnet_init():\n",
    "    \"\"\"\n",
    "    Инициализирует веса трехслойной ConvNet, для использования с\n",
    "    three_layer_convnet, определенной выше.\n",
    "    Вы можете использовать `kaiming_normal`!\n",
    "    \n",
    "     Входы: Отсутствуют\n",
    "    \n",
    "     Возвращает список, содержащий:\n",
    "     - conv_w1: переменная tf.Variable, содержащая веса для первого слоя conv\n",
    "     - conv_b1: переменная tf.Variable, содержащая смещения для первого слоя conv\n",
    "     - conv_w2: переменная tf.Variable, содержащая веса для второго слоя conv\n",
    "     - conv_b2: переменная tf.Variable, содержащая смещения для второго слоя conv\n",
    "     - fc_w: переменная tf.Variable, содержащая веса  для полносвязанного слоя\n",
    "     - fc_b: переменная tf.Variable, содержащая смещения для полносвязанного слоя\n",
    "    \"\"\"\n",
    "    \n",
    "    params = None\n",
    "    \n",
    "    ############################################################################\n",
    "    # ЗАДАНИЕ: Инициализаровать параметры 3-х слойной сети                     #\n",
    "    ############################################################################\n",
    "    # *****НАЧАЛО ВАШЕГО КОДА (НЕ УДАЛЯЙТЕ/НЕ МОДИФИЦИРУЙТЕ ЭТУ СТРОКУ)*****\n",
    "    \n",
    "    conv_w1 = tf.Variable(kaiming_normal((5, 5, 3, 32)))\n",
    "    conv_b1 = tf.Variable(tf.zeros([32]))\n",
    "    conv_w2 = tf.Variable(kaiming_normal((3, 3, 32, 16)))\n",
    "    conv_b2 = tf.Variable(tf.zeros([16]))\n",
    "    fc_w = tf.Variable(kaiming_normal((32 * 32 * 16, 10)))\n",
    "    fc_b = tf.Variable(tf.zeros([10]))\n",
    "    params = [conv_w1, conv_b1, conv_w2, conv_b2, fc_w, fc_b]\n",
    "\n",
    "    # *****КОНЕЦ ВАШЕГО КОДА (НЕ УДАЛЯЙТЕ/НЕ МОДИФИЦИРУЙТЕ ЭТУ СТРОКУ)*****\n",
    "    ############################################################################\n",
    "    #                              КОНЕЦ ВАШЕГО КОДА                           #\n",
    "    ############################################################################\n",
    "    \n",
    "    return params\n",
    "\n",
    "# Обучение трехслойной сверточной модели\n",
    "learning_rate = 3e-3\n",
    "train_part2(three_layer_convnet, three_layer_convnet_init, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "# Часть III: Использование API объекта tf.keras.Model\n",
    "Реализация нейронной сети с использованием базового API TensorFlow - это хороший способ понять, как работает TensorFlow, но  не удобно - нам пришлось вручную отслеживать все тензоры, представляющие обучаемые параметры. Это не сложно для небольшой сети, но усложняется при большой модели нейросети.\n",
    "\n",
    "К счастью, TensorFlow 2.0 имеет API  высокого уровня, такой как `tf.keras`, который упрощает создание моделей из модульных объектно-ориентированных слоев. Кроме того, TensorFlow 2.0 использует  режим \"eager\" выполнения, которое выполняет операции немедленно. Это облегчает написание и отладку моделей и сокращает стандартный код. \n",
    "\n",
    "В этой части блокнота мы определим модели нейронных сетей, используя API интерфейс высокого уровня `tf.keras.Model`. Чтобы реализовать свою собственную модель, вам необходимо сделать следующее:\n",
    "\n",
    "1. Определите новый класс, который является подклассом `tf.keras.model`. Присвойте вашему классу соответствующее имя, которое указывает его назначение, например `TwoLayerFC` или `ThreeLayerConvNet`.\n",
    "2. В конструкторе `__init __ ()` нового класса определите все слои, которые вам нужны, в виде атрибутов класса. Модуль `tf.keras.layers` предоставляет множество обобщенных нейросетевых слоёв, таких как `tf.keras.layers.Dense` для полносвязанных слоев или  `tf.keras.layers.Conv2D` для сверточных слоев. Внутри эти слои будут автоматически создавать  `Variable` тензоры  для любых обучаемых параметров. **Предупреждение**: Не забудьте вызвать `super(YourModelName, self).__init__()`  в качестве первой строки конструктора нового класса!\n",
    "3. Реализуйте метод `call ()` для вашего класса; он осуществляет прямое распространение для вашей модели и определяет *связи*  слоев вашей сети. Слои, определенные в `__init __ ()`, применяются в  `__call __ ()`,  виде функций, которые преобразуют входные тензоры в выходные тензоры. Не определяйте новые слои в `call ()`; любые слои, которые вы хотите использовать при прямом распространении, должны быть определены в `__init __ ()`.\n",
    "\n",
    "После того, как вы определили свой подкласс `tf.keras.Model`, вы можете создать его экземпляр и использовать его подобно модели из части II.\n",
    "\n",
    "### Создание  модели 2-х слойной сети в виде подкласса tf.keras.Model\n",
    "\n",
    "Ниже приведен конкретный пример использования API `tf.keras.Model` для определения двухслойной сети. \n",
    "\n",
    "Мы используем объект `Initializer` для задания начальных значений обучаемых параметров слоев; в частности, `tf.initializers.VarianceScaling` соответствует методу инициализации, использованному в функции `kaiming_normal` в части II. \n",
    "\n",
    "Объект `tf.keras.layers.Dense`  используется для представления двух полносвязанных слоев модели. В дополнение к умножению входа на весовую матрицу и добавлению вектора смещения, этот слой также может обеспечить применение нелинейности. Для первого слоя используется функция активации ReLU, для этого конструктору передается параметр `activation='relu'`; второй слой использует `softmax` функцию активации. В заключение мы определяем слой `tf.keras.layers.Flatten` для \"уплощения\" выхода предыдущего слоя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 10)\n"
     ]
    }
   ],
   "source": [
    "class TwoLayerFC(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(TwoLayerFC, self).__init__()  \n",
    "        # определяем функцию-инициализатор весов (нормальное распределение с дисперсией = sqrt(scale /fan_in))\n",
    "        initializer = tf.initializers.VarianceScaling(scale=2.0)\n",
    "        # описываем набор слоев сети \n",
    "        self.fc1 = tf.keras.layers.Dense(hidden_size, activation='relu', \n",
    "                                   kernel_initializer=initializer)\n",
    "        self.fc2 = tf.keras.layers.Dense(num_classes, activation='softmax',\n",
    "                                   kernel_initializer=initializer)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "    \n",
    "    def call(self, x, training=False):\n",
    "        # описываем порядок взаимодействия слоев при прямом распространении\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def test_TwoLayerFC():\n",
    "    \"\"\" Небольшой тест для модели TwoLayerFC\"\"\"\n",
    "    input_size, hidden_size, num_classes = 50, 42, 10\n",
    "    x = tf.zeros((64, input_size))\n",
    "    # создаем экземляр класса - модель 2-х слойной сети\n",
    "    model = TwoLayerFC(hidden_size, num_classes)\n",
    "    # выполняем прямое распространение \n",
    "    with tf.device(device):\n",
    "        scores = model(x)\n",
    "        print(scores.shape) # просто проверяем форму выходных данных\n",
    "        \n",
    "test_TwoLayerFC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание  модели 3-х слойной Conv сети в виде подкласса tf.keras.Model \n",
    "Реализуйте трехслойную сверточную сеть ConvNet с использованием API `tf.keras.Model`. Модель должна иметь ту же архитектуру, что и ранее в части II:\n",
    "\n",
    "1. Сверточный слой с 5 х 5 фильтрами и  с дополнением нулями Р=2;\n",
    "2. Нелинейность ReLU;\n",
    "3. Сверточный слой с 3 x 3 фильтрами и  с дополнением нулями Р=1;\n",
    "4. Нелинейность ReLU;\n",
    "5. Полносвязный слой, формирующий рейтинги  классов scores;\n",
    "6. Softmax нелинейность.\n",
    "\n",
    "Вы должны инициализировать веса сети, используя тот же метод инициализации, который использовался в двухслойной сети выше.\n",
    "\n",
    "**Совет**: обратитесь к документации для `tf.keras.layers.Conv2D` и `tf.keras.layers.Dense`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeLayerConvNet(tf.keras.Model):\n",
    "    def __init__(self, channel_1, channel_2, num_classes):\n",
    "        super(ThreeLayerConvNet, self).__init__()\n",
    "        \n",
    "        ########################################################################\n",
    "        # ЗАДАНИЕ:                                                             #\n",
    "        # экземпляры объектов слоя,используемые при прямом распространении     #\n",
    "        # Реализуйте метод __init__ для трехслойной ConvNet. Вы должны создать #    \n",
    "        ########################################################################\n",
    "        # *****НАЧАЛО ВАШЕГО КОДА (НЕ УДАЛЯЙТЕ/НЕ МОДИФИЦИРУЙТЕ ЭТУ СТРОКУ)*****\n",
    "        \n",
    "        # 1. Определяем слои модели внутри конструктора init\n",
    "        \n",
    "        # выбираем инициализатор весов слоев\n",
    "        initializer = tf.initializers.VarianceScaling(scale=2.0)\n",
    "        \n",
    "        # определяем слои модели, содержащие обучаемые параметры\n",
    "        self.conv1 = tf.keras.layers.Conv2D(channel_1, (5, 5), strides=1, padding='valid',\n",
    "                                           kernel_initializer=initializer, activation='relu')\n",
    "        self.conv2 = tf.keras.layers.Conv2D(channel_2, (3, 3), strides=1, padding='valid',\n",
    "                                           kernel_initializer=initializer, activation='relu')\n",
    "        self.fc = tf.keras.layers.Dense(num_classes, kernel_initializer=initializer, \n",
    "                                        activation='softmax')\n",
    "\n",
    "        # *****КОНЕЦ ВАШЕГО КОДА (НЕ УДАЛЯЙТЕ/НЕ МОДИФИЦИРУЙТЕ ЭТУ СТРОКУ)*****\n",
    "        ########################################################################\n",
    "        #                             КОНЕЦ ВАШЕГО КОДА                        #\n",
    "        ########################################################################\n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        scores = None\n",
    "        \n",
    "        ########################################################################\n",
    "        # ЗАДАНИЕ: выполнить прямое распространение для 3-х слойной ConvNet.   #\n",
    "        # Используйте объекты слоя, определенные в методе __init__.            #\n",
    "        ########################################################################\n",
    "        # *****НАЧАЛО ВАШЕГО КОДА (НЕ УДАЛЯЙТЕ/НЕ МОДИФИЦИРУЙТЕ ЭТУ СТРОКУ)*****\n",
    "        \n",
    "        # 2. Определяем связи слоев внутри метода call подкласса\n",
    "                \n",
    "        # дополняем x нулями вручную и проходим слой conv1\n",
    "        padding = tf.constant([[0, 0], [2, 2], [2, 2], [0, 0]])\n",
    "        x = tf.pad(x, padding, 'CONSTANT')\n",
    "        x = self.conv1(x)\n",
    "        \n",
    "        # дополняем x нулями вручную и проходим слой conv2\n",
    "        padding = tf.constant([[0, 0], [1, 1], [1, 1], [0, 0]])\n",
    "        x = tf.pad(x, padding, 'CONSTANT')\n",
    "        x = self.conv2(x)\n",
    "        \n",
    "        # прямое прохождение fc слоя\n",
    "        x = tf.keras.layers.Flatten()(x)\n",
    "        scores = self.fc(x)\n",
    "\n",
    "        # *****КОНЕЦ ВАШЕГО КОДА (НЕ УДАЛЯЙТЕ/НЕ МОДИФИЦИРУЙТЕ ЭТУ СТРОКУ)*****\n",
    "        ########################################################################\n",
    "        #                             КОНЕЦ ВАШЕГО КОДА                        #\n",
    "        ########################################################################\n",
    "        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После завершения реализации `ThreeLayerConvNet`  выполните код в ячейке ниже, чтобы убедиться, что ваша реализация  возвращает выходы ожидаемой формы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 10)\n"
     ]
    }
   ],
   "source": [
    "def test_ThreeLayerConvNet():    \n",
    "    channel_1, channel_2, num_classes = 12, 8, 10\n",
    "    model = ThreeLayerConvNet(channel_1, channel_2, num_classes)\n",
    "    with tf.device(device):\n",
    "        x = tf.zeros((64, 3, 32, 32))\n",
    "        scores = model(x)\n",
    "        print(scores.shape)\n",
    "\n",
    "test_ThreeLayerConvNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Организация собственного цикла обучения для моделей в виде подкласса tf.keras.Model (использование tf. GradientTape) \n",
    "\n",
    "Keras имеет простой встроенный цикл обучения (реализуется посредством `model.fit`), но иногда вам требуется большая гибкость  настроек процесса обучения. Ниже приведен пример цикла обучения, реализованного с использованием безотлагательного (eager) исполнения.\n",
    "\n",
    "Для  реализации такого цикла используется объект `tf.GradientTape`, который называется лентой (tape). В основе TensorFlow заложено автоматическое дифференцирование  для реализации обратного распространения. В ходе безотлагательного исполнения tf.GradientTape используется для трассировки операций, которые позже будут применены для вычисления градиентов. \n",
    "\n",
    "TensorFlow 2.0 поставляется с простыми в использовании встроенными метриками в модуле `tf.keras.metrics`. Каждая метрика является объектом, и мы можем использовать `update_state ()` для обновления состояний метрик и `reset_state ()` для очистки всех состояний. Мы можем получить текущее значение метрики, вызвав `result ()` для объекта-метрики."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "def train_part34(model_init_fn, optimizer_init_fn, num_epochs=1, is_training=False):\n",
    "    \"\"\"\n",
    "    Функция, реализующая простой цикл обучения для использования с моделями, \n",
    "    определенными с помощью tf.keras. Она обучает модель на одной эпохе на \n",
    "    обучающем мн-ве CIFAR-10 и периодически проверяет точность на валидационном\n",
    "    мн-ве CIFAR-10.\n",
    "    \n",
    "     Входы:\n",
    "     - model_init_fn: функция создает модель, которую мы хотим обучить: \n",
    "                      model = model_init_fn ()\n",
    "     - optimizer_init_fn: функция создает объект Optimizer, который мы будем \n",
    "                          использовать для оптимизации модели: optimizer = optimizer_init_fn ()\n",
    "     - num_epochs: количество эпох обучения\n",
    "    \n",
    "     Возвращает: ничего не возвращает, но отображает ход обучения\n",
    "      \n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.device(device):\n",
    "        \n",
    "        # Создаем экземпляр объекта для вычисления кросс-энтропийных потерь\n",
    "        loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "        # Cоздаем экземпляры объектов модели и оптимизатора\n",
    "        model = model_init_fn()\n",
    "        optimizer = optimizer_init_fn()\n",
    "        # Создаем экземпляры метрик для этапов обучения и валидации\n",
    "        train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "        train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "        val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "        val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='val_accuracy')\n",
    "        \n",
    "        t = 0 # номер шага итерации    \n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            # Сброс состояний метрик этапа обучения\n",
    "            train_loss.reset_states()\n",
    "            train_accuracy.reset_states()\n",
    "            \n",
    "            # Итерации по обучающему множеству\n",
    "            for x_np, y_np in train_dset:\n",
    "                with tf.GradientTape() as tape: \n",
    "                    \n",
    "                    # Используем model для вычислений прямого пути и потерь\n",
    "                    scores = model(x_np, training=is_training)\n",
    "                    loss = loss_fn(y_np, scores)\n",
    "                    \n",
    "                    # Вычисляем градиенты и обновляем обучаемые переменные\n",
    "                    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "                    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "                    \n",
    "                    # Обновляем метрики\n",
    "                    train_loss.update_state(loss)\n",
    "                    train_accuracy.update_state(y_np, scores)\n",
    "                    \n",
    "                    # Мониторинг процесса обучения через заданное число итераций\n",
    "                    if t % print_every == 0:\n",
    "                        val_loss.reset_states()\n",
    "                        val_accuracy.reset_states()\n",
    "                        \n",
    "                        # Предсказание и оценка потерь на множестве валидации\n",
    "                        for test_x, test_y in val_dset:\n",
    "                            # В ходе валидации  устанавливаем training в False\n",
    "                            prediction = model(test_x, training=False)\n",
    "                            t_loss = loss_fn(test_y, prediction)\n",
    "                            # Обновляем состояния метрик валидации\n",
    "                            val_loss.update_state(t_loss)\n",
    "                            val_accuracy.update_state(test_y, prediction)\n",
    "                        \n",
    "                        template = 'Итерация {}, Эпоха {}, Потери: {}, Точность: {}, Val Потери: {}, Val Точность: {}'\n",
    "                        print (template.format(t, epoch+1,\n",
    "                                             train_loss.result(),\n",
    "                                             train_accuracy.result()*100,\n",
    "                                             val_loss.result(),\n",
    "                                             val_accuracy.result()*100))\n",
    "                    t += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение двухслойной модели, реализованной в виде подкласса  tf.keras.Model\n",
    "Теперь мы можем использовать описанные выше инструменты для обучения двухслойной сети на множестве CIFAR-10. При вызове модели необходимо определить функции `model_init_fn` и` optimizer_init_fn`, которые задают используемую модель и оптимизатор, соответственно. Обучение модели будем выполнять с использованием стохастического градиентного спуска, поэтому в качестве оптимизатора используем функцию `tf.keras.optimizers.SGD`. \n",
    "\n",
    "Здесь не требуется настраивать какие-либо гиперпараметры, но вы должны достичь точности валидации выше 40% после одной эпохи обучения. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Итерация 0, Эпоха 1, Потери: 3.2795963287353516, Точность: 7.8125, Val Потери: 2.9306931495666504, Val Точность: 10.699999809265137\n",
      "Итерация 100, Эпоха 1, Потери: 2.2585976123809814, Точность: 27.923885345458984, Val Потери: 1.8452696800231934, Val Точность: 38.10000228881836\n",
      "Итерация 200, Эпоха 1, Потери: 2.090008020401001, Точность: 31.941852569580078, Val Потери: 1.8430598974227905, Val Точность: 40.099998474121094\n",
      "Итерация 300, Эпоха 1, Потери: 2.0071892738342285, Точность: 33.79360580444336, Val Потери: 1.8492001295089722, Val Точность: 36.70000076293945\n",
      "Итерация 400, Эпоха 1, Потери: 1.936073660850525, Точность: 35.750465393066406, Val Потери: 1.6857361793518066, Val Точность: 42.89999771118164\n",
      "Итерация 500, Эпоха 1, Потери: 1.8909555673599243, Точность: 36.76708984375, Val Потери: 1.6346993446350098, Val Точность: 43.5\n",
      "Итерация 600, Эпоха 1, Потери: 1.8622238636016846, Точность: 37.69239044189453, Val Потери: 1.6826175451278687, Val Точность: 42.69999694824219\n",
      "Итерация 700, Эпоха 1, Потери: 1.8366121053695679, Точность: 38.31357192993164, Val Потери: 1.6179836988449097, Val Точность: 45.0\n"
     ]
    }
   ],
   "source": [
    "hidden_size, num_classes = 4000, 10\n",
    "learning_rate = 1e-2\n",
    "\n",
    "def model_init_fn():\n",
    "    return TwoLayerFC(hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    return tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение трехслойной модели ConvNet, реализованной в виде подкласса  tf.keras.Model\n",
    "Здесь необходимо использовать инструменты, которые мы определили выше для обучения трехслойной сверточной сети ConvNet на множестве  CIFAR-10. Модель ConvNet должна содержать 32 фильтра в первом сверточном слое и 16 фильтров во втором слое.\n",
    "\n",
    "Для обученияи модели следует использовать алгоритм градиентного спуска с моментом Нестерова с  коэффициентом затухания  0,9.\n",
    "\n",
    "Вам не требуется выполнять какую-либо настройку гиперпараметров, но вы должны достичь точности вадидации более 50% после одной эпохи обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Итерация 0, Эпоха 1, Потери: 2.5686373710632324, Точность: 12.5, Val Потери: 3.1889030933380127, Val Точность: 8.0\n",
      "Итерация 100, Эпоха 1, Потери: 1.908718466758728, Точность: 33.02908706665039, Val Потери: 1.6891686916351318, Val Точность: 41.400001525878906\n",
      "Итерация 200, Эпоха 1, Потери: 1.74983811378479, Точность: 38.53389358520508, Val Потери: 1.5006937980651855, Val Точность: 49.099998474121094\n",
      "Итерация 300, Эпоха 1, Потери: 1.6562060117721558, Точность: 41.66839599609375, Val Потери: 1.4256306886672974, Val Точность: 51.099998474121094\n",
      "Итерация 400, Эпоха 1, Потери: 1.5833784341812134, Точность: 44.08510208129883, Val Потери: 1.3670945167541504, Val Точность: 51.400001525878906\n",
      "Итерация 500, Эпоха 1, Потери: 1.5328413248062134, Точность: 45.8863525390625, Val Потери: 1.3284120559692383, Val Точность: 53.79999923706055\n",
      "Итерация 600, Эпоха 1, Потери: 1.498449683189392, Точность: 47.04398727416992, Val Потери: 1.3077772855758667, Val Точность: 54.29999923706055\n",
      "Итерация 700, Эпоха 1, Потери: 1.4698565006256104, Точность: 48.07194900512695, Val Потери: 1.2767364978790283, Val Точность: 55.19999694824219\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 3e-3\n",
    "channel_1, channel_2, num_classes = 32, 16, 10\n",
    "\n",
    "def model_init_fn():\n",
    "    model = None\n",
    "    \n",
    "    ############################################################################\n",
    "    # ЗАДАНИЕ: определите  model в виде 3-х слойной conv сети                  #\n",
    "    ############################################################################\n",
    "    # *****НАЧАЛО ВАШЕГО КОДА (НЕ УДАЛЯЙТЕ/НЕ МОДИФИЦИРУЙТЕ ЭТУ СТРОКУ)*****\n",
    "   \n",
    "    model = ThreeLayerConvNet(channel_1, channel_2, num_classes)\n",
    "\n",
    "    # *****КОНЕЦ ВАШЕГО КОДА (НЕ УДАЛЯЙТЕ/НЕ МОДИФИЦИРУЙТЕ ЭТУ СТРОКУ)*****\n",
    "    ############################################################################\n",
    "    #                           КОНЕЦ ВАШЕГО КОДА                              #\n",
    "    ############################################################################\n",
    "    \n",
    "    return model\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    optimizer = None\n",
    "    \n",
    "    ############################################################################\n",
    "    # ЗАДАНИЕ: Вызовите функцию, определящую optimizer                         #\n",
    "    ############################################################################\n",
    "    # *****НАЧАЛО ВАШЕГО КОДА (НЕ УДАЛЯЙТЕ/НЕ МОДИФИЦИРУЙТЕ ЭТУ СТРОКУ)*****\n",
    "   \n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate,\n",
    "                                       momentum=0.9, nesterov=True)\n",
    "\n",
    "    # *****КОНЕЦ ВАШЕГО КОДА (НЕ УДАЛЯЙТЕ/НЕ МОДИФИЦИРУЙТЕ ЭТУ СТРОКУ)*****\n",
    "    ############################################################################\n",
    "    #                           КОНЕЦ ВАШЕГО КОДА                              #\n",
    "    ############################################################################\n",
    "    \n",
    "    return optimizer\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Часть IV:  Последовательный и функциональные API Keras\n",
    "В третьей части мы представили API `tf.keras.Model`, который позволяет Вам определять модели с любым количеством доступных для обучения слоев и с произвольными связями между слоями.\n",
    "\n",
    "Однако для многих случаев такая гибкость модели не требуется - многие модели могут быть выражены в виде последовательного стека слоев, при этом выходные данные каждого слоя передаются на следующий уровень в качестве входных данных. Если ваша модель соответствует этому шаблону, то существует еще более простой способ определить вашу модель: использовать `tf.keras.Sequential`. Вам не нужно писать какие-либо пользовательские классы; Вы просто вызываете конструктор `tf.keras.Sequential` со списком, содержащим последовательность объектов слоя.\n",
    "\n",
    "Одна сложность с `tf.keras.Sequential` заключается в том, что вы должны определить форму входных данных для модели, передав значение` input_shape` первого слоя в вашей модели.\n",
    "\n",
    "### Keras Sequential API: двухслойная сеть\n",
    "В этом подразделе мы перепишем полносвязанную двухслойную сеть, используя `tf.keras.Sequential`, и обучим ее, используя обучающий цикл, определенный выше.\n",
    "\n",
    "Здесь не требуется выполнять какую-либо настройку гиперпараметров, но вы должны достичь точности валидации более 40% после одной эпохи обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Итерация 0, Эпоха 1, Потери: 2.88728404045105, Точность: 9.375, Val Потери: 2.922335147857666, Val Точность: 12.600000381469727\n",
      "Итерация 100, Эпоха 1, Потери: 2.2514092922210693, Точность: 28.29517364501953, Val Потери: 1.9011563062667847, Val Точность: 39.0\n",
      "Итерация 200, Эпоха 1, Потери: 2.0886340141296387, Точность: 32.182838439941406, Val Потери: 1.8302842378616333, Val Точность: 40.400001525878906\n",
      "Итерация 300, Эпоха 1, Потери: 2.0088131427764893, Точность: 34.07910919189453, Val Потери: 1.8670812845230103, Val Точность: 39.099998474121094\n",
      "Итерация 400, Эпоха 1, Потери: 1.9351927042007446, Точность: 35.86346435546875, Val Потери: 1.733605146408081, Val Точность: 41.20000076293945\n",
      "Итерация 500, Эпоха 1, Потери: 1.8898835182189941, Точность: 37.0228271484375, Val Потери: 1.6667338609695435, Val Точность: 41.29999923706055\n",
      "Итерация 600, Эпоха 1, Потери: 1.8600142002105713, Точность: 37.94717025756836, Val Потери: 1.7060847282409668, Val Точность: 41.900001525878906\n",
      "Итерация 700, Эпоха 1, Потери: 1.834078073501587, Точность: 38.50080490112305, Val Потери: 1.6385880708694458, Val Точность: 45.69999694824219\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-2\n",
    "\n",
    "def model_init_fn():\n",
    "    input_shape = (32, 32, 3)\n",
    "    hidden_layer_size, num_classes = 4000, 10\n",
    "    initializer = tf.initializers.VarianceScaling(scale=2.0)\n",
    "    # список слоев модели\n",
    "    layers = [\n",
    "        tf.keras.layers.Flatten(input_shape=input_shape),\n",
    "        tf.keras.layers.Dense(hidden_layer_size, activation='relu',\n",
    "                              kernel_initializer=initializer),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax', \n",
    "                              kernel_initializer=initializer),\n",
    "    ]\n",
    "    # coздаем модель в виде экземпляра класса tf.keras.Sequential\n",
    "    model = tf.keras.Sequential(layers)\n",
    "    return model\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    return tf.keras.optimizers.SGD(learning_rate=learning_rate) \n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras: Обучение  Sequential  модели с помощью встроенного метода model.fit\n",
    "\n",
    "В предыдущих примерах мы использовали собственный настраиваемый цикл обучения для обучения моделей ( `train_part34`). Написание собственного цикла обучения требуется только в том случае, если вам нужна большая гибкость и контроль во время обучения Вашей модели. Альтернативно, Вы также можете использовать встроенные API, такие как `tf.keras.Model.fit ()` и `tf.keras.Model.evaluate`, чтобы обучать и оценивать модель. Также не забывайте  настраивать свою модель для обучения, вызвав `tf.keras.Model.compile`.\n",
    "\n",
    "Здесь не требуется выполнять какую-либо настройку гиперпараметров, но вы должны достичь точности вадидации более 42% после одной эпохи обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 49000 samples, validate on 1000 samples\n",
      "49000/49000 [==============================] - 25s 514us/sample - loss: 1.8285 - sparse_categorical_accuracy: 0.3854 - val_loss: 1.6147 - val_sparse_categorical_accuracy: 0.4460\n",
      "10000/10000 [==============================] - 4s 353us/sample - loss: 1.5909 - sparse_categorical_accuracy: 0.4564\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.590888639831543, 0.4564]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model_init_fn()\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=[tf.keras.metrics.sparse_categorical_accuracy])\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=1, validation_data=(X_val, y_val))\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функциональный API:\n",
    "### Демонстрация двухслойной сети\n",
    "\n",
    "В предыдущем разделе видели, как мы можем использовать `tf.keras.Sequential` для быстрого построения простых моделей. Но это достигается за счет потери гибкости.\n",
    "\n",
    "Часто нам приходится писать сложные модели, которые имеют непоследовательные потоки данных: у слоя может быть **несколько входов и / или выходов**, таких как объединение выходных данных двух предыдущих слоев для подачи на вход третьего слоя! (примеры: шунтирование слоев и полносвязанные  блоки.)\n",
    "\n",
    "В таких случаях мы можем использовать функциональный API Keras для написания моделей со сложными топологиями, такими как:\n",
    "\n",
    "  1. Модели с несколькими входами\n",
    "  2. Модели с несколькими выходами\n",
    "  3. Модели с общими слоями (один и тот же слой вызывается несколько раз)\n",
    "  4. Модели с непоследовательными потоками данных (например, шунтирующие соединения)\n",
    "\n",
    "Написание модели с помощью Functional API требует от нас создания экземпляра `tf.keras.Model` и явной записи входных и выходных тензоров в виде параметров слоев для этой модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 10)\n"
     ]
    }
   ],
   "source": [
    "def two_layer_fc_functional(input_shape, hidden_size, num_classes):  \n",
    "    initializer = tf.initializers.VarianceScaling(scale=2.0)\n",
    "    # описание слоев в виде функций с аргументами\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    flattened_inputs = tf.keras.layers.Flatten()(inputs)\n",
    "    fc1_output = tf.keras.layers.Dense(hidden_size, activation='relu',\n",
    "                                 kernel_initializer=initializer)(flattened_inputs)\n",
    "    scores = tf.keras.layers.Dense(num_classes, activation='softmax',\n",
    "                             kernel_initializer=initializer)(fc1_output)\n",
    "\n",
    "    # Создание экземпляра модели с заданными входами и выходами\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=scores)\n",
    "    return model\n",
    "\n",
    "def test_two_layer_fc_functional():\n",
    "    \"\"\" Небольшой тест для модели TwoLayerFC, определенной выше  \"\"\"\n",
    "    input_size, hidden_size, num_classes = 50, 42, 10\n",
    "    input_shape = (50,)\n",
    "    \n",
    "    x = tf.zeros((64, input_size))\n",
    "    model = two_layer_fc_functional(input_shape, hidden_size, num_classes)\n",
    "    \n",
    "    with tf.device(device):\n",
    "        scores = model(x)\n",
    "        print(scores.shape)\n",
    "        \n",
    "test_two_layer_fc_functional()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функциональный API Keras: обучение двухслойной сети\n",
    "Теперь вы можете обучить эту двухслойную сеть, построенную с использованием функционального API.\n",
    "\n",
    "Вам не нужно выполнять выбор гиперпараметров, но вы должны достичь валидационной точности выше 40% после обучения в течение одной эпохи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Итерация 0, Эпоха 1, Потери: 2.9675796031951904, Точность: 17.1875, Val Потери: 2.992048501968384, Val Точность: 12.800000190734863\n",
      "Итерация 100, Эпоха 1, Потери: 2.2617239952087402, Точность: 28.418933868408203, Val Потери: 1.8459010124206543, Val Точность: 38.900001525878906\n",
      "Итерация 200, Эпоха 1, Потери: 2.092485189437866, Точность: 32.26834487915039, Val Потери: 1.8734703063964844, Val Точность: 38.400001525878906\n",
      "Итерация 300, Эпоха 1, Потери: 2.010263681411743, Точность: 34.131019592285156, Val Потери: 1.8420852422714233, Val Точность: 38.400001525878906\n",
      "Итерация 400, Эпоха 1, Потери: 1.9396835565567017, Точность: 35.97646713256836, Val Потери: 1.7187179327011108, Val Точность: 42.599998474121094\n",
      "Итерация 500, Эпоха 1, Потери: 1.8951425552368164, Точность: 36.979164123535156, Val Потери: 1.6734684705734253, Val Точность: 43.20000076293945\n",
      "Итерация 600, Эпоха 1, Потери: 1.8634939193725586, Точность: 37.902976989746094, Val Потери: 1.6827170848846436, Val Точность: 41.80000305175781\n",
      "Итерация 700, Эпоха 1, Потери: 1.8364790678024292, Точность: 38.663516998291016, Val Потери: 1.6433539390563965, Val Точность: 43.900001525878906\n"
     ]
    }
   ],
   "source": [
    "input_shape = (32, 32, 3)\n",
    "hidden_size, num_classes = 4000, 10\n",
    "learning_rate = 1e-2\n",
    "\n",
    "def model_init_fn():\n",
    "    return two_layer_fc_functional(input_shape, hidden_size, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    return tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "train_part34(model_init_fn, optimizer_init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Часть IV: CIFAR-10 - открытая задача\n",
    "\n",
    "В этом разделе Вы можете поэкспериментировать с любой архитектурой ConvNet, которую Вы хотели бы использовать для клаcсификации изображений множества CIFAR-10.\n",
    "\n",
    "Вы должны поэкспериментировать с архитектурами, гиперпараметрами, функциями потерь, регуляризацией или чем-либо еще, что Вы посчитаете важным для обучения модели, которая достигает **не менее 70%** точности на **валидационном** множестве в течение 10 эпох. Вы можете использовать встроенную функцию обучения, функцию `train_part34`, определенную выше, или Вы можете реализовать свой собственный цикл обучения.\n",
    "\n",
    "Опишите Ваши эксперименты в конце этого блокнота.\n",
    "\n",
    "### С чем Вам следует экспериментировать:\n",
    "- **Размер фильтра**: Выше мы использовали 5x5 и 3x3; это оптимально?\n",
    "- **Количество фильтров**: Выше мы использовали 16 и 32 фильтра. Будет ли большее или меньшее число фильтров лучше?\n",
    "- **Пулинг**: Мы не использовали никакого пулинга выше. Будет ли пулинг улучшать модель?\n",
    "- **Нормализация**: улучшится ли ваша модель при использовании блочной нормализации, нормализации на слое, нормализации группы или какой-либо иной стратегии нормализации?\n",
    "- **Архитектура**: В приведенной выше ConvNet имеется только три слоя обучаемых параметров. Будет ли более глубокая модель работать лучше?\n",
    "- **Глобальный усредняющий пулинг**: вместо \"уплощения\" данных после последнего сверточного слоя будет ли глобальный усредняющий пулинг лучше? Эта стратегия используется, например, в  сети Google Inception  и в Остаточных (Residual) сетях.\n",
    "- **Регуляризация**: Будет ли какая-то регуляризация повышать эффективность сети? Может быть, затухание весов или dropout?\n",
    "\n",
    "### Замечание: Блочная нормализация/ Dropout\n",
    "Если вы используете Batch Normalization и Dropout, не забудьте передать `is_training = True`, если вы используете функцию` train_part34 () `. Слои BatchNorm и Dropout ведут себя по-разному во время обучения и предсказания. `training` - это специальный ключевой аргумент, зарезервированный для этой цели в любой `tf.keras.Model` функции` call () `. \n",
    "\n",
    "### Подсказки для обучения\n",
    "Для каждой сетевой архитектуры, с которой Вы экспериментируете, Вы должны выбрать скорость обучения и другие гиперпараметры. При этом есть несколько важных моментов, которые нужно иметь в виду:\n",
    "\n",
    "- Если параметры работают хорошо, вы должны видеть улучшение в течение нескольких сотен итераций;\n",
    "- Помните о грубой и тонкой настройке гиперпараметров: начните с проверки  гиперпараметров в широком диапазоне   всего на нескольких обучающих итерациях, чтобы найти комбинации параметров, которые вообще работают;\n",
    "- После того, как вы найдете несколько наборов параметров, которые работают,  найдите их более точные значения. Возможно, вам придется проводить обучение при большом числе эпох.\n",
    "- Вы должны использовать валидационное множество для поиска гиперпараметров.\n",
    "\n",
    "\n",
    "### Стремимся к лучшему\n",
    "Если Вы вдохновились, то имеются много других функций, которые можно реализовать, чтобы попытаться повысить эффективность. Вы **не обязаны** их реализовывать, но не упустите шанс, если у вас есть время!\n",
    "\n",
    "- Альтернативные оптимизаторы: вы можете попробовать Адам, Адаград, RMSprop и т. д.;\n",
    "- Альтернативные функции активации, такие как  ReLU с утечкой, параметрическое ReLU, ELU или MaxOut;\n",
    "- Ансамбли моделей;\n",
    "- Расширение набора данных;\n",
    "- Новые архитектуры.\n",
    "\n",
    "  - [ResNets](https://arxiv.org/abs/1512.03385) , где вход  предыдущего слоя добавляется к выходу.\n",
    "  - [DenseNets](https://arxiv.org/abs/1608.06993) , где входы предыдущих слоев объединяются вместе.\n",
    "  - [Этот блог содержит подробный обзор](https://chatbotslife.com/resnets-highwaynets-and-densenets-oh-my-9bb15918ee32)\n",
    "  \n",
    "### Успешного обучения! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Итерация 0, Эпоха 1, Потери: 3.3723158836364746, Точность: 10.9375, Val Потери: 4.072357177734375, Val Точность: 12.700000762939453\n",
      "Итерация 700, Эпоха 1, Потери: 1.1808515787124634, Точность: 58.603782653808594, Val Потери: 0.9286485910415649, Val Точность: 68.80000305175781\n",
      "Итерация 1400, Эпоха 2, Потери: 0.7731051445007324, Точность: 72.99458312988281, Val Потери: 0.8115264177322388, Val Точность: 73.29999542236328\n",
      "Итерация 2100, Эпоха 3, Потери: 0.579459011554718, Точность: 79.87149047851562, Val Потери: 0.804970383644104, Val Точность: 71.9000015258789\n",
      "Итерация 2800, Эпоха 4, Потери: 0.431369811296463, Точность: 85.06150817871094, Val Потери: 0.8356218338012695, Val Точность: 74.29999542236328\n",
      "Итерация 3500, Эпоха 5, Потери: 0.30733248591423035, Точность: 89.49156188964844, Val Потери: 0.8603623509407043, Val Точность: 73.5999984741211\n",
      "Итерация 4200, Эпоха 6, Потери: 0.22446732223033905, Точность: 92.26751708984375, Val Потери: 0.9268920421600342, Val Точность: 74.5\n",
      "Итерация 4900, Эпоха 7, Потери: 0.1578272134065628, Точность: 94.64651489257812, Val Потери: 1.0417580604553223, Val Точность: 74.9000015258789\n",
      "Итерация 5600, Эпоха 8, Потери: 0.1230623722076416, Точность: 95.72437286376953, Val Потери: 0.9979849457740784, Val Точность: 77.10000610351562\n",
      "Итерация 6300, Эпоха 9, Потери: 0.09475523978471756, Точность: 96.79370880126953, Val Потери: 1.0851610898971558, Val Точность: 76.70000457763672\n",
      "Итерация 7000, Эпоха 10, Потери: 0.08891887217760086, Точность: 96.9918212890625, Val Потери: 1.2245898246765137, Val Точность: 74.80000305175781\n",
      "Итерация 7700, Эпоха 11, Потери: 0.08517992496490479, Точность: 97.256103515625, Val Потери: 1.355396032333374, Val Точность: 74.19999694824219\n",
      "Итерация 8400, Эпоха 11, Потери: 0.07864941656589508, Точность: 97.2967300415039, Val Потери: 1.2533677816390991, Val Точность: 74.19999694824219\n",
      "Итерация 9100, Эпоха 12, Потери: 0.06402935087680817, Точность: 97.81713104248047, Val Потери: 1.3053853511810303, Val Точность: 73.5\n",
      "Итерация 9800, Эпоха 13, Потери: 0.0531034879386425, Точность: 98.22198486328125, Val Потери: 1.299355387687683, Val Точность: 74.9000015258789\n",
      "Итерация 10500, Эпоха 14, Потери: 0.06289999186992645, Точность: 97.76416015625, Val Потери: 1.2650337219238281, Val Точность: 75.9000015258789\n",
      "Итерация 11200, Эпоха 15, Потери: 0.05370917543768883, Точность: 98.12631225585938, Val Потери: 1.2956502437591553, Val Точность: 76.9000015258789\n"
     ]
    }
   ],
   "source": [
    "class CustomConvNet(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CustomConvNet, self).__init__()\n",
    "        \n",
    "        ############################################################################\n",
    "        # ЗАДАНИЕ: Постройте модель, которая хорошо работает на CIFAR-10           #\n",
    "        ############################################################################\n",
    "        # *****НАЧАЛО ВАШЕГО КОДА (НЕ УДАЛЯЙТЕ/НЕ МОДИФИЦИРУЙТЕ ЭТУ СТРОКУ)*****\n",
    "        \n",
    "        initializer = tf.initializers.VarianceScaling(scale=2.0)\n",
    "        self.conv1 = tf.keras.layers.Conv2D(64, (3, 3), strides=1, padding='same',\n",
    "                                           activation=tf.nn.relu, kernel_initializer=initializer)\n",
    "        self.pool1 = tf.keras.layers.MaxPool2D((2, 2))\n",
    "        self.norm1 = tf.keras.layers.BatchNormalization(axis=-1)\n",
    "        \n",
    "        self.conv2 = tf.keras.layers.Conv2D(128, (3, 3), strides=1, padding='same',\n",
    "                                           activation=tf.nn.relu, kernel_initializer=initializer)\n",
    "        self.pool2 = tf.keras.layers.MaxPool2D((2, 2))\n",
    "        self.norm2 = tf.keras.layers.BatchNormalization(axis=-1)\n",
    "        \n",
    "        self.conv3 = tf.keras.layers.Conv2D(128, (3, 3), strides=1, padding='same',\n",
    "                                           activation=tf.nn.relu, kernel_initializer=initializer)\n",
    "        self.pool3 = tf.keras.layers.MaxPool2D((2, 2))\n",
    "        self.norm3 = tf.keras.layers.BatchNormalization(axis=-1)\n",
    "        \n",
    "        self.fc1 = tf.keras.layers.Dense(128, activation=tf.nn.relu, kernel_initializer=initializer)\n",
    "        self.normfc1 = tf.keras.layers.BatchNormalization(axis=1)\n",
    "        \n",
    "        self.final = tf.keras.layers.Dense(10, activation='softmax', kernel_initializer=initializer)\n",
    "\n",
    "        # *****КОНЕЦ ВАШЕГО КОДА (НЕ УДАЛЯЙТЕ/НЕ МОДИФИЦИРУЙТЕ ЭТУ СТРОКУ)*****\n",
    "        ############################################################################\n",
    "        #                           КОНЕЦ ВАШЕГО КОДА                              #\n",
    "        ############################################################################\n",
    "        \n",
    "    \n",
    "    def call(self, input_tensor, training=False):\n",
    "        ############################################################################\n",
    "        # ЗАДАНИЕ: Постройте модель, которая хорошо работает на CIFAR-10           #\n",
    "        ############################################################################\n",
    "        # *****НАЧАЛО ВАШЕГО КОДА (НЕ УДАЛЯЙТЕ/НЕ МОДИФИЦИРУЙТЕ ЭТУ СТРОКУ)*****\n",
    "        \n",
    "        x = self.conv1(input_tensor)\n",
    "        x = self.norm1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.norm3(x)\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        x = tf.keras.layers.Flatten()(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.normfc1(x)\n",
    "        \n",
    "        scores = self.final(x)\n",
    "\n",
    "        # *****КОНЕЦ ВАШЕГО КОДА (НЕ УДАЛЯЙТЕ/НЕ МОДИФИЦИРУЙТЕ ЭТУ СТРОКУ)*****\n",
    "        ############################################################################\n",
    "        #                           КОНЕЦ ВАШЕГО КОДА                              #\n",
    "        ############################################################################\n",
    "                \n",
    "        return scores\n",
    "\n",
    "    \n",
    "#device = '/device:GPU:0'   # выберите эту строку, если хотите использовать GPU!\n",
    "device = '/cpu:0'           \n",
    "print_every = 700\n",
    "num_epochs = 15\n",
    "\n",
    "model = CustomConvNet()\n",
    "\n",
    "def model_init_fn():\n",
    "    return CustomConvNet()\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.Adam(learning_rate) \n",
    "\n",
    "# обучение на CPU на одной эпохе займет время !\n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "## Опишите Ваши эксперименты\n",
    "В приведенной ниже ячейке Вы должны объяснить, что Вы делали. Также опишите какие-либо дополнительные особенности, которые Вы использовали, и / или любые графики, которые Вы получили в процессе обучения и тестирования сети."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "ЗАДАНИЕ: Опишите, что Вы делали"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Был написан код для модели свёрточной нейронной сети типа CustomConvNet. Модель классифицирует изображения 10 классов из датасета CIFAR-10.\n",
    "\n",
    "Модель состоит из трёх сверточных слоев. Использован фильтр 3x3, количество фильтров: 64, 128, 128, пулинг MaxPooling размера (2, 2), блочная нормализация (BatchNormalization), глобальный усредняющий пулинг. Регуляризация не используется.\n",
    "\n",
    "Модель классификатора достигает поставленной точности выше 70% послей 2 эпохи обучения, всего было пройдено 15 эпох с финальной точностью 76%. Полученный результат сочтён удовлетворительным."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
