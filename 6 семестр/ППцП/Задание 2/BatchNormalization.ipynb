{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Блочная нормализация\n",
    "Одним из путей повышения эффективности обучения в глубоких сетях является использование более сложных процедур оптимизации, таких как SGD с моментом, RMSProp или Adam. Другая стратегия - изменить архитектуру сети, чтобы улучшить ее обучаемость. Одной из идей в этой области является блочная нормализация, которая была предложена в [3] в 2015 году.\n",
    "\n",
    "Идея относительно проста. Методы машинного обучения имеют тенденцию работать лучше, когда их входные данные состоят из некоррелированных признаков с нулевым средним и единичной дисперсией. При обучении нейронной сети мы можем предварительно обработать данные, прежде чем загружать их в сеть, чтобы явно декореллировать признаки; это гарантирует, что на первый уровень сети поступают данные, которые характеризуются хорошим распределением. Однако, даже если мы предварительно обрабатываем входные данные, выходы более глубоких слоев сети, скорее всего, больше не будут декореллированы и не будут иметь нулевое среднее и единичную дисперсию. Еще хуже то, что во время обучения распределение признаков на каждом уровне сети будет меняться по мере обновления весов каждого уровня.\n",
    "\n",
    "Авторы [3] предполагают, что смещение распределения прзнаков внутри глубоких нейронных сетей затрудняет обучение. Чтобы преодолеть это, [3] предлагается вставить в сеть слои блочной нормализации. Во время обучения слой блочной нормализации использует мини-блок данных для оценки среднего и стандартного отклонения каждого признака. Эти средние затем используются для центрирования и нормализации признаков на  мини-блоке в ходе обучения. Текущее среднее и текущее стандартное отклонение выичсляемые во время обучения можно сохранять, а во время тестирования использовать их для центрирования и нормализации признаков.\n",
    "\n",
    "Возможно, что эта стратегия нормализации снижает репрезентативную емкость сети, так как иногда для некоторых слоев оптимальными будут признаки, которые не обладают нулевым средним и единичной дисперсией. С этой целью слой блочной нормализации включает в себя обучаемые параметры смещения и масштаба для каждого признака.\n",
    "\n",
    "[3] [Sergey Ioffe and Christian Szegedy, \"Batch Normalization: Accelerating Deep Network Training by Reducing\n",
    "Internal Covariate Shift\", ICML 2015.](https://arxiv.org/abs/1502.03167)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выполните начальные установки блокнота.\n",
    "# Внимание! Блокнот работает в версии Python 3.6!\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dlcv.classifiers.fc_net import *\n",
    "from dlcv.data_utils import get_CIFAR10_data\n",
    "from dlcv.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
    "from dlcv.solver import Solver\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # установка размеров графиков по умолчанию\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Для перезагрузки внешних модулей python;\n",
    "# см. http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" возвращает относительную ошибку \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "def print_mean_std(x,axis=0):\n",
    "    print('  среднее: ', x.mean(axis=axis))\n",
    "    print('  стандартное отклонение:  ', x.std(axis=axis))\n",
    "    print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка и предобработка данных CIFAR10\n",
    "\n",
    "data = get_CIFAR10_data()\n",
    "for k, v in data.items():\n",
    "  print('%s: ' % k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Блочная нормализация: прямое распространение\n",
    "В файле `dlcv/layers.py` реализуйте функцию блочной нормализации  `batchnorm_forward` для прямого пути (возможно Вам повезло и функция уже написана). Затем выполните код ниже, чтобы проверить реализацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверьте прямое распространение для режима обучения, проверив средние значения и дисперсии\n",
    "# признаков как до, так  и после блочной нормализации\n",
    "\n",
    "# Моделируем прямое распросранение для 2-х слойной сети\n",
    "np.random.seed(231)\n",
    "N, D1, D2, D3 = 200, 50, 60, 3\n",
    "X = np.random.randn(N, D1)\n",
    "W1 = np.random.randn(D1, D2)\n",
    "W2 = np.random.randn(D2, D3)\n",
    "a = np.maximum(0, X.dot(W1)).dot(W2)\n",
    "\n",
    "print('Перед блочной нормализацией:')\n",
    "print_mean_std(a,axis=0)\n",
    "\n",
    "gamma = np.ones((D3,))\n",
    "beta = np.zeros((D3,))\n",
    "\n",
    "# Средние значения должны быть близки к нулю, а стандартные отклонения – к единице.\n",
    "print('После блочной нормализации (gamma=1, beta=0)')\n",
    "a_norm, _ = batchnorm_forward(a, gamma, beta, {'mode': 'train'})\n",
    "print_mean_std(a_norm,axis=0)\n",
    "\n",
    "gamma = np.asarray([1.0, 2.0, 3.0])\n",
    "beta = np.asarray([11.0, 12.0, 13.0])\n",
    "\n",
    "# Теперь средние значения должны быть близки к beta, а стандартные отклонения — к gamma.\n",
    "print('После блочной нормализации (gamma=', gamma, ', beta=', beta, ')')\n",
    "a_norm, _ = batchnorm_forward(a, gamma, beta, {'mode': 'train'})\n",
    "print_mean_std(a_norm,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверяем прямое распространение через слой BN в режиме тестирования, запустив прямое распростанение \n",
    "# в режиме обучения много раз, чтобы накопить текущие средние значения, а затем\n",
    "# проверяем средние и дисперсии на выходе слоя блочной нормализации в режиме тестирования\n",
    "\n",
    "np.random.seed(231)\n",
    "N, D1, D2, D3 = 200, 50, 60, 3\n",
    "W1 = np.random.randn(D1, D2)\n",
    "W2 = np.random.randn(D2, D3)\n",
    "\n",
    "bn_param = {'mode': 'train'}\n",
    "gamma = np.ones(D3)\n",
    "beta = np.zeros(D3)\n",
    "\n",
    "for t in range(50):\n",
    "  X = np.random.randn(N, D1)\n",
    "  a = np.maximum(0, X.dot(W1)).dot(W2)\n",
    "  batchnorm_forward(a, gamma, beta, bn_param)\n",
    "\n",
    "bn_param['mode'] = 'test'\n",
    "X = np.random.randn(N, D1)\n",
    "a = np.maximum(0, X.dot(W1)).dot(W2)\n",
    "a_norm, _ = batchnorm_forward(a, gamma, beta, bn_param)\n",
    "\n",
    "# Средние должны быть близки к нулю, а стандартные отклонения к 1,\n",
    "# но будут больше зашумлены, чем в режиме обучения\n",
    "print('После блочной нормализации (режим тестирования):')\n",
    "print_mean_std(a_norm,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Блочная нормализация: обратное рапространение\n",
    "Теперь реализуем обратное распространение для  блочной нормализации в функции `batchnorm_backward`.\n",
    "\n",
    "Чтобы реализовать обратный путь, вы должны представить граф вычислений для блочной нормализации  и обратное распространение через каждый из промежуточных узлов. Некоторые промежуточные узлы могут иметь несколько исходящих ветвей; обязательно суммируйте градиенты по этим ветвям на  обратном пути.\n",
    "\n",
    "После реализации функции (возможно Вам повезло и функция уже написана), запустите код ниже, чтобы проверить реализацию обратного распространения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверка градиентов при обратном распространении через слой блочной нормализации\n",
    "np.random.seed(231)\n",
    "N, D = 4, 5\n",
    "x = 5 * np.random.randn(N, D) + 12\n",
    "gamma = np.random.randn(D)\n",
    "beta = np.random.randn(D)\n",
    "dout = np.random.randn(N, D)\n",
    "\n",
    "bn_param = {'mode': 'train'}\n",
    "fx = lambda x: batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
    "fg = lambda a: batchnorm_forward(x, a, beta, bn_param)[0]\n",
    "fb = lambda b: batchnorm_forward(x, gamma, b, bn_param)[0]\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(fx, x, dout)\n",
    "da_num = eval_numerical_gradient_array(fg, gamma.copy(), dout)\n",
    "db_num = eval_numerical_gradient_array(fb, beta.copy(), dout)\n",
    "\n",
    "_, cache = batchnorm_forward(x, gamma, beta, bn_param)\n",
    "dx, dgamma, dbeta = batchnorm_backward(dout, cache)\n",
    "# Относительная ошибка должна быть в пределах 1e-13 and 1e-8\n",
    "print('Ошибка градиента dx: ', rel_error(dx_num, dx))\n",
    "print('Ошибка градиента dgamma: ', rel_error(da_num, dgamma))\n",
    "print('Ошибка градиента dbeta: ', rel_error(db_num, dbeta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Блочная нормализация: альтернативное распространение\n",
    "При реализации обратного распространения используют два подхода. Первый подход заключается в том, чтобы представить граф вычислений в виде простых операций над всеми промежуточными значениями и при построении обратного пути использовать производные для этих элементарных операций. Второй подход заключается в использовании аналитических производных более крупных операций. Например, вы можете получить очень простую формулу для обратного распространения градиентов через сигмовидную функцию.\n",
    "\n",
    "Удивительно, но оказывается, что вы можете сделать аналогичное упрощение и для обратного пути в случае блочной нормализации. При заданном множестве входов $X=\\begin{bmatrix}x_1\\\\x_2\\\\...\\\\x_N\\end{bmatrix}$, \n",
    "мы сначала вычисляем среднее $\\mu=\\frac{1}{N}\\sum_{k=1}^N x_k$ и дисперсию $v=\\frac{1}{N}\\sum_{k=1}^N (x_k-\\mu)^2.$    \n",
    "При вычисленных $\\mu$ and $v$ , мы можем вычислить стандартное отклонение $\\sigma=\\sqrt{v+\\epsilon}$  и нормализовать данные $Y$ с помощью $y_i=\\frac{x_i-\\mu}{\\sigma}.$\n",
    "\n",
    "<img src=\"batchnorm_graph.png\">\n",
    "\n",
    "Решить нашу проблему - это получить $\\frac{\\partial L}{\\partial X}$  из восходящего градиента $\\frac{\\partial L}{\\partial Y}$. Чтобы сделать это, вспомните цепочное правило. \n",
    "Возможно, было бы сложно прямо рассуждать о градиентах над $X$ и $Y$ - попробуйте рассуждать об этом в терминах $x_i$ и $y_i$ сначала.\n",
    "\n",
    "Вам нужно будет придумать вывод для $\\frac{\\partial L}{\\partial x_i}$, полагаясь на цепочное правило, чтобы сначала вычислить промежуточные градиенты $\\frac{\\partial \\mu}{\\partial x_i}, \\frac{\\partial v}{\\partial x_i}, \\frac{\\partial \\sigma}{\\partial x_i},$ затем собрать эти части для вычисления  $\\frac{\\partial y_i}{\\partial x_i}$. Вы должны убедиться, что каждый из промежуточных шагов максимально прост.\n",
    "\n",
    "После этого выполните проверку альтернативной блочной нормализации с помощью функции `batchnorm_backward_alt` и сравните две реализации, выполнив код ниже (возможно Вам повезло и функция уже написана, тогда просто проанализируйте её код). Ваши две реализации обратного распространения должны возвращать почти одинаковые результаты, но альтернативная реализация должна быть немного быстрее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "N, D = 100, 500\n",
    "x = 5 * np.random.randn(N, D) + 12\n",
    "gamma = np.random.randn(D)\n",
    "beta = np.random.randn(D)\n",
    "dout = np.random.randn(N, D)\n",
    "\n",
    "bn_param = {'mode': 'train'}\n",
    "out, cache = batchnorm_forward(x, gamma, beta, bn_param)\n",
    "\n",
    "#t1 = time.time()\n",
    "dx1, dgamma1, dbeta1 = batchnorm_backward(dout, cache)\n",
    "#t2 = time.time()\n",
    "dx2, dgamma2, dbeta2 = batchnorm_backward_alt(dout, cache)\n",
    "#t3 = time.time()\n",
    "\n",
    "print('Отличие в dx : ', rel_error(dx1, dx2))\n",
    "print('Отличие в dgamma: ', rel_error(dgamma1, dgamma2))\n",
    "print('Отличие в dbeta: ', rel_error(dbeta1, dbeta2))\n",
    "#print('Время выполнения для обычной нормализации : %.50fx' % (t2 - t1))\n",
    "#print('Время выполнения для альтернативной нормализации : %.50fx' %  (t3 - t2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Полносвязанные сети с блочной нормализацией\n",
    "\n",
    "Теперь, когда у вас есть рабочая реализация блочной нормализации, вернитесь к  классу FullyConnectedNet в файле dlcv/classifiers/fc_net.py. Измените его реализацию,  добавив блочную нормализацию.\n",
    "\n",
    "Когда флаг `normalization` принимает значение \"batchnorm\" в конструкторе, то необходимо  вставить слой блочной нормализации перед каждой нелинейностью ReLU. Выходы последнего слоя сети не нормализуются. После того, как изменения будут сделаны (возможно Вам повезло и код уже дописан), запустите код ниже для проверки градиента.\n",
    "\n",
    "СОВЕТ. Возможно, вам будет полезно определить дополнительный вспомогательный слой, аналогичный тому, который содержится в файле `dlcv/layer_utils.py`. Если вы решите это сделать, то сделайте это в файле `dlcv/classifiers/fc_net.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверка работоспособности полносвязной 3-х слойной сети со слоями блочной нормализации\n",
    "np.random.seed(231)\n",
    "N, D, H1, H2, C = 2, 15, 20, 30, 10\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=(N,))\n",
    "\n",
    "# Относительные ошибки должны быть в диапазоне 1e-4~1e-10 для W, \n",
    "# Относительные ошибки должны быть в диапазоне 1e-08~1e-10 для b,\n",
    "# Относительные ошибки должны быть в диапазоне  1e-08~1e-09 для beta и gamma.\n",
    "for reg in [0, 3.14]:\n",
    "  print('Выполнение проверки с reg = ', reg)\n",
    "  model = FullyConnectedNet([H1, H2], input_dim=D, num_classes=C,\n",
    "                            reg=reg, weight_scale=5e-2, dtype=np.float64,\n",
    "                            normalization='batchnorm')\n",
    "\n",
    "  loss, grads = model.loss(X, y)\n",
    "  print('Начальные потери: ', loss)\n",
    "\n",
    "  for name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    grad_num = eval_numerical_gradient(f, model.params[name], verbose=False, h=1e-5)\n",
    "    print('%s относительная ошибка: %.2e' % (name, rel_error(grad_num, grads[name])))\n",
    "  if reg == 0: print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Блочная нормализация для глубоких сетей\n",
    "Выполните код ниже, чтобы обучить шестислойную сеть на подмножестве из 1000 примеров обучения как с блочной нормализацией, так и без нее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "\n",
    "# Обучим глубокую нейросеть с использованием блочной нормализации в слоях\n",
    "hidden_dims = [100, 100, 100, 100, 100]\n",
    "\n",
    "num_train = 1000\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "weight_scale = 2e-2\n",
    "# 1. Модель сети  с блочной нормализацией\n",
    "bn_model = FullyConnectedNet(hidden_dims, weight_scale=weight_scale, normalization='batchnorm')\n",
    "# 2. Модель сети без нормализации\n",
    "model = FullyConnectedNet(hidden_dims, weight_scale=weight_scale, normalization=None)\n",
    "\n",
    "print('Обучение сети c BN')\n",
    "bn_solver = Solver(bn_model, small_data,\n",
    "                num_epochs=10, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-3,\n",
    "                },\n",
    "                verbose=True,print_every=20)\n",
    "bn_solver.train()\n",
    "\n",
    "print('Обучение сети без BN')\n",
    "solver = Solver(model, small_data,\n",
    "                num_epochs=10, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-3,\n",
    "                },\n",
    "                verbose=True, print_every=20)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполните код ячейки ниже, чтобы визуализировать результаты для двух вышеперечисленных сетей. Вы должны обнаружить, что использование блочной нормализации помогает сети сходиться намного быстрее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(title, label, baseline, bn_solvers, plot_fn, bl_marker='.', bn_marker='.', labels=None):\n",
    "    \"\"\"функция для построения графиков истории обучения\"\"\"\n",
    "    plt.title(title)\n",
    "    plt.xlabel(label)\n",
    "    bn_plots = [plot_fn(bn_solver) for bn_solver in bn_solvers]\n",
    "    bl_plot = plot_fn(baseline)\n",
    "    num_bn = len(bn_plots)\n",
    "    for i in range(num_bn):\n",
    "        label='с нормал.'\n",
    "        if labels is not None:\n",
    "            label += str(labels[i])\n",
    "        plt.plot(bn_plots[i], bn_marker, label=label)\n",
    "    label='без нормал.'\n",
    "    if labels is not None:\n",
    "        label += str(labels[0])\n",
    "    plt.plot(bl_plot, bl_marker, label=label)\n",
    "    plt.legend(loc='lower center', ncol=num_bn+1) \n",
    "\n",
    "    \n",
    "plt.subplot(3, 1, 1)\n",
    "plot_training_history('Потери обчения','Итерации', solver, [bn_solver], \\\n",
    "                      lambda x: x.loss_history, bl_marker='o', bn_marker='o')\n",
    "plt.subplot(3, 1, 2)\n",
    "plot_training_history('Точность обучения','Эпохи', solver, [bn_solver], \\\n",
    "                      lambda x: x.train_acc_history, bl_marker='-o', bn_marker='-o')\n",
    "plt.subplot(3, 1, 3)\n",
    "plot_training_history('Валидационная точность','Эпохи', solver, [bn_solver], \\\n",
    "                      lambda x: x.val_acc_history, bl_marker='-o', bn_marker='-o')\n",
    "\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Блочная нормализация и инициализация\n",
    "\n",
    "Проведем небольшой эксперимент для изучения взаимодействия блочной нормализации и инициализации весов.\n",
    "\n",
    "Первая ячейка будет обучать 8-слойные сети как с бочной нормализацией, так и без нее, используя разные масштабы для инициализации весов. Вторая ячейка будет отображать точность обучения, точность валидации и потери обучения в зависимости от масштаба инициализации весов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "\n",
    "# Обучим глубокую нейросеть с использованием блочной нормализации в слоях\n",
    "# Проведем эксперименты с разными масштабами инициализации весов сети\n",
    "hidden_dims = [50, 50, 50, 50, 50, 50, 50]\n",
    "num_train = 1000\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "bn_solvers_ws = {}\n",
    "solvers_ws = {}\n",
    "# формируем массив масштабов весов инициализации \n",
    "weight_scales = np.logspace(-4, 0, num=20)\n",
    "# обучаем сети с разными масштабами весов инициализации  \n",
    "for i, weight_scale in enumerate(weight_scales):\n",
    "  print('Обучение с weight scale %d / %d' % (i + 1, len(weight_scales)))\n",
    "  bn_model = FullyConnectedNet(hidden_dims, weight_scale=weight_scale, normalization='batchnorm')\n",
    "  model = FullyConnectedNet(hidden_dims, weight_scale=weight_scale, normalization=None)\n",
    "# 1. Модель сети  с блочной нормализацией\n",
    "  bn_solver = Solver(bn_model, small_data,\n",
    "                  num_epochs=10, batch_size=50,\n",
    "                  update_rule='adam',\n",
    "                  optim_config={\n",
    "                    'learning_rate': 1e-3,\n",
    "                  },\n",
    "                  verbose=False, print_every=200)\n",
    "  bn_solver.train()\n",
    "  bn_solvers_ws[weight_scale] = bn_solver\n",
    "# 2. Модель сети  без блочной нормализации\n",
    "  solver = Solver(model, small_data,\n",
    "                  num_epochs=10, batch_size=50,\n",
    "                  update_rule='adam',\n",
    "                  optim_config={\n",
    "                    'learning_rate': 1e-3,\n",
    "                  },\n",
    "                  verbose=False, print_every=200)\n",
    "  solver.train()\n",
    "  solvers_ws[weight_scale] = solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Построение графиков с результатами экспериментов при разных масштабах весов инициализации \n",
    "best_train_accs, bn_best_train_accs = [], []\n",
    "best_val_accs, bn_best_val_accs = [], []\n",
    "final_train_loss, bn_final_train_loss = [], []\n",
    "\n",
    "for ws in weight_scales:\n",
    "  best_train_accs.append(max(solvers_ws[ws].train_acc_history))\n",
    "  bn_best_train_accs.append(max(bn_solvers_ws[ws].train_acc_history))\n",
    "  \n",
    "  best_val_accs.append(max(solvers_ws[ws].val_acc_history))\n",
    "  bn_best_val_accs.append(max(bn_solvers_ws[ws].val_acc_history))\n",
    "  \n",
    "  final_train_loss.append(np.mean(solvers_ws[ws].loss_history[-100:]))\n",
    "  bn_final_train_loss.append(np.mean(bn_solvers_ws[ws].loss_history[-100:]))\n",
    "  \n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Лучшая валидационная точность vs масштаб весов инициализации')\n",
    "plt.xlabel('Масштаб веса инициализации')\n",
    "plt.ylabel('Лучшая валидационная точность')\n",
    "plt.semilogx(weight_scales, best_val_accs, '-o', label='без нормализации')\n",
    "plt.semilogx(weight_scales, bn_best_val_accs, '-o', label='с BN нормализацией')\n",
    "plt.legend(ncol=2, loc='lower right')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Лучшая точность обучения vs масштаб весов инициализации')\n",
    "plt.xlabel('Масштаб веса инициализации')\n",
    "plt.ylabel('Лучшая точность обучения')\n",
    "plt.semilogx(weight_scales, best_train_accs, '-o', label='без нормализации')\n",
    "plt.semilogx(weight_scales, bn_best_train_accs, '-o', label='c BN нормализацией')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Финальные потери обучения vs масштаб весов инициализации')\n",
    "plt.xlabel('Масштаб веса инициализации')\n",
    "plt.ylabel('Финальные потери обучения')\n",
    "plt.semilogx(weight_scales, final_train_loss, '-o', label='без нормализации')\n",
    "plt.semilogx(weight_scales, bn_final_train_loss, '-o', label='c BN нормализацией')\n",
    "plt.legend()\n",
    "plt.gca().set_ylim(1.0, 3.5)\n",
    "\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вопрос 1:\n",
    "Опишите результаты этого эксперимента. Как масштаб веса инициализации  влияет на модели с / без блочной нормализации и почему?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ответ:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Блочная нормализация и размер блока\n",
    "Проведем небольшой эксперимент для изучения взаимосвязи блочной нормализации и размера блока.\n",
    "\n",
    "Первая ячейка будет обучать 6-слойные сети как с блочной нормализацией, так и без нее с использованием разных размеров блоков. Вторая -  будет отображать точность обучения и точность валидации  с течением времени."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batchsize_experiments(normalization_mode):\n",
    "    # Функция обучает глубокие FC нейросети без слоев нормализации и со слоями BN\n",
    "    # при использованием миниблоков разных размеров\n",
    "    np.random.seed(231)\n",
    "    hidden_dims = [100, 100, 100, 100, 100]\n",
    "    num_train = 1000\n",
    "    small_data = {\n",
    "      'X_train': data['X_train'][:num_train],\n",
    "      'y_train': data['y_train'][:num_train],\n",
    "      'X_val': data['X_val'],\n",
    "      'y_val': data['y_val'],\n",
    "    }\n",
    "    \n",
    "    n_epochs=10              # число эпох\n",
    "    weight_scale = 2e-2\n",
    "    batch_sizes = [5,10,50]  # размеры миниблоков\n",
    "    lr = 10**(-3.5)          # скорость обучения\n",
    "    solver_bsize = batch_sizes[0]\n",
    "\n",
    "    print('Без нормализации: размер блока = ',solver_bsize)\n",
    "    model = FullyConnectedNet(hidden_dims, weight_scale=weight_scale, normalization=None)\n",
    "    solver = Solver(model, small_data,\n",
    "                    num_epochs=n_epochs, batch_size=solver_bsize,\n",
    "                    update_rule='adam',\n",
    "                    optim_config={\n",
    "                      'learning_rate': lr,\n",
    "                    },\n",
    "                    verbose=False)\n",
    "    solver.train()\n",
    "    \n",
    "    bn_solvers = []\n",
    "    for i in range(len(batch_sizes)):\n",
    "        b_size=batch_sizes[i]\n",
    "        print('С нормализацией: размер блока = ',b_size)\n",
    "        bn_model = FullyConnectedNet(hidden_dims, weight_scale=weight_scale, normalization=normalization_mode)\n",
    "        bn_solver = Solver(bn_model, small_data,\n",
    "                        num_epochs=n_epochs, batch_size=b_size,\n",
    "                        update_rule='adam',\n",
    "                        optim_config={\n",
    "                          'learning_rate': lr,\n",
    "                        },\n",
    "                        verbose=False)\n",
    "        bn_solver.train()\n",
    "        bn_solvers.append(bn_solver)\n",
    "        \n",
    "    return bn_solvers, solver, batch_sizes\n",
    "\n",
    "batch_sizes = [5,10,50]\n",
    "bn_solvers_bsize, solver_bsize, batch_sizes = run_batchsize_experiments('batchnorm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plot_training_history('Точность обучения (Блочная нормализация)','Эпохи', solver_bsize, bn_solvers_bsize, \\\n",
    "                      lambda x: x.train_acc_history, bl_marker='-^', bn_marker='-o', labels=batch_sizes)\n",
    "plt.subplot(2, 1, 2)\n",
    "plot_training_history('Валидационная точность (Блочная нормализация)','Эпохи', solver_bsize, bn_solvers_bsize, \\\n",
    "                      lambda x: x.val_acc_history, bl_marker='-^', bn_marker='-o', labels=batch_sizes)\n",
    "\n",
    "plt.gcf().set_size_inches(15, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вопрос  2:\n",
    "Опишите результаты этого эксперимента. Как влияет размер блока на точность обучения/валидации? Почему это происходит?\n",
    "\n",
    "## Ответ:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Нормализация на слое\n",
    "\n",
    "Блочной нормализация  упрощает  обучение сетей, но зависимость от размера блока делает ее менее полезной в сложных сетях, которые имеют ограничение на размер входного блока  (например, из-за ограничений аппаратного обеспечения).\n",
    "\n",
    "Для смягчения этой проблемы было предложено несколько альтернатив блочной нормализации; одним из таких методов является нормализация на слое [4]. Вместо нормализации в пределах мини-блока (т.е. с находим среднее каждого признака по количеству примеров в блоке) мы выполняем нормализацию по признакам (т.е. используем усреднение по признакам) . Другими словами, при использовании Layer Normalization каждый вектор признаков, соответствующий одной точке данных, нормируется на основе суммы всех членов внутри этого вектора.\n",
    "\n",
    "[4] [Ba, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E. Hinton. \"Layer Normalization.\" stat 1050 (2016): 21.](https://arxiv.org/pdf/1607.06450.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вопрос 3:\n",
    "Какой из  этапов предварительной обработки данных аналогичен нормализации блока, а какой - нормализации на слое?\n",
    "\n",
    "1. Масштабирование каждого изображения множества данных таким образом, что каналы RGB для каждоц строки пикселей в изображении в сумме дают 1.\n",
    "2. Масштабирование каждого изображения множества данных таким образом, что каналы RGB для всех пикселей в изображении в сумме дают 1.\n",
    "3. Вычитание среднего изображения набора данных из каждого изображения в наборе данных.\n",
    "4. Установка всех значений RGB в 0 или 1 в зависимости от заданного порогового значения.\n",
    "\n",
    "## Ответ:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Нормализация на слое: реализация\n",
    "\n",
    "Теперь реализуете нормализацию на слое. Этот шаг должен быть относительно прост, так как концептуально реализация почти идентична реализации блочной нормализации. Одно из существенных различий заключается в том, что для нормализации на слое мы не отслеживаем скользящие средние, и этап тестирования идентичен этапу обучения, где среднее и дисперсия вычисляются непосредственно для каждого примера входных данных.\n",
    "\n",
    "Вот что вам нужно сделать:\n",
    "\n",
    "    В `dlcv/layers.py` реализуете прямой путь для нормализации на слое  в функции `layernorm_forward`.\n",
    "\n",
    "Запустите ячейку ниже, чтобы проверить свои результаты.\n",
    "\n",
    "    В `dlcv/layers.py` реализуйте обратный путь для нормализации на слое функции `layernorm_backward`.\n",
    "\n",
    "Запустите вторую ячейку ниже, чтобы проверить свои результаты.\n",
    "\n",
    "    Измените `dlcv/classifiers/fc_net.py`, чтобы добавить нормализацию на слое в `FullyConnectedNet`. Когда флаг `normalization` установлен в `\"layernorm\"` в конструкторе, вы должны вставить код нормализации на слое перед каждой нелинейностью   ReLU.\n",
    "\n",
    "Запустите третью ячейку ниже, чтобы выполнить эксперимент с размером блока при нормализации на слое."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверим прямое распростанение в режиме обучения, \n",
    "# вычислив средние значения и стандартные отклонения\n",
    "# прзнаков  до и после нормализации слоя\n",
    "\n",
    "# Моделирование прямого распространения для двухслойной сети\n",
    "np.random.seed(231)\n",
    "N, D1, D2, D3 =4, 50, 60, 3\n",
    "X = np.random.randn(N, D1)\n",
    "W1 = np.random.randn(D1, D2)\n",
    "W2 = np.random.randn(D2, D3)\n",
    "a = np.maximum(0, X.dot(W1)).dot(W2)\n",
    "\n",
    "print('Перед нормализацией на слое:')\n",
    "print_mean_std(a,axis=1)\n",
    "\n",
    "gamma = np.ones(D3)\n",
    "beta = np.zeros(D3)\n",
    "\n",
    "# Средние должны быть близки к нулю, а стандартные отклонения к единице\n",
    "print('После нормализации на слое (gamma=1, beta=0)')\n",
    "a_norm, _ = layernorm_forward(a, gamma, beta, {'mode': 'train'})\n",
    "print_mean_std(a_norm,axis=1)\n",
    "\n",
    "gamma = np.asarray([3.0,3.0,3.0])\n",
    "beta = np.asarray([5.0,5.0,5.0])\n",
    "\n",
    "# Теперб средние должны быть близки к beta, а стандартные отклонения к gamma\n",
    "print('После нормализации на слое (gamma=', gamma, ', beta=', beta, ')')\n",
    "a_norm, _ = layernorm_forward(a, gamma, beta, {'mode': 'train'})\n",
    "print_mean_std(a_norm,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверка градиентов для  нормализации на слое\n",
    "np.random.seed(231)\n",
    "N, D = 4, 5\n",
    "x = 5 * np.random.randn(N, D) + 12\n",
    "gamma = np.random.randn(D)\n",
    "beta = np.random.randn(D)\n",
    "dout = np.random.randn(N, D)\n",
    "\n",
    "ln_param = {}\n",
    "fx = lambda x: layernorm_forward(x, gamma, beta, ln_param)[0]\n",
    "fg = lambda a: layernorm_forward(x, a, beta, ln_param)[0]\n",
    "fb = lambda b: layernorm_forward(x, gamma, b, ln_param)[0]\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(fx, x, dout)\n",
    "da_num = eval_numerical_gradient_array(fg, gamma.copy(), dout)\n",
    "db_num = eval_numerical_gradient_array(fb, beta.copy(), dout)\n",
    "\n",
    "_, cache = layernorm_forward(x, gamma, beta, ln_param)\n",
    "dx, dgamma, dbeta = layernorm_backward(dout, cache)\n",
    "\n",
    "# Относительная ошибка должна быть в диапазоне 1e-12 и 1e-8\n",
    "print('Ошибка в dx : ', rel_error(dx_num, dx))\n",
    "print('Ошибка в dgamma: ', rel_error(da_num, dgamma))\n",
    "print('Ошибка в dbeta: ', rel_error(db_num, dbeta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Нормализация на слое и размер блока\n",
    "\n",
    "Теперь мы проведем эксперимент с размером блока и нормализацией на слое. По сравнению с предыдущим экспериментом вы должны увидеть заметно меньшее влияние размера блока на историю обучения!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln_solvers_bsize, solver_bsize, batch_sizes = run_batchsize_experiments('layernorm')\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plot_training_history('Точность обучения (Нормализация на слое)','Эпохи', solver_bsize, ln_solvers_bsize, \\\n",
    "                      lambda x: x.train_acc_history, bl_marker='-^', bn_marker='-o', labels=batch_sizes)\n",
    "plt.subplot(2, 1, 2)\n",
    "plot_training_history('Валидационная точность (Нормализация на слое)','Эпохи', solver_bsize, ln_solvers_bsize, \\\n",
    "                      lambda x: x.val_acc_history, bl_marker='-^', bn_marker='-o', labels=batch_sizes)\n",
    "\n",
    "plt.gcf().set_size_inches(15, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вопрос 4:\n",
    "Когда нормализация на слое, вероятно, не будет работать хорошо и почему?\n",
    "\n",
    "1. При использовании в очень глубокой сети\n",
    "2. При наличии очень небольшого числа признаков\n",
    "3. При высоком уровне регуляризации\n",
    "\n",
    "\n",
    "## Ответ:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
