import numpy as np

"""
Этот файл реализует различные правила обновления первого порядка, которые обычно используются
для обучения нейронных сетей. Каждое правило обновления принимает на вход текущие веса и
градиент потерь по этим весам и обновляет значения весов. Каждое правило обновления
имеет один и тот же интерфейс:

def update(w, dw, config=None):

Входы:
   - w: массив numpy, содержащий текущие веса.
   - dw: массив numpy той же формы, что и w, содержащий градиент
     потерь по отношению к w.
   - config: словарь, содержащий значения гиперпараметров, такие как скорость
     обучения, момент и т. д. Если правило обновления требует кеширования значений
     между итерациями, то config будет хранить эти кешированные значения.

Возвращает:
   - next_w: обновленные значения весов.
   - config: словарь конфигурации, который будет передан на следующую итерацию
     правила обновления.

ПРИМЕЧАНИЕ. Для большинства правил обновления скорость обучения, задаваемая по умолчанию, скорее всего не будет
подходящей; однако значения по умолчанию для других гиперпараметров должны
хорошо работать для решения различных проблем.

Для повышения эффективности правила обновления могут выполнять обновления по месту, изменяя w и
устаналивая next_w равным  w.
"""

def sgd(w, dw, config=None):
    """
    Выполняет простой SGD.

    формат словаря config:
    - learning_rate: скорость обучения.
    """

    if config is None: config = {}
    config.setdefault('learning_rate', 1e-2)
    #print('lr=',config['learning_rate'])

    w -= config['learning_rate'] * dw
    return w, config


def sgd_momentum(w, dw, config=None):
    """
    Реализует SGD с моментом инерции.

    config формат:
    - learning_rate: скорость обучения.
    - momentum: число от 0 до 1, представляющее коэффициент "трения" (момента).
      Установка momentum = 0 приводит к алгориму sgd.
    - velocity: numpy массив такой же формы, что w и dw; используется для
    хранения скользящих средних градиентов между итерациями вызова правила.
    """

    if config is None: config = {}
    config.setdefault('learning_rate', 1e-2)
    config.setdefault('momentum', 0.9)
    v = config.get('velocity', np.zeros_like(w))

    next_w = None

    ###########################################################################
    # ЗАДАНИЕ: Реализовать формулу SGD с моментом. Сохраните                  #
    # обновленные значения в  переменной next_w. Вы также должны              #
    # обновлять значение v.                                                   #
    ###########################################################################

    v = config['momentum'] * v - config['learning_rate'] * dw   # скользящее среднее v
    next_w = w + v                                              # сохраняем обновлённые веса

    ###########################################################################
    #                             КОНЕЦ ВАШЕГО КОДА                           #
    ###########################################################################

    config['velocity'] = v  # сохраняем текущее среднее градиентов - скорость

    return next_w, config


def rmsprop(w, dw, config=None):
    """
    Реализует правило обновления RMSProp, которое использует скользящее среднее квадрата
    градиента, чтобы обеспечивать адаптацию скорости обучения для каждого параметра.

     config формат:
     - learning_rate: скорость обучения.
     - decay_rate: число от 0 до 1, задающее забывание при вычислении скользящего среднего квадрата
       градиента.
     - epsilon: небольшое число, используемое для сглаживания, чтобы избежать деления на ноль.
     - grad_squared: скользящее среднее квадратов градиентов - измеряет "крутизну" функции потерь.
    """

    if config is None: config = {}
    config.setdefault('learning_rate', 1e-2)
    config.setdefault('decay_rate', 0.99)
    config.setdefault('epsilon', 1e-8)
    config.setdefault('grad_squared', np.zeros_like(w))

    next_w = None

    ###########################################################################
    # ЗАДАНИЕ: Реализовать формулы RMSprop , сохраняя новое  значение w       #
    # в переменной next_w. Не забудьте обновить значение steep, сохраненное в #
    # config['steep'].                                                        #
    ###########################################################################

    config['grad_squared'] = (config['decay_rate'] * config['grad_squared']
                              + (1 - config['decay_rate']) * (dw ** 2))
    w += ((-config['learning_rate'] * dw) /
          (np.sqrt(config['grad_squared']) + config['epsilon']))
    next_w = w

    ###########################################################################
    #                             КОНЕЦ ВАШЕГО КОДА                           #
    ###########################################################################

    return next_w, config


def adam(w, dw, config=None):
    """
    Использует правило обновления Адам, которое включает скользящие средние как для
    градиента, так  и его квадрата, а также корректирующий коэффициент смещения.

     config формат:
     - learning_rate: скорость обучения.
     - beta1: коэффициент забывания для скользящего среднего градиента (первого момента).
     - beta2: коэффициент забывания для скользящего среднего квадрата градиента (второго момента).
     - epsilon: небольшое число, используемое для сглаживания, чтобы избежать деления на ноль.
     - m: скользящее среднее градиента.
     - v: скользящее среднее квадрата градиента.
     - t: номер шага итерации.
    """

    if config is None: config = {}
    config.setdefault('learning_rate', 1e-3)
    config.setdefault('beta1', 0.9)
    config.setdefault('beta2', 0.999)
    config.setdefault('epsilon', 1e-8)
    config.setdefault('m', np.zeros_like(w))
    config.setdefault('v', np.zeros_like(w))
    config.setdefault('t', 0)

    next_w = None
    ###########################################################################
    # ЗАДАНИЕ: Реализуте Формулы правила обновления Adam, сохраняя новое      #
    # значение w в переменнq next_w. Не забудьте обновить переменные m, v и t #
    # сохраняемые в config.                                                   #
    #                                                                         #
    # ПРИМЕЧАНИЕ. Чтобы значения совпадали с эталонными, пожалуйста,          #
    # увеличьте t перед его использованием                                    #
    ###########################################################################

    config['t'] += 1
    config['m'] = config['beta1'] * config['m'] + (1 - config['beta1']) * dw
    mt = config['m'] / (1 - config['beta1'] ** config['t'])
    config['v'] = config['beta2'] * config['v'] + (1 - config['beta2']) * (dw ** 2)
    vt = config['v'] / (1 - config['beta2'] ** config['t'])
    w += (-config['learning_rate'] * mt) / (np.sqrt(vt) + config['epsilon'])
    next_w = w

    ###########################################################################
    #                             КОНЕЦ ВАШЕГО КОДА                           #
    ###########################################################################

    return next_w, config
