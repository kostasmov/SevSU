from builtins import range
import numpy as np

# добавлено в dlcv блокнотах
from operator import itemgetter
from itertools import product

def affine_forward(x, w, b):
    """
    Выполняет прямое распространение значений для аффинного (полносвязанного) слоя.

    Входной сигнал x имеет форму (N, d_1, ..., d_k) и представляет мини-блок из N
    примеров, где каждый пример x [i] имеет форму (d_1, ..., d_k). Функция
    преобразует каждый вход в вектор размерности D = d_1 * ... * d_k и
    затем вычмсляет выходной вектор размерности M.

     Входы:
     - x: массив numpy, содержащий входные данные, формы (N, d_1, ..., d_k)
     - w: Множество весовых коэффициентов, формы (D, M)
     - b: Массив смещений, формы (M,)

     Возвращает кортеж из:
     - out: выход, формы (N, M)
     - cache: (x, w, b)
    """

    out = None

    ###########################################################################
    # ЗАДАНИЕ: Реализуйте прямой путь для афинного слоя. Сохраните результаты #
    # в out. Вам необходимо преобразовать входные данне в вектор              #
    ###########################################################################

    N = x.shape[0]
    out = x.reshape(N, -1).dot(w) + b

    ###########################################################################
    #                             КОНЕЦ ВАШЕГО КОДА                           #
    ###########################################################################

    cache = (x, w, b)
    return out, cache


def affine_backward(dout, cache):
    """
    
    Выполняет обратный проход для аффинного слоя.

     Входы:
     - dout: восходящая производная, форма (N, M)
     - cache: кортеж:
       - x: входные данные формы (N, d_1, ... d_k)
       - w: веса формы (D, M)
       - b: смещения формы (M,)

     Возвращает кортеж:
     - dx: градиент по x, форма (N, d1, ..., d_k)
     - dw: градиент по w, форма (D, M)
     - db: градиент по отношению к b, форма (M,)
    """

    x, w, b = cache
    dx, dw, db = None, None, None

    ###########################################################################
    # ЗАДАНИЕ: Реализуйте обратное распространение для афинного слоя          #
    ###########################################################################

    N = x.shape[0]
    x_flat = x.reshape(N, -1)
    dx = dout.dot(w.T).reshape(x.shape)
    dw = x_flat.T.dot(dout)
    db = dout.sum(axis=0)

    ###########################################################################
    #                            КОНЕЦ ВАШЕГО КОДА                            #
    ###########################################################################

    return dx, dw, db


def relu_forward(x):
    """
    Выполняет прямое распространение для слоя блоков ReLU.

     Входные данные:
     - x: входы любой формы

     Возвращает кортеж:
     - out: выход, такой же формы, как x
     - cache: x
    """

    out = None

    ###########################################################################
    # ЗАДАНИЕ: Реализуйте прямое рапсространение через ReLU                   #
    ###########################################################################
    
    out = np.maximum(x, 0)

    ###########################################################################
    #                              КОНЕЦ ВАШЕГО КОДА                          #
    ###########################################################################

    cache = x
    return out, cache


def relu_backward(dout, cache):
    """
     Выполняет обратный проход для слоя из блоков ReLU.

     Входные данные:
     - dout: восходящие производные
     - cache: вход x, такой же формы, как dout

     Возвращает:
     - dx: градиент по x
    """

    dx, x = None, cache

    ###########################################################################
    # ЗАДАНИЕ: Реализовать обратное распространение через ReLU                #
    ###########################################################################
    
    dx = dout
    dx[x<0] = 0

    ###########################################################################
    #                              КОНЕЦ ВАШЕГО КОДА                          #
    ###########################################################################

    return dx


def batchnorm_forward(x, gamma, beta, bn_param):
    """
    Реализует прямой путь для блочной нормализации.

    Во время обучения выборочное среднее и (нескорректированная) выборочная дисперсия
    вычисляются на основе статистики мини-блоков и используются для нормализации входящих
    данных. Во время обучения мы также вычисляем  и сохраняем скользящие средние и дисперсии 
    каждого признака, и эти средние значения используются для нормализации данных во время
    тестирования.

    На каждом временном шаге мы обновляем текущие средние значения и дисперсию, используя
    коэффициента момента (импульса) momentum :

    running_mean = momentum * running_mean + (1 - momentum) * sample_mean
    running_var = momentum * running_var + (1 - momentum) * sample_var

    Обратите внимание, что статья по блочной нормализации предлагает другое поведение во
    время тестирования: они вычисляют выборочное среднее и дисперсию для каждого признака,
    используя большое количество обучающих изображений, а не используя скользящее среднее. 
    Для этой реализации мы решили вместо этого использовать скользящие средние, поскольку 
    они не требуют дополнительного этапа вычислений; реализация блочной нормализации в torch
    также использует текущие средние значения.

    Вход:
    - x: данные формы (N, D)
    - gamma: параметр масштаба формы (D,)
    - beta: параметр смещения формы (D,)
    - bn_param: словарь со следующими ключами:
      - mode: 'train' или 'test'
      - eps: константа для обеспечения вычислительной устойчивости
      - momentum: коэффициент затухания для вычисления текущего среднего / дисперсии.
      - running_mean: массив формы (D,), хранящий текущие средние признаков 
      - running_var: массив формы(D,), хранящий текущие дисперсии признаков

    Возвращает кортеж из:
    - out: форма (N, D)
    - cache: кортеж значений, необходимых для обратного распростанения
    """
    
    mode = bn_param['mode']
    eps = bn_param.get('eps', 1e-5)
    momentum = bn_param.get('momentum', 0.9)

    N, D = x.shape
    running_mean = bn_param.get('running_mean', np.zeros(D, dtype=x.dtype))
    running_var = bn_param.get('running_var', np.zeros(D, dtype=x.dtype))

    out, cache = None, None
    if mode == 'train':
        #######################################################################
        # Задание: Реализуйте прямое распространение для слоя блочной         #
        # нормализации в  режиме обучения. Используйте статистику в пределах  #
        # мини-блоков для вычисления среднего значения и дисперсии, используя #
        # эту статистику нормализуйте входящие данные, выполните              # 
        # масштабирование и смещение нормализованных данных,используя         #
        # gamma и beta.                                                       #
        #                                                                     #
        # Вы должны сохранить выход в переменной out. Все промежуточные       #
        # переменные, необходимые для обратного распространения, должны       # 
        # храниться в переменной кэша.                                        #
        #                                                                     #
        # Вам также следует использовать вычисленное выборочное среднее       #
        # и дисперсию для вычисления скользящего среднего  и скользящей       #
        # дисперсии, сохраняя их в переменных running_mean и running_var.     #
        #                                                                     #
        # Обратите внимание: хотя вам следует отслеживать текущую дисперсию,  #
        # но нормализовать данные следует по отношению к стандартному         #
        # отклонения (квадратному корню из дисперсии)!                        #
        #######################################################################

        cache = {}
        sample_mean = np.mean(x, axis=0) # среднее признаков по миниблоку, (D,)
        # x -= sample_mean
        sample_var = np.var(x, axis=0)  # дисперсия признаков по миниблоку, (D,)
        numerator = x - sample_mean     # центрированное значение x, (N,D)
        denom=np.sqrt(sample_var+eps)   # стандартное отклонение, (D,)
        #x_norm = x / np.sqrt(sample_var + eps)
        x_norm = numerator / denom        # нормализованные значения x, (N,D)
        out = gamma * x_norm + beta         # выход слоя нормализации, (N,D)
        # обновление скользящих средних и дисперсии
        running_mean = momentum * running_mean + (1 - momentum) * sample_mean
        running_var = momentum * running_var + (1 - momentum) * sample_var
        # сохранение значений 
        cache["numerator"] = numerator
        cache["denom"] = denom
        cache["x_norm"] = x_norm
        cache["gamma"] = gamma
        cache["x"] = x
        cache["sample_mean"] = sample_mean

        pass

        #######################################################################
        #                          КОНЕЦ ВАШЕГО КОДА                          #
        #######################################################################
    elif mode == 'test':
        #######################################################################
        # Задание: реализовать прямое распространение через слой блочной      #
        # нормализации для режима тестирования. Используйте скользящее среднее#
        # и дисперсию,чтобы нормализовать входные данные, затем выполните     #
        # масштабирование и смещение нормализованных данныx с помощью         #
        # параметров gamma и beta.                                            #
        # Сохраните результат в переменной out.                               #
        #######################################################################

        x_norm = (x - running_mean) / np.sqrt(running_var + eps)
        out = gamma * x_norm + beta

        pass

        #######################################################################
        #                         КОНЕЦ ВАШЕГО КОДА                           #
        #######################################################################
    else:
        raise ValueError('Ошибочно задан режим прямой блочной нормализации "%s"' % mode)
    
    # Сохранение обновленных значений текущих средних обратно в словаре bn_param
    bn_param['running_mean'] = running_mean
    bn_param['running_var'] = running_var

    return out, cache


def batchnorm_backward(dout, cache):
    """
    Обратное распространение через слой блочной нормализации.

    Для реализации этой функции следует представить на бумаге граф вычислений
    для слоя блочной нормализации и распространить градиенты обратно через 
    промежуточные узлы слоя.

    Входы:
    - dout: восходящие производные, форма (N, D)
    - cache: промежуточные переменные, сохраненные на этапе прямого распространения через слой.

    Возвращает кортеж из:
    - dx: Градиент по отношению ко входу x, форма (N, D)
    - dgamma: Градиент по отношению к параметру масштабирования gamma, форма (D,)
    - dbeta: Градиент по отношению к параметру смещения beta, форма (D,)
    """

    dx, dgamma, dbeta = None, None, None

    ###########################################################################
    # Задание: реализовать обратное распространение через слой блочной        #
    # нормализации. Сохраните результаты в переменных dx, dgamma и dbeta.     #
    ###########################################################################

    N = dout.shape[0]        # число примеров в миниблоке
    dbeta = dout.sum(axis=0) # градиент out=gamma*x_norm+beta по beta, (D,)
    dgamma = (dout * cache['x_norm']).sum(axis=0)  # градиент out=gamma*x_norm+beta по gamma, (D,)
    dxnorm = dout * cache['gamma'] # градиент out=gamma*x_norm+beta по x_norm, (N,D)
    denom = cache['denom']
    # градиент x_norm=numerator/denom по denom  (производная 1/х = -1/(х**2))
    ddenom = -((dxnorm * cache['numerator']) / denom ** 2).sum(axis=0, keepdims=True)
    # градиент denom=np.sqrt(sample_var+eps) по sample_var (производная sqrt(x)=1/(2*sqrt(x))
    dsample_var = ddenom * 0.5 / denom
    # градиент x_norm=numerator/denom по numerator
    dnumer = dxnorm / denom
    # градиент numerator=x-sample_mean по sample_mean
    dsample_mean = -dnumer.sum(axis=0)
    x = cache['x']
    # градиент sample_var=np.var(x,axis=0) 
    dsample_var_dx = 2.0 * (x - x.mean(axis=0)) / N
    # градиент sample_mean=np.mean(x,axis=0)
    dsample_mean_dx = 1.0 / N
    # градиент по х
    dx = dnumer + dsample_var * dsample_var_dx + dsample_mean * dsample_mean_dx

    pass

    ###########################################################################
    #                          КОНЕЦ ВАШЕГО КОДА                              #
    ###########################################################################

    return dx, dgamma, dbeta


def batchnorm_backward_alt(dout, cache):
    """
    Реализует альтернативный обратный путь для блочной нормализации.

    Для реализации этой функции следует выполнить аналитическое дифференцирование 
    выражений блочной нормализации на бумаге и максимально упростить их. 
    Вы должны быть получить простое выражение для обратного распространения.
    Дополнительные подсказки см. в блокноте Jupyter.
     
     Примечание. Эта реализация должна использовать ту же переменную кэша, что
     и Batchnorm_backward, но может не использовать все значения в кэше.

    Входы / Выходы: Такие же как и у  batchnorm_backward
    """

    dx, dgamma, dbeta = None, None, None

    ###########################################################################
    # Задание: реализовать обратное распространение через слой блочной        #
    # нормализации. Сохраните результаты в переменных dx, dgamma и dbeta.     #
    # После вычисления градиента относительно центрированных входных данных   #
    # вы сможете вычислить градиенты относительно входных данных в одном      #
    # операторе; наша реализация умещается в одну строку.                     #
    ###########################################################################

    N = dout.shape[0]
    dbeta = dout.sum(axis=0)   # градиент out=gamma*x_norm+beta по beta
    dgamma = (dout * cache["x_norm"]).sum(axis=0) # градиент out=gamma*x_norm+beta по gamma
    # градиент по х от out=gamma*(x-sample_mean)/sqrt(sample_var)+beta,
    # где sample_mean и sample_var функции x 
    dx = cache["gamma"] * (dout - dbeta / N - cache["x_norm"] * dgamma / N) / cache["denom"]

    pass

    ###########################################################################
    #                          КОНЕЦ ВАШЕГО КОДА                              #
    ###########################################################################

    return dx, dgamma, dbeta


def layernorm_forward(x, gamma, beta, ln_param):
    """
    Реализует прямое распространение для нормализации на слое.
  
    Как во время обучения, так и во время тестирования входные данные нормализуются для каждого
    примера данных, а затем масштабируются  гамма- и бета-параметрами, идентичными параметрам
    блочной нормализации.
    
    Обратите внимание, что в отличие от блочной нормализации поведение нормализации на слое
    во время обучения и тестирования идентично, и  не требуется вычислять скользящие средние
    значения.

    Вход:
    - x: данные формы (N, D)
    - gamma: параметр масштаба, форма (D,)
    - beta: параметр смещения, форма (D,)
    - ln_param: словарь со следующими ключами:
        - eps: числовая константа для обеспечения устойчивости вычислений

    Возращает кортеж из:
    - out: формв (N, D)
    - cache: кортеж из значений, необходимых для реализации обратного распространения
    """

    out, cache = None, None
    eps = ln_param.get('eps', 1e-5)

    ###########################################################################
    # Задание:                                                                #
    # Реализуйте прямое распространение для режима обучения, в соответствии   #
    # с нормализацией на слое. Нормализуйте входные данные, а также           #
    # выполните масштабирование и сместите нормализованные данные с помощью   #
    # парамеров гамма и бета.                                                 #
    # СОВЕТ: это можно реализовать, слегка изменив реализацию блочной         # 
    # нормализации для режима обучения и вставив пару строк . В частности,    #
    # можете ли Dы придумать какие-либо матричные преобразования, которые     #
    # позволили бы вам скопировать код блочной нормализации и оставить его    # 
    # практически неизменным                                                  #
    ###########################################################################
    
    pass

    ###########################################################################
    #                          КОНЕЦ ВАШЕГО КОДА                              #
    ###########################################################################

    return out, cache


def layernorm_backward(dout, cache):
    """
    Реализует обратное расространение для нормализации на слое.

    При реализации функции вы можете в значительной степени использовать
    код блочной нормализации.

    Входы:
    - dout: восходящие производные, форма (N, D)
    - cache: запомненные значения переменных в layernorm_forward.

    Возвращает кортеж из
    - dx: Градиент по входу x, форма (N, D)
    - dgamma:Градиент по парметру gamma, форма (D,)
    - dbeta: Градиент по параметру beta, форма (D,)
    """

    dx, dgamma, dbeta = None, None, None

    ###########################################################################
    # Задание: Реализовать  обратное распространение для нормализации на слое.#
    #                                                                         #
    # Совет: можно реализовать, если слегка модифицировать реализацию блочной #
    # нормализации в режиме обучения                                          #
    ###########################################################################
                
    pass

    ###########################################################################
    #                          КОНЕЦ ВАШЕГО КОДА                              #
    ###########################################################################

    return dx, dgamma, dbeta


def dropout_forward(x, dropout_param):
    """
    Выполняет прямой путь для (инвертированного) dropout.

    Входы:
    - x: входные данные, форма любая
    - dropout_param: Словарь со следующими ключами:
      - p: параметр Dropout - вероятность сохранения выхода нейрона.
      - mode: 'test' или 'train'. Если режим train, то выполняем исключение dropout;
        если режим test, то просто позвращается вход input.
      - seed: установочное значение для генератора случайных чисел. Использование seed
        обеспечивает повторяемость вычислений и необходимо для проверки градиента,
        но seed не требуется для реальной нейросети.

    Outputs:
    - out: массив такой же формы, что и x.
    - cache: кортеж (dropout_param, mask). В режиме обучения mask является маской dropout,
      которая используется, для перемножения со входом; в тестовом режиме mask = None.

    Обратите внимание: Необходимо реализовать инвертированный dropout, а не его обычную версию.
    
    Замечание: Помните, что p - это вероятность сохранения выхода нейрона; это может вступать
    в противоречие с некоторыми источниками, где p - вероятность исключения выхода нейрона.
    """

    p, mode = dropout_param['p'], dropout_param['mode']
    if 'seed' in dropout_param:
        np.random.seed(dropout_param['seed'])

    mask = None
    out = None

    if mode == 'train':
        #######################################################################
        # Задание: Релизовать прямое распространнение для режима обучения     #
        # инвертированного dropout.
        # Сохранить маску dropout в переменной mask.                          #
        #######################################################################

        # формируем случайную бинарную маску такой же формы, что и х
        # для инвертированного dropout делим на p
        mask = (np.random.rand(*x.shape) < p) / p

        # оставляем те выходы, где маска содержит единицы
        out = x * mask

        pass

        #######################################################################
        #                        КОНЕЦ ВАШЕГО КОДА                            #
        #######################################################################
    elif mode == 'test':
        #######################################################################
        # Задание: Релизовать прямое распространнение для тестового режима    #
        # инвертированного dropout.
        #######################################################################

        out = x
        pass

        #######################################################################
        #                        КОНЕЦ ВАШЕГО КОДА                            #
        #######################################################################

    cache = (dropout_param, mask)
    out = out.astype(x.dtype, copy=False)

    return out, cache


def dropout_backward(dout, cache):
    """
    Выполняет обратное распространение для (инвертированного) dropout.

    Входы:
    - dout: восходящие производные, форма любая
    - cache: (dropout_param, mask) из dropout_forward.
    """

    dropout_param, mask = cache
    mode = dropout_param['mode']

    dx = None
    if mode == 'train':
        #######################################################################
        # Задание: Реализовать обратное распространение в режиме обучения для #
        # инвертированного dropout                                            #
        #######################################################################

        dx = mask * dout
        pass

        #######################################################################
        #                        КОНЕЦ ВАШЕГО КОДА                            #
        #######################################################################
    elif mode == 'test':
        dx = dout
    return dx


def conv_forward_naive(x, w, b, conv_param):
    """
    Наивная реализация прямого пути  для сверточного слоя.
    
    Вход - минимблок из N примеров данных, каждый с C каналами , высотой H и
    шириной W. Функция выполняет свертку каждыого примера с помощью F различных фильтров, где каждый фильтр
    охватывает все C каналов  и имеет высоту HH и ширину WW.

    Входы:
    - x: входные данные формы (N, C, H, W)
    - w: фильтр с весами, форма (F, C, HH, WW)
    - b: Смещения, форма (F,)
    - conv_param: словарь с ключами:
      - 'stride': число пикселей, на которое сдвигается окно фильтра свертки в горизонтальном
         и вертикальном направлениях.
      - 'pad': число пикселей, которое дополняется нулями (zero-pad). 
     
    Во время дополнения нулями, нули должны быть расположены симметрично (то есть одинаково с обеих сторон)
    вдоль осей по высоте и ширине. Будьте внимательны, чтобы не модифицировать оригинальный вход
    x непосредственно.

    Возвращает кортеж:
    - out: выходные данные, форма (N, F, H', W'), где H' и W' задаются выражениями
      H' = 1 + (H + 2 * pad - HH) / stride
      W' = 1 + (W + 2 * pad - WW) / stride
    - cache: (x, w, b, conv_param)
    """

    out = None

    ###########################################################################
    # ЗАДАНИЕ: Реализуйте прямой путь для сверточного слоя.                   #
    # Совет: вы можете использовать функцию np.pad для дополнения нулями.     #
    ###########################################################################
    
    pass

    ###########################################################################
    #                             КОНЕЦ ВАШЕГО КОДА                           #
    ###########################################################################

    cache = (x, w, b, conv_param)
    return out, cache


def conv_backward_naive(dout, cache):
    """
    Наивная реализация обратного пути для сверточного слоя.

     Входы:
     - dout: восходящие производные.
     - cache: кортеж (x, w, b, conv_param), как в conv_forward_naive

     Возвращает кортеж:
     - dx: Градиент по x
     - dw: Градиент по отношению к w
     - db: Градиент относительно b 
    """

    dx, dw, db = None, None, None

    ###########################################################################
    # ЗАДАНИЕ: Реализуйте обратный путь для сверточного слоя.                 #
    ###########################################################################
    
    pass

    ###########################################################################
    #                              КОНЕЦ ВАШЕГО КОДА                          #
    ###########################################################################

    return dx, dw, db


def max_pool_forward_naive(x, pool_param):
    """
    Наивная реализация прямого распространения для макс-пулинга

    Входы:
    - x: входные данные, форма (N, C, H, W)
    - pool_param: словарь с ключами:
      - 'pool_height': высота каждой области пулинга
      - 'pool_width': ширина каждой области пулинга
      - 'stride': расстояние между соседними областями пулинга

    Добавление нулей здесь не требуется. 

    Возвращает кортеж:
    - out: выходные данные, форма (N, C, H', W'), где H' и W' равны
      H' = 1 + (H - pool_height) / stride
      W' = 1 + (W - pool_width) / stride
    - cache: (x, pool_param)
    """

    out = None

    ###########################################################################
    # ЗАДАНИЕ: Реализуйте прямой путь для  max-pooling                        #
    ###########################################################################
        
    pass

    ###########################################################################
    #                             КОНЕЦ ВАШЕГО КОДА                           #
    ###########################################################################

    cache = (x, pool_param)
    return out, cache


def max_pool_backward_naive(dout, cache):
    """
    Наивная реализация обратного пути  для слоя с макс пулингом.

     Входы:
     - dout: восходящие производные
     - cache: кортеж (x, pool_param), как при прямом распространении.

     Возвращает:
     - dx: градиент по x
    """

    dx = None

    ###########################################################################
    # ЗАДАНИЕ: Реализуйте обратный путь для  max-pooling                      #
    ###########################################################################
    
    pass

    ###########################################################################
    #                            КОНЕЦ ВАШЕГО КОДА                            #
    ###########################################################################

    return dx


def spatial_batchnorm_forward(x, gamma, beta, bn_param):
    """
    Выполняет прямое распространение для пространственной блочной нормализации

    Входы:
    - x: Входные данные формы (N, C, H, W)
    - gamma: параметр масштаба, форма (C,)
    - beta: параметр смещения, форма (C,)
    - bn_param: Словарь с ключами:
      - mode: 'train' или 'test'; обязательный
      - eps: Константа для обеспечения вычислительной устойчивости
      - momentum: Коэффициент для текущего среднего/дисперсии. momentum=0 означает, что
        предыдущая информация полностью игнорируется на каждом шаге времени, а
        momentum=1 означает, что новая информация никак не используется. Значение 
        по умолчанию  momentum=0.9 работает хорошо в большинстве ситуаций.
      - running_mean: Массив формы (D,) хранящий текущие средние признаков
      - running_var: Массив формы (D,) хранящий текущие дисперсии признаков

    Возвращает кортеж:
    - out: Выходные данные, форма (N, C, H, W)
    - cache: Значения необходимые для реализации обратного распространения
    """

    out, cache = None, None

    ###########################################################################
    # Задание: Реализовать прямое распространение через слой пространственной #
    # блочной нормализации.                                                   #
    #                                                                         #
    # Совет: Можно реализовать пространственную блочную нормализация путем    #
    # вызова функции обычной блочной нормализации, которая реализована выше   #
    # Ваша реализация должна быть очень короткой.                             #
    ###########################################################################
    
    pass

    ###########################################################################
    #                             Конец Вашего кода                           #
    ###########################################################################

    return out, cache


def spatial_batchnorm_backward(dout, cache):
    """
    Выполняет обратное распространение для пространственной блочной нормализации

    Входы:
    - dout: Восходящие производные, форма (N, C, H, W)
    - cache: Запомненные значения прямого распространения

    Возвращает кортеж:
    - dx: Градиент по отношению ко входам, форма (N, C, H, W)
    - dgamma: Градиент по отношению к параметру масштаба, форма (C,)
    - dbeta: Градиент по отношению к параметру сдвига, форма (C,)
    """

    dx, dgamma, dbeta = None, None, None

    ###########################################################################
    # Задание: Реализовать обратное распространение для пространственной      #
    # блочной нормализации.                                                   #
    #                                                                         #
    # Совет: Можно реализовать обратное распространение для пространственной  #
    # блочной нормализация путем вызова соотвествующей функции обычной        #
    # блочной нормализации, которая реализована выше   
    # Ваша реализация должна быть очень короткой.                             #
    ###########################################################################
    
    pass

    ###########################################################################
    #                             КОНЕЦ ВАШЕГО КОДА                           #
    ###########################################################################

    return dx, dgamma, dbeta


def spatial_groupnorm_forward(x, gamma, beta, G, gn_param):
    """
    Выполняет прямое распространение для пространственной нормализации группы.
    В отличие от нормализации на слое, групповая нормализация разбивает каждую 
    запись данных на G смежных частей, которые затем нормализуются независимо.
    Затем к данным применяются по признаковое смещение и масштабирование способом,
    идентичным блочной нормализации и нормализации на слое.

    Входы:
    - x: Входные данные формы (N, C, H, W)
    - gamma: параметр масштаба, форма (C,)
    - beta: параметр сдвига, форма (C,)
    - G: Целое число групп разбивки, должно быть делителем  C
    - gn_param: Словарь с ключами:
      - eps: Константы для ычислительной устойчивости

    Возвращает кортеж:
    - out: Выходные данные, форма (N, C, H, W)
    - cache: Значения, необходимые для обратного распространения
    """

    out, cache = None, None
    eps = gn_param.get('eps',1e-5)

    ###########################################################################
    # ЗАДАНИЕ:  реализовать прямое распространение для пространственной       #
    # нормализации  группы. Это будет очень похоже на реализацию нормализации #
    # слоя. В частности, подумайте о том, как можно преобразовать матрицу,    #
    # чтобы основная часть кода была аналогична блочной нормализации          #
    # в режиме обучения  и нормализации на слое!
    ###########################################################################
    
    pass

    ###########################################################################
    #                             КОНЕЦ ВАШЕГО КОДА                           #
    ###########################################################################

    return out, cache


def spatial_groupnorm_backward(dout, cache):
    """
    Выполняет обратное распространение для пространственной нормализации группы.

    Входы:
    - dout: Восходящие производные, форма (N, C, H, W)
    - cache: Запомненные значения прямого распространения

    Возвращает кортеж:
    - dx: Градиент по входам, форма (N, C, H, W)
    - dgamma: Градиент по параметру масштаба, форма (C,)
    - dbeta: Градиент по параметру смещения, форма (C,)
    """

    dx, dgamma, dbeta = None, None, None

    ###########################################################################
    # ЗАДАНИЕ: Реализовать обратное распространение для пространственной      #
    # нормализации группы                                                     #
    # Это будет сильно похоже на реализацию нормализации на слое              #
    ###########################################################################
    
    pass

    ###########################################################################
    #                             КОНЕЦ ВАШЕГО КОДА                           #
    ###########################################################################

    return dx, dgamma, dbeta


def svm_loss(x, y):
    """
    Вычисляет потери и градиент для мультиклассового SVM классификатора

    Входы:
    - x: входные данные формы (N, C), где x[i, j] это рейтинг (score) j-го класса
       при i-ом входе.
    - y: вектор меток формы (N,), где y[i] - метка x[i] и
      0 <= y[i] < C

    Возвращает кортеж из::
    - loss: потери
    - dx: градиент функции потерь по отношению ко входам x
    """

    N = x.shape[0]
    correct_class_scores = x[np.arange(N), y]
    margins = np.maximum(0, x - correct_class_scores[:, np.newaxis] + 1.0)
    margins[np.arange(N), y] = 0
    loss = np.sum(margins) / N
    num_pos = np.sum(margins > 0, axis=1)
    dx = np.zeros_like(x)
    dx[margins > 0] = 1
    dx[np.arange(N), y] -= num_pos
    dx /= N

    return loss, dx


def softmax_loss(x, y):
    """
    Вычисляет потери и градиент для softmax классификатора
    Входы:
    - x: входные данные формы (N, C), где x[i, j] это рейтинг (score) j-го класса
       при i-ом входе.
    - y: вектор меток формы (N,), где y[i] - метка x[i] и
      0 <= y[i] < C  

    Возвращает кортеж из::
    - loss: потери
    - dx: градиент функции потерь по отношению ко входам x
    """

    shifted_logits = x - np.max(x, axis=1, keepdims=True)
    Z = np.sum(np.exp(shifted_logits), axis=1, keepdims=True)
    log_probs = shifted_logits - np.log(Z)
    probs = np.exp(log_probs)
    N = x.shape[0]
    loss = -np.sum(log_probs[np.arange(N), y]) / N
    dx = probs.copy()
    dx[np.arange(N), y] -= 1
    dx /= N

    return loss, dx
