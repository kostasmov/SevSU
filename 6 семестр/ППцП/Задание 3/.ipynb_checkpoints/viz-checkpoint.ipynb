{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Визуализация сети (TensorFlow)\n",
    "\n",
    "В этом блокноте мы рассмотрим использование \"градиентов изображений\" для создания новых изображений.\n",
    "\n",
    "При обучении модели обычно определяют функцию потерь, которая оценивает эффективность модели; затем  используют обратное распространение для вычисления градиента функции потерь по отношению к параметрам модели и выполняют градиентный спуск по параметрам модели, чтобы минимизировать потери.\n",
    "\n",
    "Здесь будем выполнять немного другое. Мы начнем с модели сверточной нейронной сети, которая была предварительно обучена для классификации изображений на наборе данных ImageNet. Будем использовать эту модель для определения функции потерь, которая оценивает текущие потери на изображении, а затем используем обратное распространение для вычисления градиента этих потерь по отношению к пикселям изображения. Сохраним модель и выполним градиентное спуск, чтобы синтезировать новое изображение, которое минимизирует потери.\n",
    "\n",
    "В этом блокноте рассматриваются три метода генерации изображений:\n",
    "\n",
    "1. **Карты значимости**: Карты значимости - это быстрый способ определить, какая часть изображения повлияла на решение о классификации, принятое сетью.\n",
    "2. **Обманные изображения**: мы можем изменить входное изображение так, чтобы оно выглядело для человека тем же, но  классифицировалось предварительно обученной сетью как изображение заданного класса.\n",
    "3. **Визуализация класса**: мы можем синтезировать изображение, чтобы максимизировать оценку классификации конкретного класса; это может дать нам некоторое представление о том, что ищет сеть, когда классифицирует изображения этого класса.\n",
    "\n",
    "В этом  блокноте воспользуемся возможностями **TensorFlow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "# Необходимые установки\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import time, os, json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from cv.classifiers.squeezenet import SqueezeNet\n",
    "from cv.data_utils import load_tiny_imagenet\n",
    "from cv.image_utils import preprocess_image, deprocess_image\n",
    "from cv.image_utils import SQUEEZENET_MEAN, SQUEEZENET_STD\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "# Предобученная модель\n",
    "\n",
    "Для всех наших экспериментов по генерации изображений будем использовать сверточную нейронную сеть, которая была предварительно обучена классификации изображений на данных из ImageNet. Здесь можно использовать любую модель нейросети, но для целей этого задания  используется сеть SqueezeNet [1], которая обеспечивает точность, сравнимую с AlexNet, но со значительно меньшим числом параметров и меньшей вычислительной сложностью.\n",
    "\n",
    "Использование SqueezeNet вместо AlexNet, VGG или ResNet означает, что мы можем легко выполнять все эксперименты по генерации изображений на CPU.\n",
    "\n",
    "Модель SqueezeNet была портирована из PyTorch  в TensorFlow; см . архитектуру модели в : `cv/ classifiers / squeezenet.py` \n",
    "\n",
    "Чтобы использовать SqueezeNet, нужно сначала **загрузить веса** в каталог `cv / datasets`, запустив `get_squeezenet_tf.sh`. Обратите внимание, что если вы ранее выполняли `get_assignment3_data.sh`, \n",
    "то модель SqueezeNet уже будет загружена.\n",
    "\n",
    "После загрузки модели Squeezenet мы можем загрузить ее в новый сеанс TensorFlow.\n",
    "\n",
    "[1] Iandola и др., \"SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and < 0.5MB model size\", arXiv 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = 'cv/datasets/squeezenet.ckpt'\n",
    "\n",
    "if not os.path.exists(SAVE_PATH + \".index\"):\n",
    "    raise ValueError(\"You need to download SqueezeNet!\")\n",
    "\n",
    "model = SqueezeNet()\n",
    "status = model.load_weights(SAVE_PATH)\n",
    "\n",
    "model.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка изображений ImageNet\n",
    "\n",
    "Вам предоставляется несколько примеров изображений из валидационного подмножества  набора данных ImageNet ILSVRC 2012 Classification. Чтобы загрузить эти изображения, перейдите в `cv / datasets /` и запустите `get_imagenet_val.sh`.\n",
    "\n",
    "Так как эти изображения извлечены из валидационного  набора, то  предварительно обученная модель \"не видела\" эти изображения во время обучения.\n",
    "\n",
    "Выполните ячейку ниже, чтобы визуализировать некоторые из этих изображений вместе с их метками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cv.data_utils import load_imagenet_val\n",
    "X_raw, y, class_names = load_imagenet_val(num=5)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(5):\n",
    "    plt.subplot(1, 5, i + 1)\n",
    "    plt.imshow(X_raw[i])\n",
    "    plt.title(class_names[y[i]])\n",
    "    plt.axis('off')\n",
    "plt.gcf().tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предварительная обработка изображений\n",
    "\n",
    "Предполагается, что входы  предварительно обученной модели нормализованы. Поэтому сначала  предварительно обработаем изображения, вычтя среднее значение по пикселям и разделив на стандартное отклонение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array([preprocess_image(img) for img in X_raw])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Карты значимости\n",
    "\n",
    "Используя предварительно обученную модель, вычислим карты значимости классов, как описано в разделе 3.1 в [2].\n",
    "\n",
    "**Карта значимости** показывает нам степень влияния каждого пикселя изображения на оценку рейтинга, используемую для классификации этого изображения. Чтобы вычислить карту значимости, вычисляется градиент оценки рейтинга, соответствующей правильному классу (который является скаляром) относительно пикселей изображения. Если изображение имеет форму `(H, W, 3)`, то градиент также будет иметь форму `(H, W, 3)`; для каждого пикселя в изображении градиент сообщает нам величину, на которую изменится оценка классификации, если значение пикселя не значительно изменится. Чтобы вычислить карту значимости, берется абсолютное значение  градиента, а затем выбирается максимальное значение из 3 входных каналов; итоговая карта значимости, таким образом, имеет форму (H, W), и все значения неотрицательны.\n",
    "\n",
    "Откройте файл `cv/classifiers/squeezenet.py` и прочитайте код, чтобы убедиться в том, что Вы понимаете как использовать модель.  Вы должны будете использовать  [`tf.GradientTape()`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/GradientTape) для вычисления градиентов по отношению к пикселям изображения В частности, весьма полезно познакомиться с   [разделом](https://www.tensorflow.org/alpha/tutorials/eager/automatic_differentiation#gradient_tapes) для лучшего понимания .\n",
    "\n",
    "[2] Karen Simonyan, Andrea Vedaldi, и Andrew Zisserman. \"Deep Inside Convolutional Networks: Visualising\n",
    "Image Classification Models and Saliency Maps\", ICLR Workshop 2014."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подсказка: Метод Tensorflow `gather_nd` \n",
    "\n",
    "Вспомните , что ранее Вам приходилось выбирать один элемент из каждой строки матрицы рейтингов; если `s` - это  массив формы` (N, C) ` и ` y` -  массив формы `(N,`), содержащий целые числа `0 <= y [i] <C`, то` s[np.arange (N), y] `- это  массив формы` (N,) `, который выбирает один элемент из каждой строки `s`, используя индексы  `y`.\n",
    "\n",
    "В Tensorflow Вы можете выполнить эту операцию, используя метод `gather_nd()`. Если `s` является тензором формы`(N, C)` и `y` является тензором формы `(N,)`, содержащим значения в диапазоне `0 <= y [i] <C`, то\n",
    "\n",
    "`tf.gather_nd(s, tf.stack((tf.range(N), y), axis=1))`\n",
    "\n",
    "будет тензором формы `(N,)`, содержащим одну запись из каждой строки `s`, выбранную в соответствии с индексами ` y`.\n",
    "\n",
    "Вы также можете обратиться к документации с описанием метода [gather_nd ](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/gather_nd)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_saliency_maps(X, y, model):\n",
    "    \"\"\"\n",
    "     Вычислите карту значимости для изображений X и меток y, используя модель model\n",
    "\n",
    "     Входные данные:\n",
    "     - X: входные изображения, массив формы (N, H, W, 3)\n",
    "     - y: метки для X,  форма (N,)\n",
    "     - model: модель SqueezeNet, которая будет использоваться для вычисления карты значимости.\n",
    "\n",
    "     Возвращает:\n",
    "     - saliency: массив значений (N, H, W), представляющий карты значимости для\n",
    "     входных изображений.\n",
    "    \n",
    "    \"\"\"\n",
    "    saliency = None\n",
    "    \n",
    "    ######################################################################################\n",
    "    # Задание: вычислить карты значимости для пакета изображений.                          #\n",
    "    #                                                                                    #\n",
    "    # 1) Определить объект GradientTape и входную переменную X сделать наблюдаемой       #\n",
    "    # 2) Вычислить «потери» для всех примеров входных изображений:                       #\n",
    "    # - получить оценки рейтингов с помощью model.call для  входных изображений          #\n",
    "    # - используйте tf.gather_nd или tf.gather для получения рейтингов корректных классов#\n",
    "    # 3) Используйте метод gradient() объекта GradientTape для вычисления                #       \n",
    "    # градиента функции потерь по отношению к изображению                                #     \n",
    "    # 4) Наконец, обработайте возвращенный градиент, чтобы вычислить карту значимости.   #\n",
    "    ######################################################################################\n",
    "    # *****НАЧАЛО ВАШЕГО КОДА (НЕ УДАЛЯЙТЕ/НЕ МОДИФИЦИРУЙТЕ ЭТУ СТРОКУ)*****\n",
    "    \n",
    "    pass\n",
    "\n",
    "    #*****КОНЕЦ ВАШЕГО КОДА (НЕ УДАЛЯЙТЕ/НЕ МОДИФИЦИРУЙТЕ ЭТУ СТРОКУ)*****\n",
    "    ##############################################################################\n",
    "    #                               Конец Вашего кода                            #\n",
    "    ##############################################################################\n",
    "    return saliency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполните ячейку ниже, чтобы визуализировать некоторые карты значимости классов на примерах изображений из валидационного подмножества ImageNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [],
   "source": [
    "def show_saliency_maps(X, y, mask):\n",
    "    mask = np.asarray(mask)\n",
    "    Xm = X[mask]\n",
    "    ym = y[mask]\n",
    "\n",
    "    saliency = compute_saliency_maps(Xm, ym, model)\n",
    "\n",
    "    for i in range(mask.size):\n",
    "        plt.subplot(2, mask.size, i + 1)\n",
    "        plt.imshow(deprocess_image(Xm[i]))\n",
    "        plt.axis('off')\n",
    "        plt.title(class_names[ym[i]])\n",
    "        plt.subplot(2, mask.size, mask.size + i + 1)\n",
    "        plt.title(mask[i])\n",
    "        plt.imshow(saliency[i], cmap=plt.cm.hot)\n",
    "        plt.axis('off')\n",
    "        plt.gcf().set_size_inches(10, 4)\n",
    "    plt.show()\n",
    "\n",
    "mask = np.arange(5)\n",
    "show_saliency_maps(X, y, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "# ВОПРОС\n",
    "\n",
    "Ваш друг говорит, что для того, чтобы найти изображение, которое максимизирует корректную оценку, следует выполнить градиентное восхождение на входном изображении, но вместо градиента можно фактически использовать карту значимости на каждом шаге для обновления изображения. Это утверждение верно? Почему да или почему нет?\n",
    "\n",
    "**Ваш ответ:** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обманные изображения\n",
    "\n",
    "Можно также использовать градиенты изображения для генерации «обманывающих изображений», как предлагается в [3]. Взяв изображение и целевой класс, мы можем выполнить градиентный **подъем** по изображению, чтобы максимизировать рейтинг целевого класса, останавившись, когда сеть классифицирует изображение как целевой класс. Реализуйте функцию для генерации обманных изображений.\n",
    "\n",
    "[3] Szegedy et al, \"Intriguing properties of neural networks\", ICLR 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_fooling_image(X, target_y, model):\n",
    "    \"\"\"\n",
    "    Создает обманное изображение, близкое к X, но которое модель классифицирует\n",
    "    как целевое target_y.\n",
    "\n",
    "     Входы:\n",
    "     - X: входное изображение, массив формы (1, 224, 224, 3)\n",
    "     - target_y: индекс класса, целое число в диапазоне [0, 1000)\n",
    "     - model: предварительно обученная модель SqueezeNet\n",
    "\n",
    "     Возвращает:\n",
    "     - X_fooling: изображение, близкое к X, но классифицируемое моделью как target_y\n",
    "        \n",
    "    \"\"\"\n",
    "      \n",
    "    #Сделаем копию входа, который будем изменять\n",
    "    X_fooling = X.copy()\n",
    "    \n",
    "    # Скорость изменения\n",
    "    learning_rate = 1\n",
    "    \n",
    "    ##############################################################################\n",
    "    # Задание: сформировать обманное изображение X_fooling, которое модель       # \n",
    "    # будет классифицировать как класс target_y. Используйте градиентное         # \n",
    "    # восхождение по рейтинговой функции целевого класса, используя model.scores,#\n",
    "    # чтобы получить оценки рейтингов классов для model.image.                   #\n",
    "    # При вычислении шага обновления сначала нормализуете градиент:              #\n",
    "    #              dX = learning_rate * g / || g || _2                           #\n",
    "    #                                                                            #\n",
    "    # Вы должны написать обучающий цикл, на каждой итерации которого             #\n",
    "    # обновляется входное изображение X_fooling (не изменяйте X). Цикл должен    #\n",
    "    # завершиться, когда входное изображение будет классифицировано как target_y #\n",
    "    #                                                                            #\n",
    "    # СОВЕТ 1: Используйте tf.GradientTape для отслеживания градиентов и           #\n",
    "    # используйту tape.gradient для получения фактического градиента             #\n",
    "    # относительно X_fooling.                                                    #\n",
    "    #                                                                            #\n",
    "    # СОВЕТ 2: Для большинства входных примеров вы должны  генерировать          #\n",
    "    # обманывающее изображение менее чем за 100 итераций градиентного восхождения#\n",
    "    # Вы можете распечатать прогресс обучения, чтобы проверить алгоритм.         #\n",
    "    ##############################################################################\n",
    "    # *****НАЧАЛО ВАШЕГО КОДА (НЕ УДАЛЯЙТЕ/НЕ МОДИФИЦИРУЙТЕ ЭТУ СТРОКУ)*****\n",
    "    \n",
    "    pass\n",
    "\n",
    "    #*****КОНЕЦ ВАШЕГО КОДА (НЕ УДАЛЯЙТЕ/НЕ МОДИФИЦИРУЙТЕ ЭТУ СТРОКУ)*****\n",
    "    ##############################################################################\n",
    "    #                               Конец Вашего кода                            #\n",
    "    ##############################################################################\n",
    "    return X_fooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполните ячейку ниже, чтобы создать обманное изображение. Вы не должны видеть никаких существенных различий между исходным и обманывающим изображениями, но сеть будет делать  предсказание на обманывающем изображении как на корректном. Тем не менее, можно обнаружить небольшой случайный шум, если увеличить в 10 раз разницу между исходным и обманывающим изображениями. Изменяйте переменную `idx` ниже, чтобы исследовать разные изображения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [],
   "source": [
    "idx = 0\n",
    "Xi = X[idx][None]\n",
    "target_y = 6\n",
    "X_fooling = make_fooling_image(Xi, target_y, model)\n",
    "\n",
    "# Проверяем, что  X_fooling классифицируется как  y_target\n",
    "scores = model(X_fooling)\n",
    "assert tf.math.argmax(scores[0]).numpy() == target_y, 'The network is not fooled!'\n",
    "\n",
    "# Отображаем исходное, обманное изображения и разность\n",
    "orig_img = deprocess_image(Xi[0])\n",
    "fool_img = deprocess_image(X_fooling[0])\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "#Масшабирование разности\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(orig_img)\n",
    "plt.axis('off')\n",
    "plt.title(class_names[y[idx]])\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.imshow(fool_img)\n",
    "plt.title(class_names[target_y])\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.title('Difference')\n",
    "plt.imshow(deprocess_image((Xi-X_fooling)[0]))\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.title('Magnified difference (10x)')\n",
    "plt.imshow(deprocess_image(10 * (Xi-X_fooling)[0]))\n",
    "plt.axis('off')\n",
    "plt.gcf().tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Визуализация класса\n",
    "\n",
    "Начиная со случайного изображения  и выполняя градиентное восхождение на целевом классе, можно создать изображение, которое сеть распознает как целевой класс. Эта идея была впервые представлена в [2];в [3]  она была расширена  с использованием  нескольких методов регуляризации, которые могут улучшить качество генерируемого изображения.\n",
    "\n",
    "Конкретно, пусть $I$ будет изображением, а $y$ будет целевым классом. Пусть $s_y(I)$ будет оценкой, которую сверточная сеть присваивает изображению $I$ для класса $y$; обратите внимание, что это необработанные оценки, а не вероятности классови. Мы хотим сгенерировать изображение $I^*$, которое получает высокий рейтинг для класса $y$, решая задачу\n",
    "\n",
    "$$\n",
    "I^* = {\\arg\\max}_I (s_y(I) - R(I))\n",
    "$$\n",
    "\n",
    "где $R$ является (возможно, неявным) регуляризатором (обратите внимание на знак $R(I)$ в argmax: мы хотим минимизировать этот член регуляризации). Решить эту задачу оптимизации можно, используя градиентное восхождение, вычисляя градиенты относительно сгенерированного изображения. Будем использовать (явную) L2 регуляризацию вида\n",
    "\n",
    "$$\n",
    "R(I) = \\lambda \\|I\\|_2^2\n",
    "$$\n",
    "\n",
    "и неявную регуляризацию, как предложено в [3], периодически размывая сгенерированное изображение. Эту задачу можно решить, используя градиентное восхождение относительно генерируемого изображения.\n",
    "\n",
    "В приведенной ниже ячейке завершите реализацию функции `create_class_visualization`.\n",
    "\n",
    "[2] Karen Simonyan, Andrea Vedaldi, и Andrew Zisserman. \"Deep Inside Convolutional Networks: Visualising\n",
    "Image Classification Models and Saliency Maps\", ICLR Workshop 2014.\n",
    "\n",
    "[3] Yosinski и др. \"Understanding Neural Networks Through Deep Visualization\", ICML 2015 Deep Learning Workshop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "from scipy.ndimage.filters import gaussian_filter1d\n",
    "#Фильтр размытия\n",
    "def blur_image(X, sigma=1):\n",
    "    X = gaussian_filter1d(X, sigma, axis=1)\n",
    "    X = gaussian_filter1d(X, sigma, axis=2)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "def jitter(X, ox, oy):\n",
    "    \"\"\"\n",
    "    Вспомогательная функция, создающая случайный джиттер изображения\n",
    "    \n",
    "    Входы\n",
    "    - X: Тензор формы (N, H, W, C)\n",
    "    - ox, oy: Целые, задающие номера пикселей для джиттера вдоль осей W и H\n",
    "    \n",
    "    Возвращает: Новый тензор формы (N, H, W, C)\n",
    "    \"\"\"\n",
    "    if ox != 0:\n",
    "        left = X[:, :, :-ox]\n",
    "        right = X[:, :, -ox:]\n",
    "        X = tf.concat([right, left], axis=2)\n",
    "    if oy != 0:\n",
    "        top = X[:, :-oy]\n",
    "        bottom = X[:, -oy:]\n",
    "        X = tf.concat([bottom, top], axis=1)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_class_visualization(target_y, model, **kwargs):\n",
    "    \"\"\"\n",
    "    Генерирует изображение, максимизирующее рейтинг target_y предобученной модели\n",
    "    Входы:\n",
    "    - target_y: Целое в диапазоне [0, 1000), задающее номер класса\n",
    "    - model: Предобученная  CNN\n",
    "    \n",
    "    Ключевые аргументы:\n",
    "    - l2_reg: Коэффициент регуляризации L2 по изображению\n",
    "    - learning_rate: Шаг изменения \n",
    "    - num_iterations: Число итераций\n",
    "    - blur_every: Как часто размывать изображение, как неявный регуляризатор\n",
    "    - max_jitter: Максимальный джиттер, как неявный регуляризатор\n",
    "    - show_every: Как часто отображать результаты\n",
    "    \"\"\"\n",
    "    l2_reg = kwargs.pop('l2_reg', 1e-3)\n",
    "    learning_rate = kwargs.pop('learning_rate', 25)\n",
    "    num_iterations = kwargs.pop('num_iterations', 100)\n",
    "    blur_every = kwargs.pop('blur_every', 10)\n",
    "    max_jitter = kwargs.pop('max_jitter', 16)\n",
    "    show_every = kwargs.pop('show_every', 25)\n",
    "    \n",
    "    \n",
    "    # Используем одно изображение случайного шума в качестве отправной точки\n",
    "    X = 255 * np.random.rand(224, 224, 3)\n",
    "    X = preprocess_image(X)[None]\n",
    "\n",
    "    \n",
    "    grad = None # градиент потерь по отношению к model.image, размер как у  model.image\n",
    "    \n",
    "    X = tf.Variable(X)\n",
    "    for t in range(num_iterations):\n",
    "        # Случайный джиттер изображения; это дает немного лучшие результаты\n",
    "        ox, oy = np.random.randint(0, max_jitter, 2)\n",
    "        X = jitter(X, ox, oy)\n",
    "        \n",
    "        ########################################################################\n",
    "        # Задание: вычислить градиент рейтинга для класса  target_y            #\n",
    "        # относительно пикселей изображения и  шаг изменения изображения с     #\n",
    "        # использованием скорости обучения. Вы должны использовать             #\n",
    "        # tf.GradientTape () и tape.gradient для вычисления градиентов.        #\n",
    "        #                                                                      #\n",
    "        # Будьте очень осторожны с знаками элементов в вашем коде.             #\n",
    "        ########################################################################\n",
    "        # *****НАЧАЛО ВАШЕГО КОДА (НЕ УДАЛЯЙТЕ/НЕ МОДИФИЦИРУЙТЕ ЭТУ СТРОКУ)*****\n",
    "    \n",
    "        pass\n",
    "\n",
    "        #*****КОНЕЦ ВАШЕГО КОДА (НЕ УДАЛЯЙТЕ/НЕ МОДИФИЦИРУЙТЕ ЭТУ СТРОКУ)*****\n",
    "        ##############################################################################\n",
    "        #                               Конец Вашего кода                            #\n",
    "        ##############################################################################\n",
    "        \n",
    "        # Отменить джиттер\n",
    "        X = jitter(X, -ox, -oy)\n",
    "        \n",
    "        # Регуляризация за счет клиппирования и периодического размытия\n",
    "        X = tf.clip_by_value(X, -SQUEEZENET_MEAN/SQUEEZENET_STD, (1.0 - SQUEEZENET_MEAN)/SQUEEZENET_STD)\n",
    "        if t % blur_every == 0:\n",
    "            X = blur_image(X, sigma=0.5)\n",
    "\n",
    "        # Отображение изображения\n",
    "        if t == 0 or (t + 1) % show_every == 0 or t == num_iterations - 1:\n",
    "            plt.imshow(deprocess_image(X[0]))\n",
    "            class_name = class_names[target_y]\n",
    "            plt.title('%s\\nIteration %d / %d' % (class_name, t + 1, num_iterations))\n",
    "            plt.gcf().set_size_inches(4, 4)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того, как Вы завершите реализацию функции выше, запустите  следующую ячейку, чтобы сгенерировать образ Тарантула:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_y = 76 # Tarantula\n",
    "out = create_class_visualization(target_y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверьте  Вашу реализацию визуализации  на других классах! Вы также можете свободно манипулировать с различными гиперпараметрами, чтобы улучшить качество генерируемого изображения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_y = np.random.randint(1000)\n",
    "# target_y = 78 # Tick\n",
    "# target_y = 187 # Yorkshire Terrier\n",
    "# target_y = 683 # Oboe\n",
    "# target_y = 366 # Gorilla\n",
    "# target_y = 604 # Hourglass\n",
    "print(class_names[target_y])\n",
    "X = create_class_visualization(target_y, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
