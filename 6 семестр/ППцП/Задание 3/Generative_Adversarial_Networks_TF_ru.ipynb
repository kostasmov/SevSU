{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Генеративные состязательные сети (GAN - Generative Adversarial Networks)\n",
    "\n",
    "Все приложения нейронных сетей, которые мы исследовали ранее, были **дискриминативными моделями**, которые принимают входные данные и обучаются формировать выход с определенными метками.  В этом блокноте  создадим **генеративные модели**, используя нейронные сети. В частности, мы узнаем, как создавать модели, которые генерируют новые образы, напоминающие образы из наборов тренировочных данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "### Что такое  GAN?\n",
    "\n",
    "В 2014 году [Goodfellow и др.](https://arxiv.org/abs/1406.2661) представили метод обучения генеративных моделей, которые называются Генеративными состязательными сетями (сокращенно GAN). В GAN мы строим две отдельные нейронные сети. Первая сеть - это традиционная классификационная сеть, называемая **дискриминатором**. Мы обучаем дискриминатор воспринимать изображения и классифицировать их как реальные (принадлежащие обучающему набору) или поддельные (не представленные в обучающем наборе). Вторая сеть, называемая **генератором**, принимает случайный шум в качестве входного сигнала и преобразовывает его, используя нейронную сеть, в изображение. Цель генератора - обмануть дискриминатор, заставив его \"думать\", что сгенерированные изображения реальны.\n",
    "\n",
    "Мы можем представить себе этот процесс применения генератора ($G$), пытающегося обмануть дискриминатор  ($D$), \n",
    "и дискриминатора, пытающегося правильно классифицировать реальные и фальшивые образы как минимаксную игру:\n",
    "$$\\underset{G}{\\text{minimize}}\\; \\underset{D}{\\text{maximize}}\\; \\mathbb{E}_{x \\sim p_\\text{data}}\\left[\\log D(x)\\right] + \\mathbb{E}_{z \\sim p(z)}\\left[\\log \\left(1-D(G(z))\\right)\\right]$$\n",
    "где $x \\sim p_\\text{data}$  - выборки из входных данных, $z \\sim p(z)$ - выборки случайных шумов, $G(z)$ - сгенерированные изображения с использованием генератора нейронной сети $G$, и $D$ - это выходной сигнал дискриминатора, определяющий вероятность того, что вход является реальным. В [Goodfellow и др.](https://arxiv.org/abs/1406.2661)  анализируется эта минимаксная игра и демонстрируется как она связана с минимизацией дивергенции Дженсена-Шеннона между распределением обучающих данных и сгенерированными выборками из $G$.\n",
    "\n",
    "Чтобы оптимизировать эту минимаксную игру, будем выполнять выбор между шагами градиентного *спуска* по целевой функции для $G$ и шагами  градиентного *подъема* по целевой функции для $D$:\n",
    "1. обновить **генератор** ($G$), чтобы минимизировать вероятность того, что __дискриминатор сделает правильный выбор__.\n",
    "2. обновить **дискриминатор** ($D$), чтобы максимизировать вероятность того, что __дискриминатор сделает правильный выбор__.\n",
    "\n",
    "Хотя эти обновления полезны при анализе, они не очень хорошо работают на практике. Вместо этого мы будем использовать другую цель при обновлении генератора: максимизировать вероятность того, что **дискриминатор сделает неправильный выбор**. Это небольшое изменение помогает снизить остроту проблемы с исчезновением градиента генератора, когда дискриминатор уверен в своем решении. Это стандартное обновление, используется в большинстве работ по GAN , оно также использовалось в оригинальной статье [Goodfellow et al.](https://arxiv.org/abs/1406.2661).\n",
    "\n",
    "\n",
    "В этом задании мы будем чередовать следующие шаги обновления параметров нейросетей:\n",
    "1. Обновить генератор ($G$) , чтобы максимизировать вероятность того, что дискриминатор сделает неправильный выбор на сгенерированных данных:\n",
    "$$\\underset{G}{\\text{maximize}}\\;  \\mathbb{E}_{z \\sim p(z)}\\left[\\log D(G(z))\\right]$$\n",
    "2. Обновить дискриминатор ($D$), чтобы максимизировать вероятность того, что дискриминатор сделает правильный выбор на реальных и сгенерированных данных\n",
    "$$\\underset{D}{\\text{maximize}}\\; \\mathbb{E}_{x \\sim p_\\text{data}}\\left[\\log D(x)\\right] + \\mathbb{E}_{z \\sim p(z)}\\left[\\log \\left(1-D(G(z))\\right)\\right]$$\n",
    "\n",
    "\n",
    "\n",
    "### Что ещё?\n",
    "\n",
    "С 2014 года сети GAN превратились в огромную область исследований с  [сотнями новых статей](https://github.com/hindupuravinash/the-gan-zoo) . GAN являются одними из самых сложных и привередливых моделей для обучения (см.[github repo](https://github.com/soumith/ganhacks), который содержит набор из 17 хаков, которые полезны для работы с моделями). Улучшение стабильности и надежности обучения GAN - открытый вопрос исследования, каждый день появляются новые статьи! Классический учебный материал по GAN см. [здесь](https://arxiv.org/abs/1701.00160). Также есть  недавние захватывающие работы, которые заменяют целевую функцию на расстояние Вассерштейна и дают гораздо более стабильные результаты для следующих архитектур моделей: [WGAN](https://arxiv.org/abs/1701.07875), [WGAN-GP](https://arxiv.org/abs/1704.00028).\n",
    "\n",
    "GAN не единственный способ тренировать генеративную модель! Другие подходы к генеративному моделированию можно найти в [книге](http://www.deeplearningbook.org/contents/generative_models.html) .  Другой популярный способ обучения нейронных сетей как генеративных моделей - это вариационные автоэнкодеры (совместно предложенные [здесь](https://arxiv.org/abs/1312.6114) и [здесь](https://arxiv.org/abs/1401.4082)). Вариационные автоэнкодеры объединяют нейронные сети с вариационным выводом для обучения глубоких генеративных моделей. Эти модели имеют тенденцию быть намного более стабильными и более простыми в обучении, но в настоящее время они не формируют образы, которые столь же качественны, как GAN.\n",
    "\n",
    "Ниже приведены примеры изображений, которые вы должны ожидать в качестве результатов выполнения этого блокнота (ваши могут выглядеть немного иначе):\n",
    "\n",
    "![caption](gan_outputs_tf.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Установки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Набор всмогательных утилит\n",
    "\n",
    "def show_images(images):\n",
    "    \"\"\" Отображает изображения в клеточной структуре\"\"\"\n",
    "    images = np.reshape(images, [images.shape[0], -1])  #реформатирование изображений к (batch_size, D)\n",
    "    sqrtn = int(np.ceil(np.sqrt(images.shape[0])))\n",
    "    sqrtimg = int(np.ceil(np.sqrt(images.shape[1])))\n",
    "\n",
    "    fig = plt.figure(figsize=(sqrtn, sqrtn))\n",
    "    gs = gridspec.GridSpec(sqrtn, sqrtn)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(img.reshape([sqrtimg,sqrtimg]))\n",
    "    return\n",
    "\n",
    "def preprocess_img(x):\n",
    "    return 2 * x - 1.0\n",
    "\n",
    "def deprocess_img(x):\n",
    "    return (x + 1.0) / 2.0\n",
    "\n",
    "def rel_error(x,y):\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "def count_params(model):\n",
    "    \"\"\"Возвращает число параметров в текущем TensorFlow графе \"\"\"\n",
    "    param_count = np.sum([np.prod(p.shape) for p in model.weights])\n",
    "    return param_count\n",
    "\n",
    "answers = np.load('gan-checks-tf.npz')\n",
    "\n",
    "NOISE_DIM = 96"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "## Набор данных\n",
    " \n",
    "Общеизвестно, что GAN привередливы к значениям гиперпараметров, а также требуют многих тренировочных эпох. Чтобы сделать возможным выполнение этого задания  без графического процессора, мы будем работать с набором данных MNIST, который состоит из 60 000 обучающих и 10 000 тестовых изображений. Каждое изображение содержит центрированное изображение белой цифры на черном фоне (от 0 до 9). Это был один из первых наборов данных, использованных для обучения сверточных нейронных сетей, и он довольно прост - стандартная модель CNN легко классифицирует данные этого набора с точностью 99%.\n",
    " \n",
    "\n",
    "**Внимание**: класс MNIST возвращает изображения как векторы формы (размер блока, 784). Если Вы хотите обрабатывать их как изображения, то необходимо привести их к форме (размер блока, 28,28) или (размер блока, 28,28,1). Тип элементов np.float32, а значения ограничены интервалом [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определим вспомогательную функцию load_data для загрузки набора MNIST\n",
    "# Необходимо, чтобы набор данных mnist.npz\n",
    "# находился в текущей папке блокнота\n",
    "\n",
    "def load_data(path):\n",
    "    with np.load(path) as f:\n",
    "        x_train, y_train = f['x_train'], f['y_train']\n",
    "        x_test, y_test = f['x_test'], f['y_test']\n",
    "        return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "# Пример обращения к функции\n",
    "#(x_train, y_train), (x_test, y_test) = load_data('mnist.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "class MNIST(object):\n",
    "    def __init__(self, batch_size, shuffle=False):\n",
    "        \"\"\"\n",
    "        Создает итератор по данным MNIST\n",
    "        \n",
    "         Входы:\n",
    "         - batch_size: целое число, задающее количество элементов в мини-блоке\n",
    "         - shuffle: (необязательный) тип Boolean, следует ли перемешивать данные на каждой эпохе\n",
    "        \"\"\"\n",
    "        # Загрузка данных с помощью определенной выше функции load_data\n",
    "        train, _ = load_data('mnist.npz')\n",
    "        # Либо можно воспользоваться возможностями tf.keras.datasets\n",
    "        # train, _ = tf.keras.datasets.mnist.load_data()\n",
    "        \n",
    "        X, y = train\n",
    "        X = X.astype(np.float32)/255\n",
    "        X = X.reshape((X.shape[0], -1))\n",
    "        self.X, self.y = X, y\n",
    "        self.batch_size, self.shuffle = batch_size, shuffle\n",
    "\n",
    "    def __iter__(self):\n",
    "        N, B = self.X.shape[0], self.batch_size\n",
    "        idxs = np.arange(N)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(idxs)\n",
    "        return iter((self.X[i:i+B], self.y[i:i+B]) for i in range(0, N, B)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# отображение миниблока изображений\n",
    "mnist = MNIST(batch_size=16) \n",
    "show_images(mnist.X[:16])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функция активации: ReLU с утечкой (LeakyReLU)\n",
    "В приведенной ниже ячейке блокнота Вы должны реализовать LeakyReLU. См.   уравнение (3) в [статье](http://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf). LeakyReLU предохраняет  ReLU нейроны от \"умирания\" и они часто используются в  GAN (как и блоки maxout, при этом увеличивается размер модели).\n",
    "\n",
    "СОВЕТ: Вы должны использовать `tf.maximum`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(x, alpha=0.01):\n",
    "    \"\"\"    \n",
    "     Вычисление функции активации  ReLU с утечкой.\n",
    "    \n",
    "     Входы:\n",
    "     - x: Тензор TensorFlow  с произвольной формой\n",
    "     - alpha: параметр утечки для  ReLU с утечкой.\n",
    "    \n",
    "     Возвращает:\n",
    "     Тензор той же формы, что и x\n",
    "    \"\"\"\n",
    "    # Задание: реализовать ReLU с утечкой\n",
    "    # *****НАЧАЛО ВАШЕГО КОДА (НЕ УДАЛЯЙТЕ/НЕ МОДИФИЦИРУЙТЕ ЭТУ СТРОКУ)*****\n",
    "            \n",
    "    pass\n",
    "    return \n",
    "    #*****КОНЕЦ ВАШЕГО КОДА (НЕ УДАЛЯЙТЕ/НЕ МОДИФИЦИРУЙТЕ ЭТУ СТРОКУ)*****\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Протестируйте Вашу реализацию leaky ReLU. Ошибки должны быть < 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_leaky_relu(x, y_true):\n",
    "    y = leaky_relu(tf.constant(x))\n",
    "    print('Максимум ошибки: %g'%rel_error(y_true, y))\n",
    "\n",
    "test_leaky_relu(answers['lrelu_x'], answers['lrelu_y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Случайный шум\n",
    "\n",
    "Сгенерируйте `Tensor` TensorFlow, содержащий равномерно распределенный шум от -1 до 1, размерность тензора `[batch_size, dim]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_noise(batch_size, dim):\n",
    "    \"\"\"\n",
    "     Генерирует случайный равномерный шум в диапазоне от -1 до 1.\n",
    "  \n",
    "     Входы:\n",
    "     - batch_size: целое число, задающее размер миниблока данных шума\n",
    "     - dim: целое число, определяющее длину вектора шума\n",
    "    \n",
    "     Возвращает:\n",
    "     Тензор TensorFlow , содержащий равномерный шум в [-1, 1] с формой [batch_size, dim]\n",
    "    \"\"\"\n",
    "    # Задание: сделать выборку и вернуть шум\n",
    "    # *****НАЧАЛО ВАШЕГО КОДА (НЕ УДАЛЯЙТЕ/НЕ МОДИФИЦИРУЙТЕ ЭТУ СТРОКУ)*****\n",
    "            \n",
    "    pass\n",
    "    return \n",
    "    #*****КОНЕЦ ВАШЕГО КОДА (НЕ УДАЛЯЙТЕ/НЕ МОДИФИЦИРУЙТЕ ЭТУ СТРОКУ)*****\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Убедитесь, что шум имеет правильный размер и тип:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sample_noise():\n",
    "    batch_size = 3\n",
    "    dim = 4\n",
    "    z = sample_noise(batch_size, dim)\n",
    "    # Проверьте, что z имеет правильную форму\n",
    "    assert z.get_shape().as_list() == [batch_size, dim]\n",
    "    # Проверьте, что z тензор, а не  numpy массив\n",
    "    assert isinstance(z, tf.Tensor)\n",
    "    # Проверьте, что мы получаем различный шум в разных попытках\n",
    "    z1 = sample_noise(batch_size, dim)\n",
    "    z2 = sample_noise(batch_size, dim)\n",
    "    assert not np.array_equal(z1, z2)\n",
    "    # Проверьте, что шум находится в заданном диапазоне\n",
    "    assert np.all(z1 >= -1.0) and np.all(z1 <= 1.0)\n",
    "    print(\"Все тесты пройдены!\")\n",
    "    \n",
    "test_sample_noise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дискриминатор\n",
    "\n",
    "Выполним первый шаг - построим дискриминатор. Подсказка: Вы должны использовать слои из `tf.keras.layers` для построения модели. Все полносвязанные слои должны содержать  смещения. Для инициализации  используйте инициализатор по умолчанию, реализуемый функциями `tf.keras.layers`.\n",
    "\n",
    "Архитектура в порядке следования слоев:\n",
    "\n",
    "     * Полносвязанный слой с размерами: вход - 784 и выход - 256\n",
    "     * LeakyReLU с альфа 0,01\n",
    "     * Полносвязанный слой с выходным размером 256\n",
    "     * LeakyReLU с альфа 0,01\n",
    "     * Полносвязанный слой с выходным размером 1\n",
    "\n",
    "Таким образом, выходные данные дискриминатора должны иметь форму [batch_size, 1] и содержать действительные числа, соответствующие рейтингам каждого входного изображения из мини блока размером `batch_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator():\n",
    "    \"\"\"\n",
    "     Вычислить рейтинги на выходе дискриминатора для миниблока входных изображений.\n",
    "    \n",
    "     Входы:\n",
    "     - x: Тензор TensorFlow  сглаженных входных изображений, форма [batch_size, 784]\n",
    "    \n",
    "     Возвращает:\n",
    "     Тензор ТensorFlow  с формой [batch_size, 1], содержащий рейтинги\n",
    "     принадлежности входных изображений к реальным изображениям.\n",
    "    \"\"\"\n",
    "    model = tf.keras.models.Sequential([\n",
    "        # Задание: реализовать архитектуру, указанную выше\n",
    "        # *****НАЧАЛО ВАШЕГО КОДА (НЕ УДАЛЯЙТЕ/НЕ МОДИФИЦИРУЙТЕ ЭТУ СТРОКУ)*****\n",
    "         \n",
    "        \n",
    "        #*****КОНЕЦ ВАШЕГО КОДА (НЕ УДАЛЯЙТЕ/НЕ МОДИФИЦИРУЙТЕ ЭТУ СТРОКУ)*****\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Проверьте на правильность количество параметров дискриминатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_discriminator(true_count=267009):\n",
    "    model = discriminator()\n",
    "    cur_count = count_params(model)\n",
    "    if cur_count != true_count:\n",
    "        print('Неправильное количество параметров дискриминатора. {0} вмсето {1}. Проверьте Вашу архитектуру.'.format(cur_count,true_count))\n",
    "    else:\n",
    "        print('Ваша архитектура имеет правильное количество параметров дискриминатора.')\n",
    "        \n",
    "test_discriminator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Генератор\n",
    "\n",
    "Теперь построим генератор. Вы должны использовать слои из `tf.keras.layers` для построения модели. Все полносвязанные слои должны содержать смещения. Обратите внимание, что можно использовать модуль tf.nn для доступа к функциям активации. Еще раз, используйте инициализаторы по умолчанию для параметров.\n",
    "\n",
    "Архитектура генератора:\n",
    "  * Полносвязанный слой с размером входа tf.shape (z) [1] (количество измерений шума) и размером выхода 1024\n",
    "  * `ReLU`\n",
    "  * Полносвязанный слой с выходным размером 1024\n",
    "  * `ReLU`\n",
    "  * Полносвязанный слой с выходным размером 784\n",
    "  * `TanH` (чтобы ограничить выходные значения диапазоном [-1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(noise_dim=NOISE_DIM):\n",
    "    \"\"\"\n",
    "     Генерирует изображения из случайного вектора шума.\n",
    "    \n",
    "     Входы:\n",
    "     - z: Тензор, содержащий случайный шум с формой [batch_size, noise_dim]\n",
    "    \n",
    "     Возвращает:\n",
    "     Тензор  сгенерированных изображений формы [batch_size, 784].\n",
    "    \"\"\"\n",
    "    model = tf.keras.models.Sequential([\n",
    "        # Задание: реализовать архитектуру\n",
    "        # *****НАЧАЛО ВАШЕГО КОДА (НЕ УДАЛЯЙТЕ/НЕ МОДИФИЦИРУЙТЕ ЭТУ СТРОКУ)*****\n",
    "         \n",
    "        \n",
    "        #*****КОНЕЦ ВАШЕГО КОДА (НЕ УДАЛЯЙТЕ/НЕ МОДИФИЦИРУЙТЕ ЭТУ СТРОКУ)*****\n",
    "        \n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверьте на правильность число параметров генератора:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_generator(true_count=1858320):\n",
    "    model = generator(4)\n",
    "    cur_count = count_params(model)\n",
    "    if cur_count != true_count:\n",
    "        print('Неправильное количество параметров генератора. {0} вместо {1}. Проверьте Ващу архитектуру.'.format(cur_count,true_count))\n",
    "    else:\n",
    "        print('Ваша архитектура имеет правильное количество параметров генератора.')\n",
    "        \n",
    "test_generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Потери GAN\n",
    "\n",
    "\n",
    "Вычислите потери генератора и дискриминатора. Потери генератора составляют:\n",
    "$$\\ell_G  =  -\\mathbb{E}_{z \\sim p(z)}\\left[\\log D(G(z))\\right]$$\n",
    "а потери дискриминатора равны:\n",
    "$$ \\ell_D = -\\mathbb{E}_{x \\sim p_\\text{data}}\\left[\\log D(x)\\right] - \\mathbb{E}_{z \\sim p(z)}\\left[\\log \\left(1-D(G(z))\\right)\\right]$$\n",
    "\n",
    "Обратите внимание, что они снижаются, поскольку мы будем «минимизировать» эти потери.\n",
    "\n",
    "**ПОДСКАЗКА**: Используйте `tf.ones` и `tf.zeros`  для генерации меток для дискриминатора. Используйте `tf.keras.losses.BinaryCrossentropy`  для вычисления функции потерь.\n",
    "\n",
    "Если ячейка ниже заполнена, то просто ознакомьтесь с кодом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(logits_real, logits_fake):\n",
    "    \"\"\"\n",
    "     Вычисляет потери дискриминатора, описанные выше.\n",
    "    \n",
    "     Входы:\n",
    "     - logits_real: тензор формы (N, 1), содержащий рейтинги для реальных данных.\n",
    "     - logits_fake: тензор формы (N, 1), содержащий  рейтинги для поддельных данных.\n",
    "    \n",
    "     Возвращает:\n",
    "     - loss: Тензор, содержащий (скаляр) потери дискриминатора.\n",
    "    \"\"\"\n",
    "    loss = None\n",
    "    # Задание: определите функцию\n",
    "    # *****НАЧАЛО ВАШЕГО КОДА (НЕ УДАЛЯЙТЕ/НЕ МОДИФИЦИРУЙТЕ ЭТУ СТРОКУ)*****\n",
    "    bce = tf.keras.losses.BinaryCrossentropy(from_logits = True)\n",
    "    ones = tf.ones_like(logits_real)\n",
    "    zeros = tf.zeros_like(logits_fake)\n",
    "    loss = bce(ones, logits_real) + bce(zeros, logits_fake)        \n",
    "    pass\n",
    "\n",
    "    #*****КОНЕЦ ВАШЕГО КОДА (НЕ УДАЛЯЙТЕ/НЕ МОДИФИЦИРУЙТЕ ЭТУ СТРОКУ)*****\n",
    "    return loss\n",
    "\n",
    "def generator_loss(logits_fake):\n",
    "    \"\"\"\n",
    "     Вычисляет потери генератора, описанные выше.\n",
    "\n",
    "     Входы:\n",
    "     - logits_fake: Тензор формы (N,), содержащий рейтинги  для поддельных данных.\n",
    "    \n",
    "     Возвращает:\n",
    "     - loss: Тензор , содержащий (скаляр) потери генератора.\n",
    "    \"\"\"\n",
    "    loss = None\n",
    "    # Задание: определите функцию\n",
    "    #*****НАЧАЛО ВАШЕГО КОДА (НЕ УДАЛЯЙТЕ/НЕ МОДИФИЦИРУЙТЕ ЭТУ СТРОКУ)*****\n",
    "    bce = tf.keras.losses.BinaryCrossentropy(from_logits = True)\n",
    "    ones = tf.ones_like(logits_fake)\n",
    "    loss = bce(ones, logits_fake)        \n",
    "    pass\n",
    "\n",
    "    #*****КОНЕЦ ВАШЕГО КОДА (НЕ УДАЛЯЙТЕ/НЕ МОДИФИЦИРУЙТЕ ЭТУ СТРОКУ)*****\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверьте вычисление потерь GAN. Убедитесь, что потери генератора и дискриминатора вычисляются правильно. Ошибка должна быть менее 1e-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_discriminator_loss(logits_real, logits_fake, d_loss_true):\n",
    "    d_loss = discriminator_loss(tf.constant(logits_real),\n",
    "                                tf.constant(logits_fake))\n",
    "    print(\"Максимум ошибки d_loss: %g\"%rel_error(d_loss_true, d_loss))\n",
    "\n",
    "test_discriminator_loss(answers['logits_real'], answers['logits_fake'],\n",
    "                        answers['d_loss_true'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_generator_loss(logits_fake, g_loss_true):\n",
    "    g_loss = generator_loss(tf.constant(logits_fake))\n",
    "    print(\"Максимум ошибки g_loss: %g\"%rel_error(g_loss_true, g_loss))\n",
    "\n",
    "test_generator_loss(answers['logits_fake'], answers['g_loss_true'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Оптимизация потерь\n",
    "Создайте оптимизатор `Adam` со скоростью обучения 1e-3, beta1 = 0,5, чтобы минимизировать G_loss и D_loss по отдельности. Трюк  с уменьшением beta показал свою эффективность для схождения GAN, см. статью [Improved Techniques for Training GANs](https://arxiv.org/abs/1606.03498) . На самом деле, если вы установите для beta1 значение по умолчанию, равное 0,9, есть большая вероятность того, что  потери дискриминатора упадут до нуля, и генератор не сможет полностью обучиться. Фактически, это распространенный режим отказа  GAN: если  D(x) обучается слишком быстро (например, потери приближаются к нулю), то G(z) никогда не сможет обучиться. Часто D(x) обучают на основе SGD с моментом или RMSProp вместо использования Adam , но здесь мы будем использовать Adam  как для D(x), так и для G(z)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Задание: создайте оптимизатор Adam для D_solver и G_solver\n",
    "def get_solvers(learning_rate=1e-3, beta1=0.5):\n",
    "    \"\"\"\n",
    "     Создает оптимизаторы для обучения GAN.\n",
    "    \n",
    "     Входы:\n",
    "     - learning_rate: скорость обучения для обоих решателей\n",
    "     - beta1: параметр beta1 для обоих решателей (затухание в первого момента)\n",
    "    \n",
    "     Возвращает:\n",
    "     - D_solver: экземпляр tf.optimizers.Adam с корректной скоростью обучения и beta1\n",
    "     - G_solver: экземпляр tf.optimizers.Adam с корректной скоростью обучения и beta1\n",
    "    \"\"\"\n",
    "    D_solver = None\n",
    "    G_solver = None\n",
    "    # Задание: определите функцию\n",
    "    #*****НАЧАЛО ВАШЕГО КОДА (НЕ УДАЛЯЙТЕ/НЕ МОДИФИЦИРУЙТЕ ЭТУ СТРОКУ)*****\n",
    "           \n",
    "    pass\n",
    "\n",
    "    #*****КОНЕЦ ВАШЕГО КОДА (НЕ УДАЛЯЙТЕ/НЕ МОДИФИЦИРУЙТЕ ЭТУ СТРОКУ)*****\n",
    "    return D_solver, G_solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "# Обучение GAN!\n",
    "После первой эпохи Вы должны видеть нечеткие контуры, четкие изображения при приближении к эпохе 3 и хорошие изображения, примерно половина из которых будет четкой и отчетливо узнаваемой, при прохождении эпохи 5. В нашем случае мы просто будем обучать D(х) и G(z) на одном миниблоке на каждой итерации. Тем не менее, часто экспериментируют с различными расписаниями обучения D(x) и G(z), иногда делая больше шагов для D(х) или G(z), или даже обучают их по отдельности, пока потери не станут «достаточно хорошими» для одной подсети, а затем переключаются на обучение другой подсети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "# Гигантская вспомогательная функция\n",
    "def run_a_gan(D, G, D_solver, G_solver, discriminator_loss, generator_loss,\\\n",
    "              show_every=20, print_every=20, batch_size=128, num_epochs=10, noise_size=96):\n",
    "    \"\"\"\n",
    "     Обучение GAN на определенном количестве эпох.\n",
    "    \n",
    "     Входы:\n",
    "     - D: модель дискриминатора\n",
    "     - G: модель генератора\n",
    "     - D_solver: оптимизатор для дискриминатора\n",
    "     - G_solver: оптимизатор для генератора\n",
    "     - generator_loss: потери генератора\n",
    "     - discriminator_loss: потери дискриминатора\n",
    "     Возвращает: Ничего\n",
    "    \"\"\"\n",
    "    mnist = MNIST(batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    iter_count = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        for (x, _) in mnist:\n",
    "            with tf.GradientTape() as tape:\n",
    "                real_data = x\n",
    "                logits_real = D(preprocess_img(real_data))\n",
    "\n",
    "                g_fake_seed = sample_noise(batch_size, noise_size)\n",
    "                fake_images = G(g_fake_seed)\n",
    "                logits_fake = D(tf.reshape(fake_images, [batch_size, 784]))\n",
    "\n",
    "                d_total_error = discriminator_loss(logits_real, logits_fake)\n",
    "                d_gradients = tape.gradient(d_total_error, D.trainable_variables)      \n",
    "                D_solver.apply_gradients(zip(d_gradients, D.trainable_variables))\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                g_fake_seed = sample_noise(batch_size, noise_size)\n",
    "                fake_images = G(g_fake_seed)\n",
    "\n",
    "                gen_logits_fake = D(tf.reshape(fake_images, [batch_size, 784]))\n",
    "                g_error = generator_loss(gen_logits_fake)\n",
    "                g_gradients = tape.gradient(g_error, G.trainable_variables)      \n",
    "                G_solver.apply_gradients(zip(g_gradients, G.trainable_variables))\n",
    "\n",
    "            if (iter_count % show_every == 0):\n",
    "                print('Эпоха: {}, Итерация: {}, D: {:.4}, G:{:.4}'.format(epoch, iter_count,d_total_error,g_error))\n",
    "                imgs_numpy = fake_images.cpu().numpy()\n",
    "                show_images(imgs_numpy[0:16])\n",
    "                plt.show()\n",
    "            iter_count += 1\n",
    "    \n",
    "    # случайный шум, подаваемый на вход генератора\n",
    "    z = sample_noise(batch_size, noise_size)\n",
    "    # сгенерированное изображение\n",
    "    G_sample = G(z)\n",
    "    print('Конечные изображения')\n",
    "    show_images(G_sample[:16])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обучите Вашу GAN. Это займет 10 минут на CPU или около 2 минут на GPU.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Создание дискриминатора\n",
    "D = discriminator()\n",
    "\n",
    "# Создание генератора\n",
    "G = generator()\n",
    "\n",
    "# Используйте функцию, которую вы написали ранее, чтобы получить оптимизаторы для  дискриминатора и генератора\n",
    "D_solver, G_solver = get_solvers()\n",
    "\n",
    "# Запуск!\n",
    "run_a_gan(D, G, D_solver, G_solver, discriminator_loss, generator_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN на основе метода наименьших квадратов\n",
    "Теперь мы рассмотрим [GAN на основе наименьших квадратов](https://arxiv.org/abs/1611.04076) - более новую, более стабильную альтернативу исходной функции потерь GAN. Для этой части все, что нам нужно сделать, это изменить функцию потерь и переобучить модель. Мы реализуем уравнение (9) из статьи. Потери генератора:\n",
    "$$\\ell_G  =  \\frac{1}{2}\\mathbb{E}_{z \\sim p(z)}\\left[\\left(D(G(z))-1\\right)^2\\right]$$\n",
    "Потери дискриминатора:\n",
    "$$ \\ell_D = \\frac{1}{2}\\mathbb{E}_{x \\sim p_\\text{data}}\\left[\\left(D(x)-1\\right)^2\\right] + \\frac{1}{2}\\mathbb{E}_{z \\sim p(z)}\\left[ \\left(D(G(z))\\right)^2\\right]$$\n",
    "\n",
    "**СОВЕТЫ**: Вместо того, чтобы вычислять математическое ожидание, будем выполнять усреднение по элементам мини-блока, поэтому убедитесь, что Вы вычисляете потери путем усреднения, а не суммирования. При подключении к $D(x)$ и $D(G(z))$ используйте прямой выход дискриминатора (`score_real` и` score_fake`).\n",
    "\n",
    "Если ячейка ниже заполнена, то просто ознакомьтесь с кодом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ls_discriminator_loss(scores_real, scores_fake):\n",
    "    \"\"\"\n",
    "     Вычисляет потери наименьших квадратов для дискриминатора GAN .\n",
    "     Входы:\n",
    "     - scores_real: тензор формы (N, 1), содержащий рейтинги для реальных данных.\n",
    "     - scores_fake: тензор формы (N, 1), содержащий рейтинги для поддельных данных.\n",
    "    \n",
    "     Выходы:\n",
    "     - loss: Тензор, содержащий потери\n",
    "    \n",
    "     \"\"\"\n",
    "    loss = None\n",
    "    # Задание: определите функцию\n",
    "    #*****НАЧАЛО ВАШЕГО КОДА (НЕ УДАЛЯЙТЕ/НЕ МОДИФИЦИРУЙТЕ ЭТУ СТРОКУ)*****\n",
    "    loss = 0.5*tf.reduce_mean(tf.square(scores_real - 1)) + 0.5*tf.reduce_mean(tf.square(scores_fake))        \n",
    "    pass\n",
    "\n",
    "    #*****КОНЕЦ ВАШЕГО КОДА (НЕ УДАЛЯЙТЕ/НЕ МОДИФИЦИРУЙТЕ ЭТУ СТРОКУ)*****\n",
    "    return loss\n",
    "\n",
    "def ls_generator_loss(scores_fake):\n",
    "    \"\"\"\n",
    "     Вычисляет потери  наименьших квадратов для генератора GAN.\n",
    "    \n",
    "     Входы:\n",
    "     - scores_fake: тензор формы (N, 1), содержащий рейтинги для поддельных данных.\n",
    "    \n",
    "     Выходы:\n",
    "     - loss: Тензор, содержащий потери.\n",
    "    \"\"\"\n",
    "    loss = None\n",
    "    \n",
    "    #*****НАЧАЛО ВАШЕГО КОДА (НЕ УДАЛЯЙТЕ/НЕ МОДИФИЦИРУЙТЕ ЭТУ СТРОКУ)*****\n",
    "    loss = 0.5*tf.reduce_mean((tf.square(scores_fake-1)))        \n",
    "    pass\n",
    "\n",
    "    #*****КОНЕЦ ВАШЕГО КОДА (НЕ УДАЛЯЙТЕ/НЕ МОДИФИЦИРУЙТЕ ЭТУ СТРОКУ)*****\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверьте LSGAN потери. Ошибки должны быть менее 1e-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_lsgan_loss(score_real, score_fake, d_loss_true, g_loss_true):\n",
    "    \n",
    "    d_loss = ls_discriminator_loss(tf.constant(score_real), tf.constant(score_fake))\n",
    "    g_loss = ls_generator_loss(tf.constant(score_fake))\n",
    "    print(\"Максимальная ошибка d_loss: %g\"%rel_error(d_loss_true, d_loss))\n",
    "    print(\"Максимальная ошибка g_loss: %g\"%rel_error(g_loss_true, g_loss))\n",
    "\n",
    "test_lsgan_loss(answers['logits_real'], answers['logits_fake'],\n",
    "                answers['d_loss_lsgan_true'], answers['g_loss_lsgan_true'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создайте новые обучающие шаги, так чтобы минимизировать LSGAN потери:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание дискриминатора\n",
    "D = discriminator()\n",
    "\n",
    "# Создание генератора\n",
    "G = generator()\n",
    "\n",
    "# Используйте функцию, которую вы написали ранее, чтобы получить оптимизаторы для  дискриминатора и генератора\n",
    "D_solver, G_solver = get_solvers()\n",
    "\n",
    "# Запуск!\n",
    "run_a_gan(D, G, D_solver, G_solver, ls_discriminator_loss, ls_generator_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Глубокие сверточные GAN сети\n",
    "\n",
    "В первой части блокнота мы реализовали почти прямую копию оригинальной сети GAN  Яна Гудфеллоу. Однако эта сетевая архитектура не обеспечивает понимание реальных пространственных отношений. Она не может делать общих заключений о таких составляющих изображений, как «края», потому что в ней отсутствуют какие-либо сверточные слои. В этом разделе мы реализуем некоторые идеи из [DCGAN](https://arxiv.org/abs/1511.06434), где  используются сверточные сети в качестве дискриминаторов и генераторов.\n",
    "\n",
    "#### Дискриминатор\n",
    "Будем использовать дискриминатор, инспирированный  классификатором TensorFlow MNIST,который способен довольно быстро получить точность на данных MNIST выше 99%. *Обязательно проверьте размеры x и измените их при необходимости*, полносвязанные слои принимают [N, D] тензоры, в то время как блоки conv2d принимают  [N, H, W, C] тензоры. Используйте `tf.keras.layers`, чтобы определить следующую архитектуру:\n",
    "\n",
    "* Conv2D: 32 Фильтра  5x5, Stride 1, padding 0\n",
    "* Leaky ReLU(alpha=0.01)\n",
    "* Max Pool 2x2, Stride 2\n",
    "* Conv2D: 64 Фильтра 5x5, Stride 1, padding 0\n",
    "* Leaky ReLU(alpha=0.01)\n",
    "* Max Pool 2x2, Stride 2\n",
    "* Уплощение (Flatten)\n",
    "* Полносвязанный слой с размером выхода 4 x 4 x 64\n",
    "* Leaky ReLU(alpha=0.01)\n",
    "* Полносвязанный слой с размером выхода 1\n",
    "\n",
    "\n",
    "Используйте смещения для всех сверточных и полносвязанных слоев и используйте инициализаторы параметров по умолчанию. Обратите внимание, что дополнение 0 может быть выполнено с помощью параметра padding = 'VALID'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator():\n",
    "    \"\"\"\n",
    "     Вычисляет рейтинг дискриминатора для миниблока входных изображений.\n",
    "    \n",
    "     Входы:\n",
    "     - x: Тензор TensorFlow  сглаженных входных изображений, форма [batch_size, 784]\n",
    "    \n",
    "     Возвращает:\n",
    "     Тензор TensorFlow  с формой [batch_size, 1], содержащий рейтинги\n",
    "     принадлежности входных изображений к реальным.\n",
    "    \"\"\"\n",
    "    model = tf.keras.models.Sequential([\n",
    "        # Задание: реализовать архитектуру\n",
    "        # *****НАЧАЛО ВАШЕГО КОДА (НЕ УДАЛЯЙТЕ/НЕ МОДИФИЦИРУЙТЕ ЭТУ СТРОКУ)*****\n",
    "       \n",
    "        \n",
    "        #*****КОНЕЦ ВАШЕГО КОДА (НЕ УДАЛЯЙТЕ/НЕ МОДИФИЦИРУЙТЕ ЭТУ СТРОКУ)*****\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = discriminator()\n",
    "test_discriminator(1102721)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Генератор\n",
    "Для генератора мы скопируем архитектуру из статьи [InfoGAN](https://arxiv.org/pdf/1606.03657.pdf). См. Приложение C.1 MNIST. Используйте `tf.keras.layers`  для своей реализации. Будет полезно знакомство с `tf.keras.layers.Conv2DTranspose`. Архитектура выглядит следующим образом:\n",
    "\n",
    "\n",
    "* Полносвязанный слой с размером выхода 1024 \n",
    "* `ReLU`\n",
    "* Блочная нормализация (batchNorm)\n",
    "* Полносвязанный слой с размером выхода 7 x 7 x 128 \n",
    "* `ReLU`\n",
    "* Блочная нормализация (batchNorm)\n",
    "* Приведение размера тензора изображения к 7, 7, 128\n",
    "* Conv2DTranspose: 64 фильтра 4x4, stride 2\n",
    "* `ReLU`\n",
    "* Блочная нормализация (batchNorm)\n",
    "* Conv2DTranspose: 1 фильтр of 4x4, stride 2\n",
    "* `TanH`\n",
    "\n",
    "Используйте смещения для полносвязанных и транспонирующих сверточных слоев. Используйте инициализаторы по умолчанию для  параметров. Для параметра padding выберите значение same в случае транспонированной свертки. Для  блочной нормализации предполагаем, что она выполняется всегда в режиме «обучения»."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(noise_dim=NOISE_DIM):\n",
    "    \"\"\"\n",
    "     Генерация изображений из случайного вектора шума.\n",
    "    \n",
    "     Входы:\n",
    "     - z: Тензор TensorFlow случайного шума с формой [batch_size, noise_dim]\n",
    "    \n",
    "     Возвращает:\n",
    "      Тензор TensorFlow  сгенерированных изображений с формой [batch_size, 784].\n",
    "    \"\"\"\n",
    "    model = tf.keras.models.Sequential([\n",
    "        # Задание: реализовать архитектуру\n",
    "        # *****НАЧАЛО ВАШЕГО КОДА (НЕ УДАЛЯЙТЕ/НЕ МОДИФИЦИРУЙТЕ ЭТУ СТРОКУ)*****\n",
    "          \n",
    "        ])\n",
    "        #*****КОНЕЦ ВАШЕГО КОДА (НЕ УДАЛЯЙТЕ/НЕ МОДИФИЦИРУЙТЕ ЭТУ СТРОКУ)*****\n",
    "    return model\n",
    "test_generator(6595521)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Мы должны воссоздать нашу сеть, так как мы изменили наши функции.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Обучение и оценка DCGAN\n",
    "Это та часть задания, которая значительно выигрывает от использования графического процессора. Требуется 3 минуты на GPU для запрошенных пяти эпох. Или около 50 минут на двухъядерном ноутбуке с CPU (используйте именно 3 эпохи, если вы выполняете задачу на CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание дискриминатора\n",
    "D = discriminator()\n",
    "\n",
    "# Создание генератора\n",
    "G = generator()\n",
    "\n",
    "# Используйте функцию, которую вы написали ранее, чтобы получить оптимизаторы для  дискриминатора и генератора\n",
    "D_solver, G_solver = get_solvers()\n",
    "\n",
    "# Запуск!\n",
    "run_a_gan(D, G, D_solver, G_solver, discriminator_loss, generator_loss, num_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "## Вопрос 1\n",
    "\n",
    "Рассмотрим пример, чтобы понять, почему альтернативная минимизация одной и той же цели (как в GAN) может быть непростым делом.\n",
    "\n",
    "Рассмотрим $f(x,y)=xy$. Что оценивает следующая мимнимаксная функция $\\min_x\\max_y f(x,y)$? (Подсказка: minmax пытается минимизировать максимально достижимое значение.)\n",
    "\n",
    "Теперь попробуйте оценить эту функцию численно за 6 шагов, начиная с точки $(1,1)$,\n",
    "и попеременно используя градиенты переменных (сначала обновляя y, затем обновляя x с использованием этого обновленного y) с размером шага $1$. **Здесь размер шага - скорость обучения, а  сами обновления будут равны: \n",
    "скорость_обучения * градиент**.\n",
    "Будет полезным записать шаг обновления в терминах $x_t,y_t,x_{t+1},y_{t+1}$ .\n",
    "\n",
    "Кратко объясните, что оценивает $\\min_x\\max_y f(x,y)$, и запишите шесть пар явных значений для $ (x_t, y_t) $ в таблице ниже.\n",
    "\n",
    "### Ваши ответы:\n",
    " \n",
    " $y_0$ | $y_1$ | $y_2$ | $y_3$ | $y_4$ | $y_5$ | $y_6$ \n",
    " ----- | ----- | ----- | ----- | ----- | ----- | ----- \n",
    "   1   |       |       |       |       |       |       \n",
    " $x_0$ | $x_1$ | $x_2$ | $x_3$ | $x_4$ | $x_5$ | $x_6$ \n",
    "   1   |       |       |       |       |       |       \n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "## Вопрос 2\n",
    "\n",
    "Используя этот метод, мы когда-нибудь достигнем оптимального значения? Почему да или почему нет?\n",
    "\n",
    "### Ваш ответ:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "## Вопрос 3\n",
    "Если потери генератора уменьшаются во время обучения, в то время как потери дискриминатора с самого начала остаются на постоянном высоком значении, это хороший знак? Почему да или почему нет? Достаточно качественного ответа. \n",
    "### Ваш ответ:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
