import numpy as np
from random import shuffle

def svm_loss_naive(W, X, y, reg):
  """
  SVM функция потерь, наивная реализация (с циклами).

  D - длина вектора данных, C - число классов,
  операции выполняются над миниблоками с N примерами.

  Входы:
  - W: numpy масссив формы (D, C), содержащий веса.
  - X: numpy масссив формы (N, D), содержащий миниблок данных.
  - Y: numpy масссив формы (N,), содержащий обучающие метки; y[i] = c означает,
    что x[i] имеет метку c, где 0 <= c < C.
  - reg: (float) коэффициент регуляризации

  Возращает кортеж из:
  - потери в формате single float
  - градиент по отношению к весам W; массив такой же формы как W
  """

  dW = np.zeros(W.shape) # инициализируем градиент

  num_classes = W.shape[1]
  num_train = X.shape[0]

  #############################################################################
  # ЗАДАНИЕ:                                                                  #
  # Вычислите градиент функции потерь по отношению к W и сохраните его в dW.  #
  # Вместо того, чтобы сначала вычислять функцию потерь, а затем вычислять    #
  # производную, вычисляйте производную в процессе вычисления функции потерь. #
  # Поэтому просто модифицируйте код, приведенный ниже, включив в него        #
  # вычисление требуемого градиента                                           #
  #############################################################################
  
  # вычисление функции потерь по всем обучающим примерам X[i]   
  loss = 0.0
  for i in range(num_train):
    scores = X[i].dot(W)
    correct_class_score = scores[y[i]]

    # вычисление функции потерь L_i для X[i] и её аналитического градиента dwL_i
    count = 0     # число классов, не удовлетворяющих отступу
    for j in range(num_classes):
      margin = scores[j] - correct_class_score + 1  # вычисляем отступ для класса j

      # если отступ > 0 и j != корректному классу y[i]
      if margin > 0 and j != y[i]:
        count += 1            # увеличиваем счётчик рейтингов классов, не удовлетворяющих условию SVM
        dW[:,j] += X[i,:].T   # градиент некорректного класса, суммируем все градиенты по миниблоку

      if j == y[i]:
        margin = 0  # если класс корректный, то для него Li=0

      if margin > 0:
        loss += margin

    # вычисляем вектор градиента для коррект. класса
    dW[:,y[i]] += (-count) * (X[i,:].T)   # суммируем все градиенты по миниблоку
            
  # усреднение
  loss /= num_train
  dW /= num_train

  # регуляризация
  loss += reg * np.sum(W * W)
  dW += reg * 2 * W
  
  return loss, dW


def svm_loss_vectorized(W, X, y, reg):
  """
  SVM функцмя потерь, векторизованная реализация.

  Входы и выходы такие же, как и у svm_loss_naive.
  """
  loss = 0.0
  dW = np.zeros(W.shape) # инициализируем градиент нулями

  #############################################################################
  # ЗАДАНИЕ:                                                                  #
  # Реализуйте векторизованную версию кода для вычисления SVM функции потерь. #
  # Сохраните результат в переменной loss.                                    #
  #############################################################################

  # вычисляем потери
  N = X.shape[0]
  scores = X.dot(W)
  correct = scores[range(N), y].reshape((N, 1)) # рейтинг корр. класса
  margin = np.maximum(scores - correct + 1, 0)  # потери по всем примерам
  margin[range(N), y] = 0                       # если класс корректн., для него Li=0
  loss = margin.sum() / N                       # средние потери
  loss += 0.5 * reg * np.sum(W * W)              # потери с учётом регуляризации

  #############################################################################
  #                             КОНЕЦ ВАШЕГО КОДА                             #
  #############################################################################


  #############################################################################
  # ЗАДАНИЕ:                                                                  #  
  # Реализуйте векторизованную версию кода для вычисления градиента SVM       #
  # функции потерь. Сохраните результат в переменной dW.                      #
  # Совет: Используйте некоторые промежуточные значения, которые были         #
  # получены при вычислении функции потерь.                                   #
  #############################################################################

  # бинарная матрица с размером, равным margin
  # в позициях матрицы, где условие SVM не выполняется, 1, иначе 0
  dscores = (margin > 0).astype("float32")

  # в каждую строку dscores в позицию корр. класса вносим суммарное число
  # рейтингов классов, нарушающих условие SVM
  dscores[range(N), y] = -dscores.sum(axis=1)

  # делим на N для вычисления среднего
  dscores /= N

  # градиент регуляризационного члена
  dreg = reg * W

  # полный градиент функции потерь
  dW = (X.T).dot(dscores) + dreg

  #############################################################################
  #                             КОНЕЦ ВАШЕГО КОДА                             #
  #############################################################################

  return loss, dW
