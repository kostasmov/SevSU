import numpy as np
from random import shuffle

def softmax_loss_naive(W, X, y, reg):
  """
  Softmax фнкция потерь, наивная реализация (с циклами)

  Число пикселей изображения - D, число классов - С, мы оперируем миниблоками по N примеров

  Входы:
  - W: numpy массив формы (D, C), содержащий веса линейного слоя.
  - X: numpy массив формы (N, D), содержащий миниблок входных данных.
  - y: numpy массив формы (N,), содержащий обучающие метки; y[i] = c означает
    что X[i] имеет метку c, где 0 <= c < C.
  - reg: (float) коэффициент регуляризации

  Возвращает кортеж:
  - значение потерь (тип single float)
  - градиент функции потерь по отношению к весам W; массив такой же формв, как и W
  """
  
  # Инициализация потерь и градиентов.
  loss = 0.0
  dW = np.zeros_like(W)

  #############################################################################
  # ЗАДАНИЕ: Вычислите softmax  потери и  градиенты, используя явные циклы.   #
  # Сохраните потери в переменной loss, а градиенты в dW.  Не забывайте о     #
  # регуляризации!                                                            #
  #############################################################################

  # опредеяем число классов и число обучающих примеров
  num_classes = W.shape[1]
  num_train = X.shape[0]
   
  # вычисление функции потерь -log(softmax) и градиентов этой функции для всех примеров X[i]   
  for i in range(num_train):
    # вычисляем вектор рейтингов классов для примера X[i]
    scores = X[i].dot(W)
    # смещаем все значения в scores на -max(scores) для исключения возможного переполнения при вычислении exp(scores)
    scores -= np.max(scores)
    # вычисляем вектор экспонент рейтингов классов
    exps = np.exp(scores)
    # сумма экспонент рейтингов, т.е. знаменатель softmax
    expsum = np.sum(exps)
    # вектор вероятностей принадлежности примеров из X[i] классам (функция softmax)
    p = exps / expsum
    # вычисляем потери (правдоподобие) для текущего корректного класса y[i]=с
    L_i = -np.log(p[y[i]])
    # аккумулируем потери по всем примерам
    loss += L_i
    
    # вычисляем векторы градиентов: dL_i/dw_y = (p_y-1)*x_i и dL/dw_j = p_j*x_i
    for j in range(num_classes):
        dW[:,j] += (p[j] - (j == y[i])) * X[i,:].T

  # средние потери      
  loss /= num_train
  # добавляем регуляризацию L2 
  loss += 0.5 * reg * np.sum(W * W)
  # усредненный градиент по всем примерам из мини-блока
  dW /= num_train
  # добавляем градиент от l2    
  dW += reg*W

  #############################################################################
  #                          КОНЕЦ ВАШЕГО КОДА                                #
  #############################################################################

  return loss, dW


def softmax_loss_vectorized(W, X, y, reg):
  """
  Softmax функция потерь, векторизованная версия.

  Входы и выходы те же, что и у функции softmax_loss_naive.
  """

  # Инициализация потерь и градиентов.
  loss = 0.0
  dW = np.zeros_like(W)

  #############################################################################
  # ЗАДАНИЕ: Вычислите softmax  потери и  градиенты без использования циклов. #
  # Сохраните потери в переменной loss, а градиенты в dW.  Не забывайте о     #
  # регуляризации!                                                            #
  #############################################################################

  # опредеяем число классов и число обучающих примеров
  num_classes = W.shape[1]
  num_train = X.shape[0]

  # вычисление матрицы рейтингов
  scores = X.dot(W)

  # смещаем все значения в scores на -max(scores) для исключения возможного переполнения при вычислении exp(scores)
  scores -= scores.max()

  # вычисляем матрицу экспонент рейтингов классов
  exps = np.exp(scores)

  # вычисляем вероятности принадлежности классам (функция softmax)
  p = exps / exps.sum(axis=1, keepdims=True)

  # средние потери по всем примерам
  loss = -np.log(p[range(num_train), y]).sum() / num_train

  # добавляем регуляризация
  loss += 0.5 * reg * np.sum(W * W)

  # градиенты функции потерь: dL_i/dw_y = (p_y-1)*x_i и dL/dw_j = p_j*x_i
  dscores = p
  dscores[range(num_train), y] -= 1
  dreg = reg * W
  dW = X.T.dot(dscores) / num_train + dreg

  #############################################################################
  #                         КОНЕЦ ВАШЕГО КОДА                                 #
  #############################################################################

  return loss, dW

