from __future__ import print_function

import numpy as np
from dlcv.classifiers.linear_svm import *
from dlcv.classifiers.softmax import *

class LinearClassifier(object):

  def __init__(self):
    self.W = None

  def train(self, X, y, learning_rate=1e-3, reg=1e-5, num_iters=100,
            batch_size=200, verbose=False):
    """
    Обучение линейного классификатора на основе стохастического градиентного спуска.

    Входы:
    - X: numpy масссив формы (N, D),содержащий обучающие данные; 
      N обучающих выборок каждая размерностью  D.
    - y: numpy масссив формы (N,) содержащий обучающие метки; y[i] = c
      означает что X[i] имеет метку 0 <= c < C , где C число классов.
    - learning_rate: (float) скорость обучения .
    - reg: (float) коэффициент регуляризации .
    - num_iters: (integer) число шагов оптимизации
    - batch_size: (integer) число обучающих примеров в мини-блоке.
    - verbose: (boolean) Если true, то печаются промежуточные данные в ходе оптимизации.

    Выходы:
    Список, содержаший значения функции потерь на каждом шаге обучения.
    """

    num_train, dim = X.shape
    num_classes = np.max(y) + 1 #  y принимает значения 0...K-1, где K число классов
    if self.W is None:
      # ленивая инициализация W
      self.W = 0.001 * np.random.randn(dim, num_classes)

    # Выполнение стохастического градиентного спуска для оптимизации W
    loss_history = []             #список для хранения значений потерь 
    for it in range(num_iters):
      X_batch = None
      y_batch = None

      #########################################################################
      # ЗАДАНИЕ: Ознакомьтесь с этой частью кода                              #
      # Сделайте выборку batch_size элементов из обучающих данных и их        #
      # меток для применения при градиентном спуске.                          #
      # Сохраните данные в X_batch и сответсвующие метки в y_batch;           #
      # Блок X_batch должен иметь размерность (D, batch_size),                #
      # а блок y_batch - размерность (batch_size,)                            #
      #                                                                       #
      # Совет: Используйте np.random.choice для генерации индексов. Выборка с #
      # с заменой быстрее, чем выборка без замены.                            #
      #########################################################################

      mask = np.random.choice(num_train, batch_size)  # формируем случайные индексы
      X_batch = X[mask]                               # выборка мини-блока данных
      y_batch = y[mask]

      #########################################################################
      #                       КОНЕЦ ВАШЕГО КОДА                               #
      #########################################################################

      # оценка потерь и градиента
      loss, grad = self.loss(X_batch, y_batch, reg)
      loss_history.append(loss) # добавляем значение потерь в список

      #########################################################################
      # ЗАДАНИЕ:  Реализуйте SGD                                              #
      # Для этого обновите веса, используя градиент и скорость обучения.      #         
      #########################################################################

      # обновление параметров
      self.W -= learning_rate * grad

      #########################################################################
      #                        КОНЕЦ ВАШЕГО КОДА                              #
      #########################################################################

      if verbose and it % 100 == 0:
        print('итерация %d / %d: потери %f' % (it, num_iters, loss))

    return loss_history

  def predict(self, X):
    """
    Использует обученные веса линейного классификатора для предсказания меток 
    данных

    Входы:
    - X: numpy масссив формы (N, D), содержащий обучающие данные; 
      N обучающих выборок каждая размером D.

    Выходы:
    - y_pred: Предсказанные метки данных в X.
              y_pred -  1-мерный массив длиной N,
              где каждый элемент - это целое число, 
              соотвествующее предсказанной метке класса.
    """

    y_pred = np.zeros(X.shape[0])

    ###########################################################################
    # ЗАДАНИЕ:                                                                #
    # Реализуте этот метод. Сохраните предсказанные метки в y_pred.           #
    ###########################################################################
    
    scores = X.dot(self.W)              # вычисляем рейтинги s=Wx
    y_pred = np.argmax(scores, axis=1)  # индекс предсказанного класса

    ###########################################################################
    #                            КОНЕЦ ВАШЕГО КОДА                            #
    ###########################################################################

    return y_pred
  
  def loss(self, X_batch, y_batch, reg):
    """
    Вычисляет функцию потерь и её производную.
    Подклассы ниже  переопределяют её
    
    Входы:
    - X_batch: numpy масссив формы (N, D),содержащий миниблок данных; 
      N обучающих выборок каждая размерностью  D.
    - y_batch: numpy масссив формы (N,) содержащий метки выборок миниблока;
    - reg: (float) коэффициент регуляризации.

    Возвращает кортеж:
    - потери в виде single float
    - градиент по отношению к self.W; массив такой формы как и  W
    """
    pass


class LinearSVM(LinearClassifier):
  """ Подкласс, который использует мультиклассовую SVM функцию потерь """
  # переопределение функции потерь базового класса LinearClassifier
  def loss(self, X_batch, y_batch, reg):
    return svm_loss_vectorized(self.W, X_batch, y_batch, reg)


class Softmax(LinearClassifier):
  """ Подкласс, который использует Softmax + кросс-энтропийную функцию потерь """
  # переопределение функции потерь базового класса LinearClassifier
  def loss(self, X_batch, y_batch, reg):
    return softmax_loss_vectorized(self.W, X_batch, y_batch, reg)

